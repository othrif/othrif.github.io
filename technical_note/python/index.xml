<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Othmane Rifki</title>
    <link>http://othrif.github.io/technical_note/python/</link>
      <atom:link href="http://othrif.github.io/technical_note/python/index.xml" rel="self" type="application/rss+xml" />
    <description>Othmane Rifki</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 12 Apr 2020 14:41:32 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Othmane Rifki</title>
      <link>http://othrif.github.io/technical_note/python/</link>
    </image>
    
    <item>
      <title>Agglomerative Hiearchial clustering with scipy</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/clustering_hierarchy/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/clustering_hierarchy/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Begin with a seperate cluster, at each step merge the the two closest clusters&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
df = pd.read_csv(&#39;Grains/seeds.csv&#39;, header=None)
df = df.sample(n=50)
samples = df.iloc[:,:-1].to_numpy()
labels = df.iloc[:,-1].to_numpy()-1
label_names = np.array([&#39;Kama wheat&#39;, &#39;Rosa wheat&#39;, &#39;Canadian wheat&#39;])
varieties = label_names[labels]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt

# Calculate the linkage: mergings
mergings = linkage(samples, method=&#39;complete&#39;)

# Plot the dendrogram, using varieties as labels
fig, ax = plt.subplots(figsize=(20,5))
dendrogram(mergings,
           labels=varieties,
           leaf_rotation=90,
           leaf_font_size=12,
)
fig
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_hierarchy_3_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;hierarchical-clustering&#34;&gt;Hierarchical clustering&lt;/h3&gt;
&lt;p&gt;Several methods of clustering:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;complete&lt;/code&gt;: farthest elements of each cluster&lt;/li&gt;
&lt;li&gt;&lt;code&gt;single&lt;/code&gt;: closest elements of each cluster&lt;/li&gt;
&lt;li&gt;Note height of dendrogram is the distance between clusters, the merging point is the distance between them&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
import pandas as pd
from scipy.cluster.hierarchy import fcluster

# Use fcluster to extract labels: labels
labels = fcluster(mergings, 7, criterion=&#39;distance&#39;)

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({&#39;labels&#39;: labels, &#39;varieties&#39;: varieties})

# Create crosstab: ct
ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;varieties&#39;])

# Display ct
print(ct)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[2 1 3 3 1 2 3 3 3 3 1 3 3 3 3 2 2 2 2 1 3 3 3 3 1 2 3 1 3 3 1 3 1 3 3 2 3
 3 3 3 3 1 2 2 3 2 2 1 3 3]
[&#39;Canadian wheat&#39; &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Rosa wheat&#39;
 &#39;Canadian wheat&#39; &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Canadian wheat&#39;
 &#39;Canadian wheat&#39; &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39;
 &#39;Kama wheat&#39; &#39;Canadian wheat&#39; &#39;Canadian wheat&#39; &#39;Canadian wheat&#39;
 &#39;Canadian wheat&#39; &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39;
 &#39;Kama wheat&#39; &#39;Rosa wheat&#39; &#39;Canadian wheat&#39; &#39;Kama wheat&#39; &#39;Rosa wheat&#39;
 &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Rosa wheat&#39;
 &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Canadian wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39;
 &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Rosa wheat&#39; &#39;Rosa wheat&#39; &#39;Canadian wheat&#39;
 &#39;Canadian wheat&#39; &#39;Kama wheat&#39; &#39;Canadian wheat&#39; &#39;Canadian wheat&#39;
 &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Canadian wheat&#39;]
varieties  Canadian wheat  Kama wheat  Rosa wheat
labels                                           
1                       0           0          10
2                      12           0           0
3                       3          23           2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Classification in Scikit-Learn with Iris</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/classification_sklearn/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/classification_sklearn/</guid>
      <description>&lt;h3 id=&#34;exploratory-data-analysis-eda&#34;&gt;Exploratory data analysis (EDA)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use(&#39;ggplot&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris = datasets.load_iris()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.keys()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;type(iris.data), type(iris.target)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(numpy.ndarray, numpy.ndarray)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.data.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(150, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.target_names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&amp;lt;U10&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.feature_names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;sepal length (cm)&#39;,
 &#39;sepal width (cm)&#39;,
 &#39;petal length (cm)&#39;,
 &#39;petal width (cm)&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = iris.data
y = iris.target
df = pd.DataFrame(X, columns=iris.feature_names)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sepal length (cm)&lt;/th&gt;
      &lt;th&gt;sepal width (cm)&lt;/th&gt;
      &lt;th&gt;petal length (cm)&lt;/th&gt;
      &lt;th&gt;petal width (cm)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5.1&lt;/td&gt;
      &lt;td&gt;3.5&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4.9&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;4.7&lt;/td&gt;
      &lt;td&gt;3.2&lt;/td&gt;
      &lt;td&gt;1.3&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.6&lt;/td&gt;
      &lt;td&gt;3.1&lt;/td&gt;
      &lt;td&gt;1.5&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;3.6&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;_ = pd.plotting.scatter_matrix(df, c = y, figsize = [8,8], s = 150, marker = &#39;D&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;classification_sklearn_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Clustering with k-means</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/clustering_kmeans/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/clustering_kmeans/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import datasets
iris = datasets.load_iris()
data = iris[&#39;data&#39;]
features = iris[&#39;feature_names&#39;]
target = iris[&#39;target&#39;]
target_names = iris[&#39;target_names&#39;]

# map target labels to species names =&amp;gt; Ground Truth
species = target_names[target]
print(species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import KMeans
from sklearn.cluster import KMeans

# Create a KMeans instance with 3 clusters: model
model = KMeans(n_clusters=3)

# Fit model to points
model.fit(data)

# Determine the cluster labels of iris data: labels =&amp;gt; Prediction
labels = model.predict(data)

# can also use: labels = model.fit_predict(data)

# Calculate inertia: Measures how spread out the clusters are (lower is be!er) 
print(model.inertia_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;78.85144142614601
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pyplot
import matplotlib.pyplot as plt

# Assign the columns of new_points: xs and ys
xs = data[:,0]
ys = data[:,2]

fig, ax = plt.subplots()
# Make a scatter plot of xs and ys, using labels to define the colors
ax.scatter(xs, ys, c=labels, alpha=0.3)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,2]

# Make a scatter plot of centroids_x and centroids_y
ax.scatter(centroids_x, centroids_y, marker=&#39;D&#39;, s=100, color=&#39;r&#39;)
ax.set_title(&#39;K-means clustering of Iris dataset&#39;)
ax.set_xlabel(features[0])
ax.set_ylabel(features[2])
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_kmeans_3_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;compare-ground-truth-to-prediction&#34;&gt;Compare ground truth to prediction&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.DataFrame({&#39;labels&#39;: labels, &#39;species&#39;: species})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;species&#39;])
print(ct)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;species  setosa  versicolor  virginica
labels                                
0             0          48         14
1            50           0          0
2             0           2         36
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;what-is-the-best-clusters-to-choose&#34;&gt;What is the best clusters to choose?&lt;/h3&gt;
&lt;p&gt;The elbow rule, the point where the decrease slows down&lt;br&gt;
See below, &lt;strong&gt;3 is a good choice&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ks = range(1, 10)
inertias = []

for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(data)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
# Plot ks vs inertias
plt.plot(ks, inertias, &#39;-o&#39;)
plt.xlabel(&#39;number of clusters, k&#39;)
plt.ylabel(&#39;inertia&#39;)
plt.xticks(ks)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_kmeans_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;pipelines-with-kmeans-and-standardscaller&#34;&gt;Pipelines with Kmeans and StandardScaller&lt;/h2&gt;
&lt;h3 id=&#34;standard-scaller&#34;&gt;Standard scaller&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in kmeans: feature variance = feature influence&lt;/li&gt;
&lt;li&gt;&lt;code&gt;StandardScaller&lt;/code&gt; transforms each feature to have mean 0 and variance 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;fish.csv&#39;, header=None) # prevent first row from becoming header
samples = df.iloc[:,1:].to_numpy()
species = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Create scaler: scaler
scaler = StandardScaler()

# Create KMeans instance: kmeans
kmeans = KMeans(n_clusters=4)

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, kmeans)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit the pipeline to samples
pipeline.fit(samples)

# Calculate the cluster labels: labels
labels = pipeline.predict(samples)

# Create a DataFrame with labels and species as columns: df
df = pd.DataFrame({&#39;labels&#39;:labels, &#39;species&#39;:species})

# Create crosstab: ct
ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;species&#39;])

# Display ct
print(ct)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;species  Bream  Pike  Roach  Smelt
labels                            
0           33     0      1      0
1            1     0     19      1
2            0    17      0      0
3            0     0      0     13
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;full-pipeline-with-stocks&#34;&gt;Full pipeline with stocks&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;stock.csv&#39;)
df.head()
movements = df.iloc[:,1:].to_numpy()
companies = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.cluster import KMeans

# Create a normalizer: normalizer
normalizer = Normalizer()

# Create a KMeans model with 10 clusters: kmeans
kmeans = KMeans(n_clusters=10)

# Make a pipeline chaining normalizer and kmeans: pipeline
pipeline = make_pipeline(normalizer, kmeans)

# Fit pipeline to the daily price movements
pipeline.fit(movements)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Pipeline(memory=None,
         steps=[(&#39;normalizer&#39;, Normalizer(copy=True, norm=&#39;l2&#39;)),
                (&#39;kmeans&#39;,
                 KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;,
                        max_iter=300, n_clusters=10, n_init=10, n_jobs=None,
                        precompute_distances=&#39;auto&#39;, random_state=None,
                        tol=0.0001, verbose=0))],
         verbose=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pandas
import pandas as pd

# Predict the cluster labels: labels
labels = pipeline.predict(movements)

# Create a DataFrame aligning labels and companies: df
df = pd.DataFrame({&#39;labels&#39;: labels, &#39;companies&#39;: companies})

# Display df sorted by cluster label
print(df.sort_values(&#39;labels&#39;))

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    labels                           companies
42       0                   Royal Dutch Shell
52       0                            Unilever
53       0                       Valero Energy
23       0                                 IBM
49       0                               Total
47       0                            Symantec
13       0                   DuPont de Nemours
12       0                             Chevron
46       0                      Sanofi-Aventis
19       0                     GlaxoSmithKline
10       0                      ConocoPhillips
8        0                         Caterpillar
35       0                            Navistar
6        0            British American Tobacco
37       0                            Novartis
44       0                        Schlumberger
57       0                               Exxon
39       0                              Pfizer
32       0                                  3M
43       0                                 SAP
36       1                    Northrop Grumman
29       1                     Lookheed Martin
4        1                              Boeing
9        2                   Colgate-Palmolive
40       2                      Procter Gamble
25       2                   Johnson &amp;amp; Johnson
27       2                      Kimberly-Clark
1        3                                 AIG
3        3                    American express
5        3                     Bank of America
55       3                         Wells Fargo
18       3                       Goldman Sachs
58       3                               Xerox
26       3                      JPMorgan Chase
15       3                                Ford
16       3                   General Electrics
20       4                          Home Depot
45       4                                Sony
34       4                          Mitsubishi
7        4                               Canon
48       4                              Toyota
54       4                            Walgreen
21       4                               Honda
0        5                               Apple
33       5                           Microsoft
22       5                                  HP
14       5                                Dell
11       5                               Cisco
24       6                               Intel
50       6  Taiwan Semiconductor Manufacturing
51       6                   Texas instruments
38       7                               Pepsi
31       7                           McDonalds
28       7                           Coca Cola
56       7                            Wal-Mart
41       7                       Philip Morris
17       8                     Google/Alphabet
2        8                              Amazon
59       8                               Yahoo
30       9                          MasterCard
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Clustering with k-means</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/clustering_tsne/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/clustering_tsne/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;t-SNE = “t-distributed stochastic neighbor embedding”&lt;/li&gt;
&lt;li&gt;Maps samples to 2D space (or 3D)&lt;/li&gt;
&lt;li&gt;only has &lt;code&gt;fit_transform&lt;/code&gt; method, have to repeat everytime&lt;/li&gt;
&lt;li&gt;Try &lt;code&gt;learning_rate&lt;/code&gt; between 50 and 200&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iris-dataset&#34;&gt;Iris dataset&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import datasets
iris = datasets.load_iris()
samples = iris[&#39;data&#39;]
features = iris[&#39;feature_names&#39;]
species = iris[&#39;target&#39;]
target_names = iris[&#39;target_names&#39;]

# map target labels to species names =&amp;gt; Ground Truth
#species = target_names[species]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

model = TSNE(learning_rate=100)

transformed = model.fit_transform(samples)
xs=transformed[:,0]
ys=transformed[:,1]
plt.scatter(xs, ys, c=species)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_tsne_4_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;stock-price&#34;&gt;Stock price&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;stock.csv&#39;)
df.head()
movements = df.iloc[:,1:].to_numpy()
companies = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;normalize-data-using-a-transform&#34;&gt;Normalize data using a transform&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.preprocessing import Normalizer

# Create a normalizer: normalizer
normalizer = Normalizer()

normalized_movements = normalizer.transform(movements)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import TSNE
from sklearn.manifold import TSNE

# Create a TSNE instance: model
model = TSNE(learning_rate=50)

# Apply fit_transform to normalized_movements: tsne_features
tsne_features = model.fit_transform(normalized_movements)

# Select the 0th feature: xs
xs = tsne_features[:,0]

# Select the 1th feature: ys
ys = tsne_features[:,1]

# Scatter plot
fig, ax = plt.subplots(figsize=(20,20))
ax.scatter(xs, ys, alpha=0.5)

# Annotate the points
for x, y, company in zip(xs, ys, companies):
    ax.annotate(company, (x, y), fontsize=10, alpha=0.75)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_tsne_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dimension reduction with PCA transformation</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/pca_tranf/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/pca_tranf/</guid>
      <description>&lt;p&gt;PCA: Principle Component Analysis: step1 &amp;gt; decorrelation, step2 &amp;gt; reduce dimension&lt;/p&gt;
&lt;p&gt;Principle component: direction in which the sample varies the most&lt;/p&gt;
&lt;p&gt;1- Decorrelation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rotates data samples to be aligned with axes&lt;/li&gt;
&lt;li&gt;mean ~ 0&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iris-dataset&#34;&gt;Iris dataset&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
df = pd.read_csv(&#39;Grains/seeds-width-vs-length.csv&#39;, header=None)
grains = df.to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Assign the 0th column of grains: width
width = grains[:, 0]

# Assign the 1st column of grains: length
length = grains[:, 1]

# Scatter plot width vs length
plt.scatter(width, length)
plt.axis(&#39;equal&#39;)
plt.show()

# Calculate the Pearson correlation
correlation, pvalue = pearsonr(width, length)

# Display the correlation
print(f&#39;Pearson Correlation before-PCA: {correlation}&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;pca_tranf_4_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Pearson Correlation before-PCA: 0.8604149377143466
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import PCA
from sklearn.decomposition import PCA

# Create PCA instance: model
model = PCA()

# Apply the fit_transform method of model to grains: pca_features
pca_features = model.fit_transform(grains)

# Assign 0th column of pca_features: xs
xs = pca_features[:,0]

# Assign 1st column of pca_features: ys
ys = pca_features[:,1]

# Scatter plot xs vs ys
plt.scatter(xs, ys)
plt.axis(&#39;equal&#39;)
plt.show()

# Calculate the Pearson correlation of xs and ys
correlation, pvalue = pearsonr(xs, ys)

# Display the correlation
print(f&#39;Pearson Correlation after-PCA: {correlation}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;pca_tranf_5_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Pearson Correlation after-PCA: 2.5478751053409354e-17
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Regression and classification in Scikit-Learn</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/regression/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/regression/</guid>
      <description>&lt;h3 id=&#34;linear-regression&#34;&gt;Linear regression&lt;/h3&gt;
&lt;p&gt;make predictions, visualize the model fit, and analyze the formula used to generate your fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import matplotlib.pyplot as plt
weather = pd.read_csv(&#39;weatherAUS.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;prepare-your-data&#34;&gt;Prepare your data&lt;/h4&gt;
&lt;p&gt;Do I have any null values in my dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;any(weather.isnull())
# For numpy
# np.any(np.isnan(mat))
# np.all(np.isfinite(mat))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#weather = weather.fillna(method=&#39;ffill&#39;) # inpute missing values
weather.dropna(inplace=True) # drop raws with null values, can loose important information
weather_train = weather.sample(n=100) 
weather_test = weather.sample(n=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;weather = weather_train
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression 
X = np.array(weather[&#39;Humidity9am&#39;]).reshape(-1,1)
y = weather[&#39;Humidity3pm&#39;]


# Create and fit your linear regression model
lm = LinearRegression()
lm.fit(X, y)

# Assign and print predictions
preds = lm.predict(X)

# Plot your fit to visualize your model
plt.scatter(X, y)
plt.plot(X, preds, color=&#39;red&#39;)
plt.show()

# Assign and print coefficient 
coef = lm.coef_
print(f&#39;Coefficient: {np.round(coef,2)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;regression_6_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Coefficient: [0.72]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that for every 1 unit of humidity in the morning, we can expect about 0.71 units of humidity in the afternoon. More practically, this information tells us that humidity drops about 29% from morning to afternoon!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# R-squared score
r2 = lm.score(X,y)
print(f&#39;R-squared score: {r2:.2f}&#39;)

# Mean squared error
from sklearn.metrics import mean_squared_error
preds = lm.predict(X)
mse = mean_squared_error(y, preds)
print(f&#39;Mean squared error: {mse:.2f}&#39;)

# Mean absolute error
from sklearn.metrics import mean_absolute_error
preds = lm.predict(X)
mae = mean_absolute_error(y, preds)
print(f&#39;Mean absolute error: {mae:.2f}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;R-squared score: 0.43
Mean squared error: 210.46
Mean absolute error: 11.54
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that our R-squared value tells us the percentage of the variance of y that X is responsible for. Which error metric would you recommend for this dataset? If you remember from when you plotted your model fit, there aren&amp;rsquo;t too many outliers, so mean squared error would be a good choice to go with!&lt;/p&gt;
&lt;h5 id=&#34;bias-variance-tradeoff&#34;&gt;Bias-Variance tradeoff&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import operator
from sklearn.preprocessing import PolynomialFeatures
polynomial_features= PolynomialFeatures(degree=10)

x_poly = polynomial_features.fit_transform(X)

lm2 = LinearRegression()
lm2.fit(x_poly, y)
preds2 = lm2.predict(x_poly)


plt.scatter(X, y)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(X,preds2), key=sort_axis)
x, y_poly_pred = zip(*sorted_zip)
plt.plot(x, y_poly_pred, color=&#39;m&#39;)
plt.show()


# R-squared score
r2 = lm2.score(x_poly, preds2)
print(f&#39;R-squared score: {r2:.2f}&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;regression_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;R-squared score: 1.00
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;logistic-regression&#34;&gt;Logistic regression&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
from sklearn import preprocessing 
def scaleX(df):
    x = df.values #returns a numpy array
    min_max_scaler = preprocessing.MinMaxScaler()
    x_scaled = min_max_scaler.fit_transform(x)
    return pd.DataFrame(x_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train = np.array(scaleX(weather_train.loc[:,[&#39;Humidity9am&#39;,&#39;Humidity3pm&#39;]])).reshape(-1,2)
y_train = np.array(weather_train[&#39;RainTomorrow&#39;])
y_train = (y_train==&#39;Yes&#39;).astype(int)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_test = np.array(scaleX(weather_test.loc[:,[&#39;Humidity9am&#39;,&#39;Humidity3pm&#39;]])).reshape(-1,2)
y_test = np.array(weather_test[&#39;RainTomorrow&#39;])
y_test = (y_test==&#39;Yes&#39;).astype(int)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression
# Create and fit your model
clf = LogisticRegression()
clf.fit(X_train, y_train)

# Compute and print the accuracy
acc = clf.score(X_test, y_test)
print(f&#39;Accuracy: {np.round(acc,2)}&#39;)

# Assign and print the coefficents
coefs = clf.coef_
print(f&#39;Coefficient: {np.round(coefs,2)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 0.83
Coefficient: [[0.05 6.81]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our features were normalized beforehand, we can look at the magnitude of our coefficients to tell us the importance of each independent variable. Here you can see the the second variable, Humidity3pm was much more important to our outcome than humidity from that morning. This is intuitive since we are trying to predict the rain for tomorrow!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate and output the confusion matrix
from sklearn.metrics import confusion_matrix
preds = clf.predict(X_test)
matrix = confusion_matrix(y_test, preds)
print(f&#39;Confusion matrix: {np.round(matrix,2)}&#39;)

# Compute and print the precision
from sklearn.metrics import precision_score
preds = clf.predict(X_test)
precision = precision_score(y_test, preds)
print(f&#39;Precision: {np.round(precision,2)}&#39;)

# Compute and print the recall
from sklearn.metrics import recall_score
preds = clf.predict(X_test)
recall = recall_score(y_test, preds)
print(f&#39;Recall: {np.round(recall,2)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Confusion matrix: [[7508  262]
 [1440  790]]
Precision: 0.75
Recall: 0.35
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see here that the precision of our rain prediction model is higher, meaning that we didn&amp;rsquo;t make too many Type I errors. However, there were plenty of Type II errors shown in the bottom-left quadrant of the confusion matrix. This is indicated further by the low recall score, meaning that there were plenty of rainy days that we missed out on.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
