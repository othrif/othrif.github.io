<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Othmane Rifki</title>
    <link>http://othrif.github.io/technical_note/python/</link>
      <atom:link href="http://othrif.github.io/technical_note/python/index.xml" rel="self" type="application/rss+xml" />
    <description>Othmane Rifki</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 12 Apr 2020 14:41:32 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Othmane Rifki</title>
      <link>http://othrif.github.io/technical_note/python/</link>
    </image>
    
    <item>
      <title>Classification in Scikit-Learn with Iris</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/classification_sklearn/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/classification_sklearn/</guid>
      <description>&lt;h3 id=&#34;exploratory-data-analysis-eda&#34;&gt;Exploratory data analysis (EDA)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use(&#39;ggplot&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris = datasets.load_iris()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.keys()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;type(iris.data), type(iris.target)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(numpy.ndarray, numpy.ndarray)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.data.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(150, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.target_names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&amp;lt;U10&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.feature_names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;sepal length (cm)&#39;,
 &#39;sepal width (cm)&#39;,
 &#39;petal length (cm)&#39;,
 &#39;petal width (cm)&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = iris.data
y = iris.target
df = pd.DataFrame(X, columns=iris.feature_names)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sepal length (cm)&lt;/th&gt;
      &lt;th&gt;sepal width (cm)&lt;/th&gt;
      &lt;th&gt;petal length (cm)&lt;/th&gt;
      &lt;th&gt;petal width (cm)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5.1&lt;/td&gt;
      &lt;td&gt;3.5&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4.9&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;4.7&lt;/td&gt;
      &lt;td&gt;3.2&lt;/td&gt;
      &lt;td&gt;1.3&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.6&lt;/td&gt;
      &lt;td&gt;3.1&lt;/td&gt;
      &lt;td&gt;1.5&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;3.6&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;_ = pd.plotting.scatter_matrix(df, c = y, figsize = [8,8], s = 150, marker = &#39;D&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;classification_sklearn_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Clustering with k-means</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/clustering_kmeans/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/clustering_kmeans/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import datasets
iris = datasets.load_iris()
data = iris[&#39;data&#39;]
features = iris[&#39;feature_names&#39;]
target = iris[&#39;target&#39;]
target_names = iris[&#39;target_names&#39;]

# map target labels to species names =&amp;gt; Ground Truth
species = target_names[target]
print(species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import KMeans
from sklearn.cluster import KMeans

# Create a KMeans instance with 3 clusters: model
model = KMeans(n_clusters=3)

# Fit model to points
model.fit(data)

# Determine the cluster labels of iris data: labels =&amp;gt; Prediction
labels = model.predict(data)

# can also use: labels = model.fit_predict(data)

# Calculate inertia: Measures how spread out the clusters are (lower is be!er) 
print(model.inertia_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;78.85144142614601
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pyplot
import matplotlib.pyplot as plt

# Assign the columns of new_points: xs and ys
xs = data[:,0]
ys = data[:,2]

fig, ax = plt.subplots()
# Make a scatter plot of xs and ys, using labels to define the colors
ax.scatter(xs, ys, c=labels, alpha=0.3)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,2]

# Make a scatter plot of centroids_x and centroids_y
ax.scatter(centroids_x, centroids_y, marker=&#39;D&#39;, s=100, color=&#39;r&#39;)
ax.set_title(&#39;K-means clustering of Iris dataset&#39;)
ax.set_xlabel(features[0])
ax.set_ylabel(features[2])
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_kmeans_3_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;compare-ground-truth-to-prediction&#34;&gt;Compare ground truth to prediction&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.DataFrame({&#39;labels&#39;: labels, &#39;species&#39;: species})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;species&#39;])
print(ct)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;species  setosa  versicolor  virginica
labels                                
0             0          48         14
1            50           0          0
2             0           2         36
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;what-is-the-best-clusters-to-choose&#34;&gt;What is the best clusters to choose?&lt;/h3&gt;
&lt;p&gt;The elbow rule, the point where the decrease slows down&lt;br&gt;
See below, &lt;strong&gt;3 is a good choice&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ks = range(1, 10)
inertias = []

for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(data)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
# Plot ks vs inertias
plt.plot(ks, inertias, &#39;-o&#39;)
plt.xlabel(&#39;number of clusters, k&#39;)
plt.ylabel(&#39;inertia&#39;)
plt.xticks(ks)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_kmeans_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;pipelines-with-kmeans-and-standardscaller&#34;&gt;Pipelines with Kmeans and StandardScaller&lt;/h2&gt;
&lt;h3 id=&#34;standard-scaller&#34;&gt;Standard scaller&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in kmeans: feature variance = feature influence&lt;/li&gt;
&lt;li&gt;&lt;code&gt;StandardScaller&lt;/code&gt; transforms each feature to have mean 0 and variance 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;fish.csv&#39;, header=None) # prevent first row from becoming header
samples = df.iloc[:,1:].to_numpy()
species = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Create scaler: scaler
scaler = StandardScaler()

# Create KMeans instance: kmeans
kmeans = KMeans(n_clusters=4)

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, kmeans)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit the pipeline to samples
pipeline.fit(samples)

# Calculate the cluster labels: labels
labels = pipeline.predict(samples)

# Create a DataFrame with labels and species as columns: df
df = pd.DataFrame({&#39;labels&#39;:labels, &#39;species&#39;:species})

# Create crosstab: ct
ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;species&#39;])

# Display ct
print(ct)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;species  Bream  Pike  Roach  Smelt
labels                            
0           33     0      1      0
1            1     0     19      1
2            0    17      0      0
3            0     0      0     13
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;full-pipeline-with-stocks&#34;&gt;Full pipeline with stocks&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;stock.csv&#39;)
df.head()
movements = df.iloc[:,1:].to_numpy()
companies = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.cluster import KMeans

# Create a normalizer: normalizer
normalizer = Normalizer()

# Create a KMeans model with 10 clusters: kmeans
kmeans = KMeans(n_clusters=10)

# Make a pipeline chaining normalizer and kmeans: pipeline
pipeline = make_pipeline(normalizer, kmeans)

# Fit pipeline to the daily price movements
pipeline.fit(movements)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Pipeline(memory=None,
         steps=[(&#39;normalizer&#39;, Normalizer(copy=True, norm=&#39;l2&#39;)),
                (&#39;kmeans&#39;,
                 KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;,
                        max_iter=300, n_clusters=10, n_init=10, n_jobs=None,
                        precompute_distances=&#39;auto&#39;, random_state=None,
                        tol=0.0001, verbose=0))],
         verbose=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pandas
import pandas as pd

# Predict the cluster labels: labels
labels = pipeline.predict(movements)

# Create a DataFrame aligning labels and companies: df
df = pd.DataFrame({&#39;labels&#39;: labels, &#39;companies&#39;: companies})

# Display df sorted by cluster label
print(df.sort_values(&#39;labels&#39;))

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    labels                           companies
18       0                       Goldman Sachs
55       0                         Wells Fargo
3        0                    American express
5        0                     Bank of America
26       0                      JPMorgan Chase
16       0                   General Electrics
15       0                                Ford
48       1                              Toyota
45       1                                Sony
7        1                               Canon
21       1                               Honda
34       1                          Mitsubishi
22       1                                  HP
28       2                           Coca Cola
41       2                       Philip Morris
9        2                   Colgate-Palmolive
25       2                   Johnson &amp;amp; Johnson
38       2                               Pepsi
27       2                      Kimberly-Clark
40       2                      Procter Gamble
49       3                               Total
52       3                            Unilever
43       3                                 SAP
42       3                   Royal Dutch Shell
37       3                            Novartis
6        3            British American Tobacco
39       3                              Pfizer
19       3                     GlaxoSmithKline
46       3                      Sanofi-Aventis
36       4                    Northrop Grumman
29       4                     Lookheed Martin
54       4                            Walgreen
4        4                              Boeing
44       5                        Schlumberger
53       5                       Valero Energy
12       5                             Chevron
35       5                            Navistar
10       5                      ConocoPhillips
8        5                         Caterpillar
57       5                               Exxon
51       6                   Texas instruments
50       6  Taiwan Semiconductor Manufacturing
0        6                               Apple
11       6                               Cisco
24       6                               Intel
33       6                           Microsoft
32       7                                  3M
2        7                              Amazon
13       7                   DuPont de Nemours
14       7                                Dell
17       7                     Google/Alphabet
23       7                                 IBM
47       7                            Symantec
58       7                               Xerox
30       7                          MasterCard
31       7                           McDonalds
20       7                          Home Depot
59       7                               Yahoo
56       8                            Wal-Mart
1        9                                 AIG
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Clustering with k-means</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/clustering_tsne/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/clustering_tsne/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import datasets
iris = datasets.load_iris()
data = iris[&#39;data&#39;]
features = iris[&#39;feature_names&#39;]
target = iris[&#39;target&#39;]
target_names = iris[&#39;target_names&#39;]

# map target labels to species names =&amp;gt; Ground Truth
species = target_names[target]
print(species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import KMeans
from sklearn.cluster import KMeans

# Create a KMeans instance with 3 clusters: model
model = KMeans(n_clusters=3)

# Fit model to points
model.fit(data)

# Determine the cluster labels of iris data: labels =&amp;gt; Prediction
labels = model.predict(data)

# can also use: labels = model.fit_predict(data)

# Calculate inertia: Measures how spread out the clusters are (lower is be!er) 
print(model.inertia_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;78.85144142614601
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pyplot
import matplotlib.pyplot as plt

# Assign the columns of new_points: xs and ys
xs = data[:,0]
ys = data[:,2]

fig, ax = plt.subplots()
# Make a scatter plot of xs and ys, using labels to define the colors
ax.scatter(xs, ys, c=labels, alpha=0.3)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,2]

# Make a scatter plot of centroids_x and centroids_y
ax.scatter(centroids_x, centroids_y, marker=&#39;D&#39;, s=100, color=&#39;r&#39;)
ax.set_title(&#39;K-means clustering of Iris dataset&#39;)
ax.set_xlabel(features[0])
ax.set_ylabel(features[2])
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_tsne_3_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;compare-ground-truth-to-prediction&#34;&gt;Compare ground truth to prediction&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.DataFrame({&#39;labels&#39;: labels, &#39;species&#39;: species})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;species&#39;])
print(ct)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;species  setosa  versicolor  virginica
labels                                
0             0          48         14
1            50           0          0
2             0           2         36
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;what-is-the-best-clusters-to-choose&#34;&gt;What is the best clusters to choose?&lt;/h3&gt;
&lt;p&gt;The elbow rule, the point where the decrease slows down&lt;br&gt;
See below, &lt;strong&gt;3 is a good choice&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ks = range(1, 10)
inertias = []

for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(data)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
# Plot ks vs inertias
plt.plot(ks, inertias, &#39;-o&#39;)
plt.xlabel(&#39;number of clusters, k&#39;)
plt.ylabel(&#39;inertia&#39;)
plt.xticks(ks)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_tsne_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;pipelines-with-kmeans-and-standardscaller&#34;&gt;Pipelines with Kmeans and StandardScaller&lt;/h2&gt;
&lt;h3 id=&#34;standard-scaller&#34;&gt;Standard scaller&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in kmeans: feature variance = feature influence&lt;/li&gt;
&lt;li&gt;&lt;code&gt;StandardScaller&lt;/code&gt; transforms each feature to have mean 0 and variance 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;fish.csv&#39;, header=None) # prevent first row from becoming header
samples = df.iloc[:,1:].to_numpy()
species = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Create scaler: scaler
scaler = StandardScaler()

# Create KMeans instance: kmeans
kmeans = KMeans(n_clusters=4)

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, kmeans)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit the pipeline to samples
pipeline.fit(samples)

# Calculate the cluster labels: labels
labels = pipeline.predict(samples)

# Create a DataFrame with labels and species as columns: df
df = pd.DataFrame({&#39;labels&#39;:labels, &#39;species&#39;:species})

# Create crosstab: ct
ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;species&#39;])

# Display ct
print(ct)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;species  Bream  Pike  Roach  Smelt
labels                            
0           33     0      1      0
1            1     0     19      1
2            0    17      0      0
3            0     0      0     13
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;full-pipeline-with-stocks&#34;&gt;Full pipeline with stocks&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;stock.csv&#39;)
df.head()
movements = df.iloc[:,1:].to_numpy()
companies = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.cluster import KMeans

# Create a normalizer: normalizer
normalizer = Normalizer()

# Create a KMeans model with 10 clusters: kmeans
kmeans = KMeans(n_clusters=10)

# Make a pipeline chaining normalizer and kmeans: pipeline
pipeline = make_pipeline(normalizer, kmeans)

# Fit pipeline to the daily price movements
pipeline.fit(movements)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Pipeline(memory=None,
         steps=[(&#39;normalizer&#39;, Normalizer(copy=True, norm=&#39;l2&#39;)),
                (&#39;kmeans&#39;,
                 KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;,
                        max_iter=300, n_clusters=10, n_init=10, n_jobs=None,
                        precompute_distances=&#39;auto&#39;, random_state=None,
                        tol=0.0001, verbose=0))],
         verbose=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pandas
import pandas as pd

# Predict the cluster labels: labels
labels = pipeline.predict(movements)

# Create a DataFrame aligning labels and companies: df
df = pd.DataFrame({&#39;labels&#39;: labels, &#39;companies&#39;: companies})

# Display df sorted by cluster label
print(df.sort_values(&#39;labels&#39;))

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    labels                           companies
18       0                       Goldman Sachs
55       0                         Wells Fargo
3        0                    American express
5        0                     Bank of America
26       0                      JPMorgan Chase
16       0                   General Electrics
15       0                                Ford
48       1                              Toyota
45       1                                Sony
7        1                               Canon
21       1                               Honda
34       1                          Mitsubishi
22       1                                  HP
28       2                           Coca Cola
41       2                       Philip Morris
9        2                   Colgate-Palmolive
25       2                   Johnson &amp;amp; Johnson
38       2                               Pepsi
27       2                      Kimberly-Clark
40       2                      Procter Gamble
49       3                               Total
52       3                            Unilever
43       3                                 SAP
42       3                   Royal Dutch Shell
37       3                            Novartis
6        3            British American Tobacco
39       3                              Pfizer
19       3                     GlaxoSmithKline
46       3                      Sanofi-Aventis
36       4                    Northrop Grumman
29       4                     Lookheed Martin
54       4                            Walgreen
4        4                              Boeing
44       5                        Schlumberger
53       5                       Valero Energy
12       5                             Chevron
35       5                            Navistar
10       5                      ConocoPhillips
8        5                         Caterpillar
57       5                               Exxon
51       6                   Texas instruments
50       6  Taiwan Semiconductor Manufacturing
0        6                               Apple
11       6                               Cisco
24       6                               Intel
33       6                           Microsoft
32       7                                  3M
2        7                              Amazon
13       7                   DuPont de Nemours
14       7                                Dell
17       7                     Google/Alphabet
23       7                                 IBM
47       7                            Symantec
58       7                               Xerox
30       7                          MasterCard
31       7                           McDonalds
20       7                          Home Depot
59       7                               Yahoo
56       8                            Wal-Mart
1        9                                 AIG
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Regression and classification in Scikit-Learn</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/regression/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/regression/</guid>
      <description>&lt;h3 id=&#34;linear-regression&#34;&gt;Linear regression&lt;/h3&gt;
&lt;p&gt;make predictions, visualize the model fit, and analyze the formula used to generate your fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import matplotlib.pyplot as plt
weather = pd.read_csv(&#39;weatherAUS.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;prepare-your-data&#34;&gt;Prepare your data&lt;/h4&gt;
&lt;p&gt;Do I have any null values in my dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;any(weather.isnull())
# For numpy
# np.any(np.isnan(mat))
# np.all(np.isfinite(mat))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#weather = weather.fillna(method=&#39;ffill&#39;) # inpute missing values
weather.dropna(inplace=True) # drop raws with null values, can loose important information
weather_train = weather.sample(n=100) 
weather_test = weather.sample(n=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;weather = weather_train
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression 
X = np.array(weather[&#39;Humidity9am&#39;]).reshape(-1,1)
y = weather[&#39;Humidity3pm&#39;]


# Create and fit your linear regression model
lm = LinearRegression()
lm.fit(X, y)

# Assign and print predictions
preds = lm.predict(X)

# Plot your fit to visualize your model
plt.scatter(X, y)
plt.plot(X, preds, color=&#39;red&#39;)
plt.show()

# Assign and print coefficient 
coef = lm.coef_
print(f&#39;Coefficient: {np.round(coef,2)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;regression_6_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Coefficient: [0.72]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that for every 1 unit of humidity in the morning, we can expect about 0.71 units of humidity in the afternoon. More practically, this information tells us that humidity drops about 29% from morning to afternoon!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# R-squared score
r2 = lm.score(X,y)
print(f&#39;R-squared score: {r2:.2f}&#39;)

# Mean squared error
from sklearn.metrics import mean_squared_error
preds = lm.predict(X)
mse = mean_squared_error(y, preds)
print(f&#39;Mean squared error: {mse:.2f}&#39;)

# Mean absolute error
from sklearn.metrics import mean_absolute_error
preds = lm.predict(X)
mae = mean_absolute_error(y, preds)
print(f&#39;Mean absolute error: {mae:.2f}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;R-squared score: 0.43
Mean squared error: 210.46
Mean absolute error: 11.54
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that our R-squared value tells us the percentage of the variance of y that X is responsible for. Which error metric would you recommend for this dataset? If you remember from when you plotted your model fit, there aren&amp;rsquo;t too many outliers, so mean squared error would be a good choice to go with!&lt;/p&gt;
&lt;h5 id=&#34;bias-variance-tradeoff&#34;&gt;Bias-Variance tradeoff&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import operator
from sklearn.preprocessing import PolynomialFeatures
polynomial_features= PolynomialFeatures(degree=10)

x_poly = polynomial_features.fit_transform(X)

lm2 = LinearRegression()
lm2.fit(x_poly, y)
preds2 = lm2.predict(x_poly)


plt.scatter(X, y)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(X,preds2), key=sort_axis)
x, y_poly_pred = zip(*sorted_zip)
plt.plot(x, y_poly_pred, color=&#39;m&#39;)
plt.show()


# R-squared score
r2 = lm2.score(x_poly, preds2)
print(f&#39;R-squared score: {r2:.2f}&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;regression_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;R-squared score: 1.00
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;logistic-regression&#34;&gt;Logistic regression&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
from sklearn import preprocessing 
def scaleX(df):
    x = df.values #returns a numpy array
    min_max_scaler = preprocessing.MinMaxScaler()
    x_scaled = min_max_scaler.fit_transform(x)
    return pd.DataFrame(x_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train = np.array(scaleX(weather_train.loc[:,[&#39;Humidity9am&#39;,&#39;Humidity3pm&#39;]])).reshape(-1,2)
y_train = np.array(weather_train[&#39;RainTomorrow&#39;])
y_train = (y_train==&#39;Yes&#39;).astype(int)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_test = np.array(scaleX(weather_test.loc[:,[&#39;Humidity9am&#39;,&#39;Humidity3pm&#39;]])).reshape(-1,2)
y_test = np.array(weather_test[&#39;RainTomorrow&#39;])
y_test = (y_test==&#39;Yes&#39;).astype(int)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression
# Create and fit your model
clf = LogisticRegression()
clf.fit(X_train, y_train)

# Compute and print the accuracy
acc = clf.score(X_test, y_test)
print(f&#39;Accuracy: {np.round(acc,2)}&#39;)

# Assign and print the coefficents
coefs = clf.coef_
print(f&#39;Coefficient: {np.round(coefs,2)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 0.83
Coefficient: [[0.05 6.81]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our features were normalized beforehand, we can look at the magnitude of our coefficients to tell us the importance of each independent variable. Here you can see the the second variable, Humidity3pm was much more important to our outcome than humidity from that morning. This is intuitive since we are trying to predict the rain for tomorrow!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate and output the confusion matrix
from sklearn.metrics import confusion_matrix
preds = clf.predict(X_test)
matrix = confusion_matrix(y_test, preds)
print(f&#39;Confusion matrix: {np.round(matrix,2)}&#39;)

# Compute and print the precision
from sklearn.metrics import precision_score
preds = clf.predict(X_test)
precision = precision_score(y_test, preds)
print(f&#39;Precision: {np.round(precision,2)}&#39;)

# Compute and print the recall
from sklearn.metrics import recall_score
preds = clf.predict(X_test)
recall = recall_score(y_test, preds)
print(f&#39;Recall: {np.round(recall,2)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Confusion matrix: [[7508  262]
 [1440  790]]
Precision: 0.75
Recall: 0.35
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see here that the precision of our rain prediction model is higher, meaning that we didn&amp;rsquo;t make too many Type I errors. However, there were plenty of Type II errors shown in the bottom-left quadrant of the confusion matrix. This is indicated further by the low recall score, meaning that there were plenty of rainy days that we missed out on.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
