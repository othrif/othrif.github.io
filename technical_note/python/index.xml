<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Othmane Rifki</title>
    <link>http://othrif.github.io/technical_note/python/</link>
      <atom:link href="http://othrif.github.io/technical_note/python/index.xml" rel="self" type="application/rss+xml" />
    <description>Othmane Rifki</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 12 Apr 2020 14:41:32 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Othmane Rifki</title>
      <link>http://othrif.github.io/technical_note/python/</link>
    </image>
    
    <item>
      <title>A tf-idf word-frequency array and mining Wikipedia</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/tf-idf/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/tf-idf/</guid>
      <description>&lt;p&gt;tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.&lt;/p&gt;
&lt;h3 id=&#34;tfidfvectorizer&#34;&gt;TfidfVectorizer&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt; from sklearn. It transforms a list of documents into a word frequency array, which it outputs as a csr_matrix. It has fit() and transform() methods like other sklearn objects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;documents = [&#39;cats say meow&#39;, &#39;dogs say woof&#39;, &#39;dogs chase cats&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Create a TfidfVectorizer: tfidf
tfidf = TfidfVectorizer() 

# Apply fit_transform to document: csr_mat
csr_mat = tfidf.fit_transform(documents)

# Print result of toarray() method
print(csr_mat.toarray())

# Get the words: words
words = tfidf.get_feature_names()

# Print words
print(words)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[0.51785612 0.         0.         0.68091856 0.51785612 0.        ]
 [0.         0.         0.51785612 0.         0.51785612 0.68091856]
 [0.51785612 0.68091856 0.51785612 0.         0.         0.        ]]
[&#39;cats&#39;, &#39;chase&#39;, &#39;dogs&#39;, &#39;meow&#39;, &#39;say&#39;, &#39;woof&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;mining-wikipedia&#34;&gt;Mining Wikipedia&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
from scipy.sparse import csr_matrix

df = pd.read_csv(&#39;wikipedia/wikipedia-vectors.csv&#39;, index_col=0)
articles = csr_matrix(df.transpose())
titles = list(df.columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import KMeans
from sklearn.pipeline import make_pipeline

# Create a TruncatedSVD instance: svd
svd = TruncatedSVD(n_components=50)

# Create a KMeans instance: kmeans
kmeans = KMeans(n_clusters=6)

# Create a pipeline: pipeline
pipeline = make_pipeline(svd, kmeans)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pandas
import pandas as pd

# Fit the pipeline to articles
pipeline.fit(articles)

# Calculate the cluster labels: labels
labels = pipeline.predict(articles)

# Create a DataFrame aligning labels and titles: df
df = pd.DataFrame({&#39;label&#39;: labels, &#39;article&#39;: titles})

# Display df sorted by cluster label
print(df.sort_values(&#39;label&#39;))

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    label                                        article
59      0                                    Adam Levine
50      0                                   Chad Kroeger
51      0                                     Nate Ruess
52      0                                     The Wanted
53      0                                   Stevie Nicks
58      0                                         Sepsis
55      0                                  Black Sabbath
56      0                                       Skrillex
57      0                          Red Hot Chili Peppers
54      0                                 Arctic Monkeys
21      1                             Michael Fassbender
28      1                                  Anne Hathaway
27      1                                 Dakota Fanning
26      1                                     Mila Kunis
25      1                                  Russell Crowe
24      1                                   Jessica Biel
23      1                           Catherine Zeta-Jones
22      1                              Denzel Washington
20      1                                 Angelina Jolie
29      1                               Jennifer Aniston
0       2                                       HTTP 404
1       2                                 Alexa Internet
2       2                              Internet Explorer
3       2                                    HTTP cookie
4       2                                  Google Search
5       2                                         Tumblr
6       2                    Hypertext Transfer Protocol
7       2                                  Social search
8       2                                        Firefox
9       2                                       LinkedIn
38      3                                         Neymar
37      3                                       Football
36      3              2014 FIFA World Cup qualification
35      3                Colombia national football team
39      3                                  Franck Ribéry
33      3                                 Radamel Falcao
32      3                                   Arsenal F.C.
31      3                              Cristiano Ronaldo
30      3                  France national football team
34      3                             Zlatan Ibrahimović
49      4                                       Lymphoma
48      4                                     Gabapentin
46      4                                     Prednisone
45      4                                    Hepatitis C
47      4                                          Fever
43      4                                       Leukemia
42      4                                    Doxycycline
41      4                                    Hepatitis B
40      4                                    Tonsillitis
44      4                                           Gout
19      5  2007 United Nations Climate Change Conference
10      5                                 Global warming
11      5       Nationally Appropriate Mitigation Action
12      5                                   Nigel Lawson
13      5                               Connie Hedegaard
14      5                                 Climate change
15      5                                 Kyoto Protocol
16      5                                        350.org
17      5  Greenhouse gas emissions by the United States
18      5  2010 United Nations Climate Change Conference
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Agglomerative Hiearchial clustering with scipy</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/clustering_hierarchy/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/clustering_hierarchy/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Begin with a seperate cluster, at each step merge the the two closest clusters&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
df = pd.read_csv(&#39;Grains/seeds.csv&#39;, header=None)
df = df.sample(n=50)
samples = df.iloc[:,:-1].to_numpy()
labels = df.iloc[:,-1].to_numpy()-1
label_names = np.array([&#39;Kama wheat&#39;, &#39;Rosa wheat&#39;, &#39;Canadian wheat&#39;])
varieties = label_names[labels]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt

# Calculate the linkage: mergings
mergings = linkage(samples, method=&#39;complete&#39;)

# Plot the dendrogram, using varieties as labels
fig, ax = plt.subplots(figsize=(20,5))
dendrogram(mergings,
           labels=varieties,
           leaf_rotation=90,
           leaf_font_size=12,
)
fig
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_hierarchy_3_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;hierarchical-clustering&#34;&gt;Hierarchical clustering&lt;/h3&gt;
&lt;p&gt;Several methods of clustering:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;complete&lt;/code&gt;: farthest elements of each cluster&lt;/li&gt;
&lt;li&gt;&lt;code&gt;single&lt;/code&gt;: closest elements of each cluster&lt;/li&gt;
&lt;li&gt;Note height of dendrogram is the distance between clusters, the merging point is the distance between them&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
import pandas as pd
from scipy.cluster.hierarchy import fcluster

# Use fcluster to extract labels: labels
labels = fcluster(mergings, 7, criterion=&#39;distance&#39;)

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({&#39;labels&#39;: labels, &#39;varieties&#39;: varieties})

# Create crosstab: ct
ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;varieties&#39;])

# Display ct
print(ct)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[2 1 3 3 1 2 3 3 3 3 1 3 3 3 3 2 2 2 2 1 3 3 3 3 1 2 3 1 3 3 1 3 1 3 3 2 3
 3 3 3 3 1 2 2 3 2 2 1 3 3]
[&#39;Canadian wheat&#39; &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Rosa wheat&#39;
 &#39;Canadian wheat&#39; &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Canadian wheat&#39;
 &#39;Canadian wheat&#39; &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39;
 &#39;Kama wheat&#39; &#39;Canadian wheat&#39; &#39;Canadian wheat&#39; &#39;Canadian wheat&#39;
 &#39;Canadian wheat&#39; &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39;
 &#39;Kama wheat&#39; &#39;Rosa wheat&#39; &#39;Canadian wheat&#39; &#39;Kama wheat&#39; &#39;Rosa wheat&#39;
 &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Rosa wheat&#39;
 &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Canadian wheat&#39; &#39;Kama wheat&#39; &#39;Kama wheat&#39;
 &#39;Kama wheat&#39; &#39;Kama wheat&#39; &#39;Rosa wheat&#39; &#39;Rosa wheat&#39; &#39;Canadian wheat&#39;
 &#39;Canadian wheat&#39; &#39;Kama wheat&#39; &#39;Canadian wheat&#39; &#39;Canadian wheat&#39;
 &#39;Rosa wheat&#39; &#39;Kama wheat&#39; &#39;Canadian wheat&#39;]
varieties  Canadian wheat  Kama wheat  Rosa wheat
labels                                           
1                       0           0          10
2                      12           0           0
3                       3          23           2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Classification in Scikit-Learn with Iris</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/classification_sklearn/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/classification_sklearn/</guid>
      <description>&lt;h3 id=&#34;exploratory-data-analysis-eda&#34;&gt;Exploratory data analysis (EDA)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use(&#39;ggplot&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris = datasets.load_iris()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.keys()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;type(iris.data), type(iris.target)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(numpy.ndarray, numpy.ndarray)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.data.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(150, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.target_names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&amp;lt;U10&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.feature_names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;sepal length (cm)&#39;,
 &#39;sepal width (cm)&#39;,
 &#39;petal length (cm)&#39;,
 &#39;petal width (cm)&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = iris.data
y = iris.target
df = pd.DataFrame(X, columns=iris.feature_names)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sepal length (cm)&lt;/th&gt;
      &lt;th&gt;sepal width (cm)&lt;/th&gt;
      &lt;th&gt;petal length (cm)&lt;/th&gt;
      &lt;th&gt;petal width (cm)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5.1&lt;/td&gt;
      &lt;td&gt;3.5&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4.9&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;4.7&lt;/td&gt;
      &lt;td&gt;3.2&lt;/td&gt;
      &lt;td&gt;1.3&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.6&lt;/td&gt;
      &lt;td&gt;3.1&lt;/td&gt;
      &lt;td&gt;1.5&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;3.6&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;_ = pd.plotting.scatter_matrix(df, c = y, figsize = [8,8], s = 150, marker = &#39;D&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;classification_sklearn_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Clustering with k-means</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/clustering_kmeans/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/clustering_kmeans/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import datasets
iris = datasets.load_iris()
data = iris[&#39;data&#39;]
features = iris[&#39;feature_names&#39;]
target = iris[&#39;target&#39;]
target_names = iris[&#39;target_names&#39;]

# map target labels to species names =&amp;gt; Ground Truth
species = target_names[target]
print(species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import KMeans
from sklearn.cluster import KMeans

# Create a KMeans instance with 3 clusters: model
model = KMeans(n_clusters=3)

# Fit model to points
model.fit(data)

# Determine the cluster labels of iris data: labels =&amp;gt; Prediction
labels = model.predict(data)

# can also use: labels = model.fit_predict(data)

# Calculate inertia: Measures how spread out the clusters are (lower is be!er) 
print(model.inertia_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;78.85144142614601
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pyplot
import matplotlib.pyplot as plt

# Assign the columns of new_points: xs and ys
xs = data[:,0]
ys = data[:,2]

fig, ax = plt.subplots()
# Make a scatter plot of xs and ys, using labels to define the colors
ax.scatter(xs, ys, c=labels, alpha=0.3)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,2]

# Make a scatter plot of centroids_x and centroids_y
ax.scatter(centroids_x, centroids_y, marker=&#39;D&#39;, s=100, color=&#39;r&#39;)
ax.set_title(&#39;K-means clustering of Iris dataset&#39;)
ax.set_xlabel(features[0])
ax.set_ylabel(features[2])
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_kmeans_3_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;compare-ground-truth-to-prediction&#34;&gt;Compare ground truth to prediction&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.DataFrame({&#39;labels&#39;: labels, &#39;species&#39;: species})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;species&#39;])
print(ct)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;species  setosa  versicolor  virginica
labels                                
0             0          48         14
1            50           0          0
2             0           2         36
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;what-is-the-best-clusters-to-choose&#34;&gt;What is the best clusters to choose?&lt;/h3&gt;
&lt;p&gt;The elbow rule, the point where the decrease slows down&lt;br&gt;
See below, &lt;strong&gt;3 is a good choice&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ks = range(1, 10)
inertias = []

for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(data)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
# Plot ks vs inertias
plt.plot(ks, inertias, &#39;-o&#39;)
plt.xlabel(&#39;number of clusters, k&#39;)
plt.ylabel(&#39;inertia&#39;)
plt.xticks(ks)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_kmeans_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;pipelines-with-kmeans-and-standardscaller&#34;&gt;Pipelines with Kmeans and StandardScaller&lt;/h2&gt;
&lt;h3 id=&#34;standard-scaller&#34;&gt;Standard scaller&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in kmeans: feature variance = feature influence&lt;/li&gt;
&lt;li&gt;&lt;code&gt;StandardScaller&lt;/code&gt; transforms each feature to have mean 0 and variance 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;fish.csv&#39;, header=None) # prevent first row from becoming header
samples = df.iloc[:,1:].to_numpy()
species = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Create scaler: scaler
scaler = StandardScaler()

# Create KMeans instance: kmeans
kmeans = KMeans(n_clusters=4)

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, kmeans)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit the pipeline to samples
pipeline.fit(samples)

# Calculate the cluster labels: labels
labels = pipeline.predict(samples)

# Create a DataFrame with labels and species as columns: df
df = pd.DataFrame({&#39;labels&#39;:labels, &#39;species&#39;:species})

# Create crosstab: ct
ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;species&#39;])

# Display ct
print(ct)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;species  Bream  Pike  Roach  Smelt
labels                            
0           33     0      1      0
1            1     0     19      1
2            0    17      0      0
3            0     0      0     13
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;full-pipeline-with-stocks&#34;&gt;Full pipeline with stocks&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;stock.csv&#39;)
df.head()
movements = df.iloc[:,1:].to_numpy()
companies = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.cluster import KMeans

# Create a normalizer: normalizer
normalizer = Normalizer()

# Create a KMeans model with 10 clusters: kmeans
kmeans = KMeans(n_clusters=10)

# Make a pipeline chaining normalizer and kmeans: pipeline
pipeline = make_pipeline(normalizer, kmeans)

# Fit pipeline to the daily price movements
pipeline.fit(movements)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Pipeline(memory=None,
         steps=[(&#39;normalizer&#39;, Normalizer(copy=True, norm=&#39;l2&#39;)),
                (&#39;kmeans&#39;,
                 KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;,
                        max_iter=300, n_clusters=10, n_init=10, n_jobs=None,
                        precompute_distances=&#39;auto&#39;, random_state=None,
                        tol=0.0001, verbose=0))],
         verbose=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pandas
import pandas as pd

# Predict the cluster labels: labels
labels = pipeline.predict(movements)

# Create a DataFrame aligning labels and companies: df
df = pd.DataFrame({&#39;labels&#39;: labels, &#39;companies&#39;: companies})

# Display df sorted by cluster label
print(df.sort_values(&#39;labels&#39;))

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    labels                           companies
42       0                   Royal Dutch Shell
52       0                            Unilever
53       0                       Valero Energy
23       0                                 IBM
49       0                               Total
47       0                            Symantec
13       0                   DuPont de Nemours
12       0                             Chevron
46       0                      Sanofi-Aventis
19       0                     GlaxoSmithKline
10       0                      ConocoPhillips
8        0                         Caterpillar
35       0                            Navistar
6        0            British American Tobacco
37       0                            Novartis
44       0                        Schlumberger
57       0                               Exxon
39       0                              Pfizer
32       0                                  3M
43       0                                 SAP
36       1                    Northrop Grumman
29       1                     Lookheed Martin
4        1                              Boeing
9        2                   Colgate-Palmolive
40       2                      Procter Gamble
25       2                   Johnson &amp;amp; Johnson
27       2                      Kimberly-Clark
1        3                                 AIG
3        3                    American express
5        3                     Bank of America
55       3                         Wells Fargo
18       3                       Goldman Sachs
58       3                               Xerox
26       3                      JPMorgan Chase
15       3                                Ford
16       3                   General Electrics
20       4                          Home Depot
45       4                                Sony
34       4                          Mitsubishi
7        4                               Canon
48       4                              Toyota
54       4                            Walgreen
21       4                               Honda
0        5                               Apple
33       5                           Microsoft
22       5                                  HP
14       5                                Dell
11       5                               Cisco
24       6                               Intel
50       6  Taiwan Semiconductor Manufacturing
51       6                   Texas instruments
38       7                               Pepsi
31       7                           McDonalds
28       7                           Coca Cola
56       7                            Wal-Mart
41       7                       Philip Morris
17       8                     Google/Alphabet
2        8                              Amazon
59       8                               Yahoo
30       9                          MasterCard
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Clustering with k-means</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/clustering_tsne/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/clustering_tsne/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;t-SNE = “t-distributed stochastic neighbor embedding”&lt;/li&gt;
&lt;li&gt;Maps samples to 2D space (or 3D)&lt;/li&gt;
&lt;li&gt;only has &lt;code&gt;fit_transform&lt;/code&gt; method, have to repeat everytime&lt;/li&gt;
&lt;li&gt;Try &lt;code&gt;learning_rate&lt;/code&gt; between 50 and 200&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iris-dataset&#34;&gt;Iris dataset&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import datasets
iris = datasets.load_iris()
samples = iris[&#39;data&#39;]
features = iris[&#39;feature_names&#39;]
species = iris[&#39;target&#39;]
target_names = iris[&#39;target_names&#39;]

# map target labels to species names =&amp;gt; Ground Truth
#species = target_names[species]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

model = TSNE(learning_rate=100)

transformed = model.fit_transform(samples)
xs=transformed[:,0]
ys=transformed[:,1]
plt.scatter(xs, ys, c=species)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_tsne_4_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;stock-price&#34;&gt;Stock price&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;stock.csv&#39;)
df.head()
movements = df.iloc[:,1:].to_numpy()
companies = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;normalize-data-using-a-transform&#34;&gt;Normalize data using a transform&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.preprocessing import Normalizer

# Create a normalizer: normalizer
normalizer = Normalizer()

normalized_movements = normalizer.transform(movements)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import TSNE
from sklearn.manifold import TSNE

# Create a TSNE instance: model
model = TSNE(learning_rate=50)

# Apply fit_transform to normalized_movements: tsne_features
tsne_features = model.fit_transform(normalized_movements)

# Select the 0th feature: xs
xs = tsne_features[:,0]

# Select the 1th feature: ys
ys = tsne_features[:,1]

# Scatter plot
fig, ax = plt.subplots(figsize=(20,20))
ax.scatter(xs, ys, alpha=0.5)

# Annotate the points
for x, y, company in zip(xs, ys, companies):
    ax.annotate(company, (x, y), fontsize=10, alpha=0.75)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;clustering_tsne_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dimension reduction with PCA transformation</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/pca_tranf/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/pca_tranf/</guid>
      <description>&lt;p&gt;PCA: Principle Component Analysis: step1 &amp;gt; decorrelation, step2 &amp;gt; reduce dimension&lt;/p&gt;
&lt;p&gt;1- Decorrelation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rotates data samples to be aligned with axes&lt;/li&gt;
&lt;li&gt;mean ~ 0&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;decorrelation&#34;&gt;Decorrelation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
df = pd.read_csv(&#39;Grains/seeds-width-vs-length.csv&#39;, header=None)
grains = df.to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Assign the 0th column of grains: width
width = grains[:, 0]

# Assign the 1st column of grains: length
length = grains[:, 1]

# Scatter plot width vs length
plt.scatter(width, length)
plt.axis(&#39;equal&#39;)
plt.show()

# Calculate the Pearson correlation
correlation, pvalue = pearsonr(width, length)

# Display the correlation
print(f&#39;Pearson Correlation before-PCA: {correlation}&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;pca_tranf_4_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Pearson Correlation before-PCA: 0.8604149377143466
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import PCA
from sklearn.decomposition import PCA

# Create PCA instance: model
model = PCA()

# Apply the fit_transform method of model to grains: pca_features
pca_features = model.fit_transform(grains)

# Assign 0th column of pca_features: xs
xs = pca_features[:,0]

# Assign 1st column of pca_features: ys
ys = pca_features[:,1]

# Scatter plot xs vs ys
plt.scatter(xs, ys)
plt.axis(&#39;equal&#39;)
plt.show()

# Calculate the Pearson correlation of xs and ys
correlation, pvalue = pearsonr(xs, ys)

# Display the correlation
print(f&#39;Pearson Correlation after-PCA: {correlation}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;pca_tranf_5_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Pearson Correlation after-PCA: 2.5478751053409354e-17
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;principal-component-visualized&#34;&gt;Principal component visualized&lt;/h3&gt;
&lt;p&gt;Principle component: direction in which the sample varies the most&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make a scatter plot of the untransformed points
plt.scatter(grains[:,0], grains[:,1])

# Create a PCA instance: model
model = PCA()

# Fit model to points
model.fit(grains)

# Get the mean of the grain samples: mean
mean = model.mean_

# Get the first principal component: first_pc
first_pc = model.components_[0,:]

# Plot first_pc as an arrow, starting at mean
plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color=&#39;red&#39;, width=0.02)

# Keep axes on same scale
plt.axis(&#39;equal&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;pca_tranf_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;intrinsic-dimension&#34;&gt;Intrinsic dimension&lt;/h3&gt;
&lt;p&gt;Intrinsic dimension = number of PCA features with significant variance&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;fish.csv&#39;, header=None) # prevent first row from becoming header
samples = df.iloc[:,1:].to_numpy()
species = df.iloc[:,0].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt

# Create scaler: scaler
scaler = StandardScaler()

# Create a PCA instance: pca
pca = PCA()

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, pca)

# Fit the pipeline to &#39;samples&#39;
pipeline.fit(samples)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xlabel(&#39;PCA feature&#39;)
plt.ylabel(&#39;variance&#39;)
plt.xticks(features)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;pca_tranf_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since PCA features 0 and 1 have significant variance, the intrinsic dimension of this dataset appears to be 2.&lt;/p&gt;
&lt;h3 id=&#34;dimension-reduction&#34;&gt;Dimension reduction&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(samples)
scaled_samples = scaler.transform(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(f&#39;Shape of data pre-pca: {scaled_samples.shape}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shape of data (85, 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import PCA
from sklearn.decomposition import PCA

# Create a PCA model with 2 components: pca
pca = PCA(n_components=2)

# Fit the PCA instance to the scaled samples
pca.fit(scaled_samples)

# Transform the scaled samples: pca_features
pca_features = pca.transform(scaled_samples)

# Print the shape of pca_features
print(f&#39;Shape of data  post-pca:{pca_features.shape}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shape of data  post-pca:(85, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(pca_features[:,0], pca_features[:,1])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;pca_tranf_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dimensionality reduction with NMF and recommender system</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/nmf/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/nmf/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;NMF = &amp;ldquo;non-negative matrix factorization&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Dimension reduction technique&lt;/li&gt;
&lt;li&gt;NMF models are interpretable (unlike PCA)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nmf-applied-to-wikipedia-articles&#34;&gt;NMF applied to Wikipedia articles&lt;/h3&gt;
&lt;p&gt;apply NMF, this time using the tf-idf word-frequency array of Wikipedia articles, given as a csr matrix &lt;code&gt;articles&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
from scipy.sparse import csr_matrix

df = pd.read_csv(&#39;wikipedia/wikipedia-vectors.csv&#39;, index_col=0)
words = [x.strip(&#39;\n&#39;).split(&#39; &#39;)[0] for x in open(&#39;wikipedia/wikipedia-vocabulary-utf8.txt&#39;).readlines()]
articles = csr_matrix(df.transpose())
titles = list(df.columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import NMF
from sklearn.decomposition import NMF

# Create an NMF instance: model
model = NMF(n_components=6)

# Fit the model to articles
model.fit(articles)

# Transform the articles: nmf_features
nmf_features = model.transform(articles)

# Print the NMF features
print(articles.shape)
print(nmf_features.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(60, 13125)
(60, 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pandas
import pandas as pd

# Create a pandas DataFrame: df
df = pd.DataFrame(nmf_features, index=titles)

# Print the row for &#39;Anne Hathaway&#39;
print(df.loc[&#39;Anne Hathaway&#39;])

# Print the row for &#39;Denzel Washington&#39;
print(df.loc[&#39;Denzel Washington&#39;])

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    0.003846
1    0.000000
2    0.000000
3    0.575663
4    0.000000
5    0.000000
Name: Anne Hathaway, dtype: float64
0    0.000000
1    0.005601
2    0.000000
3    0.422345
4    0.000000
5    0.000000
Name: Denzel Washington, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component.&lt;/p&gt;
&lt;h4 id=&#34;nmf-learns-topics-of-documents&#34;&gt;NMF learns topics of documents&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import pandas
import pandas as pd

# Create a DataFrame: components_df
components_df = pd.DataFrame(model.components_, columns=words)

# Print the shape of the DataFrame
print(components_df.shape)

# Select row 3: component
component = components_df.iloc[3,:]

# Print result of nlargest
print(component.nlargest())

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(6, 13125)
film       0.627908
award      0.253144
starred    0.245297
role       0.211462
actress    0.186407
Name: 3, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a moment to recognise the topics that the articles about Anne Hathaway and Denzel Washington have in common!&lt;/p&gt;
&lt;h3 id=&#34;recommender-system-which-articles-are-similar-to-cristiano-ronaldo&#34;&gt;Recommender system: which articles are similar to &amp;lsquo;Cristiano Ronaldo&amp;rsquo;?&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Perform the necessary imports
import pandas as pd
from sklearn.preprocessing import normalize

# Normalize the NMF features: norm_features
norm_features = normalize(nmf_features)

# Create a DataFrame: df
df = pd.DataFrame(norm_features, index=titles)

# Select the row corresponding to &#39;Cristiano Ronaldo&#39;: article
article = df.loc[&#39;Cristiano Ronaldo&#39;]

# Compute the dot products: similarities
similarities = df.dot(article)

# Display those with the largest cosine similarity
print(similarities.nlargest())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Cristiano Ronaldo                1.000000
Franck Ribéry                    0.999972
Radamel Falcao                   0.999942
Zlatan Ibrahimović               0.999942
France national football team    0.999923
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;led-digits-dataset&#34;&gt;LED digits dataset&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
from matplotlib import pyplot as plt

samples = pd.read_csv(&#39;lcd-digits.csv&#39;, header=None ).to_numpy()

def show_as_image(sample):
    bitmap = sample.reshape((13, 8))
    plt.figure()
    plt.imshow(bitmap, cmap=&#39;gray&#39;, interpolation=&#39;nearest&#39;)
    plt.colorbar()
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;digit = samples[0,:]
show_as_image(digit)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;nmf_14_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;nmf-learns-the-parts-of-images&#34;&gt;NMF learns the parts of images&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import NMF
from sklearn.decomposition import NMF

# Create an NMF model: model
model = NMF(n_components = 7) #7 is the number of cells in an LED display

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
for component in model.components_:
    show_as_image(component)

# Assign the 0th row of features: digit_features
digit_features = features[0,:]

# Print digit_features
print(digit_features)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;nmf_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nmf_16_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nmf_16_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nmf_16_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nmf_16_4.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nmf_16_5.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nmf_16_6.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[4.76823559e-01 0.00000000e+00 0.00000000e+00 5.90605054e-01
 4.81559442e-01 0.00000000e+00 7.37557191e-16]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a moment to look through the plots and notice how NMF has expressed the digit as a sum of the components!
7 = 0.47 * feature1 + 0.59 * feature4 + 0.48 * feature5&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Regression and classification in Scikit-Learn</title>
      <link>http://othrif.github.io/technical_note/python/sklearn/regression/</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/regression/</guid>
      <description>&lt;h3 id=&#34;linear-regression&#34;&gt;Linear regression&lt;/h3&gt;
&lt;p&gt;make predictions, visualize the model fit, and analyze the formula used to generate your fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import matplotlib.pyplot as plt
weather = pd.read_csv(&#39;weatherAUS.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;prepare-your-data&#34;&gt;Prepare your data&lt;/h4&gt;
&lt;p&gt;Do I have any null values in my dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;any(weather.isnull())
# For numpy
# np.any(np.isnan(mat))
# np.all(np.isfinite(mat))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#weather = weather.fillna(method=&#39;ffill&#39;) # inpute missing values
weather.dropna(inplace=True) # drop raws with null values, can loose important information
weather_train = weather.sample(n=100) 
weather_test = weather.sample(n=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;weather = weather_train
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression 
X = np.array(weather[&#39;Humidity9am&#39;]).reshape(-1,1)
y = weather[&#39;Humidity3pm&#39;]


# Create and fit your linear regression model
lm = LinearRegression()
lm.fit(X, y)

# Assign and print predictions
preds = lm.predict(X)

# Plot your fit to visualize your model
plt.scatter(X, y)
plt.plot(X, preds, color=&#39;red&#39;)
plt.show()

# Assign and print coefficient 
coef = lm.coef_
print(f&#39;Coefficient: {np.round(coef,2)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;regression_6_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Coefficient: [0.72]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that for every 1 unit of humidity in the morning, we can expect about 0.71 units of humidity in the afternoon. More practically, this information tells us that humidity drops about 29% from morning to afternoon!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# R-squared score
r2 = lm.score(X,y)
print(f&#39;R-squared score: {r2:.2f}&#39;)

# Mean squared error
from sklearn.metrics import mean_squared_error
preds = lm.predict(X)
mse = mean_squared_error(y, preds)
print(f&#39;Mean squared error: {mse:.2f}&#39;)

# Mean absolute error
from sklearn.metrics import mean_absolute_error
preds = lm.predict(X)
mae = mean_absolute_error(y, preds)
print(f&#39;Mean absolute error: {mae:.2f}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;R-squared score: 0.43
Mean squared error: 210.46
Mean absolute error: 11.54
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that our R-squared value tells us the percentage of the variance of y that X is responsible for. Which error metric would you recommend for this dataset? If you remember from when you plotted your model fit, there aren&amp;rsquo;t too many outliers, so mean squared error would be a good choice to go with!&lt;/p&gt;
&lt;h5 id=&#34;bias-variance-tradeoff&#34;&gt;Bias-Variance tradeoff&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import operator
from sklearn.preprocessing import PolynomialFeatures
polynomial_features= PolynomialFeatures(degree=10)

x_poly = polynomial_features.fit_transform(X)

lm2 = LinearRegression()
lm2.fit(x_poly, y)
preds2 = lm2.predict(x_poly)


plt.scatter(X, y)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(X,preds2), key=sort_axis)
x, y_poly_pred = zip(*sorted_zip)
plt.plot(x, y_poly_pred, color=&#39;m&#39;)
plt.show()


# R-squared score
r2 = lm2.score(x_poly, preds2)
print(f&#39;R-squared score: {r2:.2f}&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;regression_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;R-squared score: 1.00
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;logistic-regression&#34;&gt;Logistic regression&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
from sklearn import preprocessing 
def scaleX(df):
    x = df.values #returns a numpy array
    min_max_scaler = preprocessing.MinMaxScaler()
    x_scaled = min_max_scaler.fit_transform(x)
    return pd.DataFrame(x_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train = np.array(scaleX(weather_train.loc[:,[&#39;Humidity9am&#39;,&#39;Humidity3pm&#39;]])).reshape(-1,2)
y_train = np.array(weather_train[&#39;RainTomorrow&#39;])
y_train = (y_train==&#39;Yes&#39;).astype(int)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_test = np.array(scaleX(weather_test.loc[:,[&#39;Humidity9am&#39;,&#39;Humidity3pm&#39;]])).reshape(-1,2)
y_test = np.array(weather_test[&#39;RainTomorrow&#39;])
y_test = (y_test==&#39;Yes&#39;).astype(int)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression
# Create and fit your model
clf = LogisticRegression()
clf.fit(X_train, y_train)

# Compute and print the accuracy
acc = clf.score(X_test, y_test)
print(f&#39;Accuracy: {np.round(acc,2)}&#39;)

# Assign and print the coefficents
coefs = clf.coef_
print(f&#39;Coefficient: {np.round(coefs,2)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 0.83
Coefficient: [[0.05 6.81]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our features were normalized beforehand, we can look at the magnitude of our coefficients to tell us the importance of each independent variable. Here you can see the the second variable, Humidity3pm was much more important to our outcome than humidity from that morning. This is intuitive since we are trying to predict the rain for tomorrow!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate and output the confusion matrix
from sklearn.metrics import confusion_matrix
preds = clf.predict(X_test)
matrix = confusion_matrix(y_test, preds)
print(f&#39;Confusion matrix: {np.round(matrix,2)}&#39;)

# Compute and print the precision
from sklearn.metrics import precision_score
preds = clf.predict(X_test)
precision = precision_score(y_test, preds)
print(f&#39;Precision: {np.round(precision,2)}&#39;)

# Compute and print the recall
from sklearn.metrics import recall_score
preds = clf.predict(X_test)
recall = recall_score(y_test, preds)
print(f&#39;Recall: {np.round(recall,2)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Confusion matrix: [[7508  262]
 [1440  790]]
Precision: 0.75
Recall: 0.35
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see here that the precision of our rain prediction model is higher, meaning that we didn&amp;rsquo;t make too many Type I errors. However, there were plenty of Type II errors shown in the bottom-left quadrant of the confusion matrix. This is indicated further by the low recall score, meaning that there were plenty of rainy days that we missed out on.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://othrif.github.io/technical_note/python/sklearn/wikipedia/preprocessing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://othrif.github.io/technical_note/python/sklearn/wikipedia/preprocessing/</guid>
      <description>&lt;p&gt;To preprocess &lt;code&gt;wikipedia-vectors.csv&lt;/code&gt; into the format in which you used it in the exercises, you have to take its transpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{python}&#34;&gt;import pandas as pd
from scipy.sparse import csr_matrix

df = pd.read_csv(&#39;wikipedia-vectors.csv&#39;, index_col=0)
articles = csr_matrix(df.transpose())
titles = list(df.columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reason for taking this transpose is that without it, there would be 13,000 columns (corresponding to the 13,000 words in the file), which is a lot of columns for a CSV to have.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
