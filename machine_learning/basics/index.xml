<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Othmane Rifki</title>
    <link>https://othrif.github.io/machine_learning/basics/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Apr 2020 14:41:32 +0200</lastBuildDate>
    
        <atom:link href="https://othrif.github.io/machine_learning/basics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Activation functions: sigmoid, softmax, tanh, ReLU</title>
      <link>https://othrif.github.io/machine_learning/basics/softmax_calc.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/basics/softmax_calc.html</guid>
      <description>Sigmoid Sigmoid or logistic function models the probability that sample x belongs to the positive class in a binary classification task.
Given $z = w_0x_0 + w_1x_1 + \cdots + w_mx_m = \sum_{i=0}^m w_ix_i = w^Tx $
$\phi_\text{logistic} (z) = \frac{1}{1 + e^{-z}}$
import numpy as np X = np.array([1, 1.4, 2.5]) ## first value must be 1 w = np.array([0.4, 0.3, 0.5]) def net_input(X, w): return np.dot(X, w) def logistic(z): return 1.</description>
    </item>
    
    <item>
      <title>Calculate error in ensemble</title>
      <link>https://othrif.github.io/machine_learning/basics/ensemble_proba.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/basics/ensemble_proba.html</guid>
      <description>Error rate of the ensemble Assume you have $n$ classifiers and each has an error $\epsilon$, the combined error of the ensemble is smaller than the error in the individual classifiers.
from scipy.special import comb import math def ensemble_error(n_classifier, error): k_start = int(math.ceil(n_classifier / 2.)) probs = [comb(n_classifier, k) * error**k * (1-error)**(n_classifier - k) for k in range(k_start, n_classifier + 1)] return sum(probs) ensemble_error(11, 0.25) 0.03432750701904297  Ensemble vs base error rates The base error should be less than 0.</description>
    </item>
    
    <item>
      <title>Check from CLI if Jupyter is running and kill it</title>
      <link>https://othrif.github.io/machine_learning/basics/dl_server.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/basics/dl_server.html</guid>
      <description>Check what is running and kill it For tensorflow and pytorch, you need gcc8 to compile code which is not the system default. Do the following to use gcc8:
jupyter notebook list jupyter notebook stop 8888 </description>
    </item>
    
    <item>
      <title>Check package versions</title>
      <link>https://othrif.github.io/machine_learning/basics/versions.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/basics/versions.html</guid>
      <description>Check deep learning library versions !python -c &amp;#34;import tensorflow as tf; print(&amp;#39;Tensorflow:&amp;#39;, tf.__version__)&amp;#34; !python -c &amp;#34;import torch; print(&amp;#39;Pytorch:&amp;#39;, torch.__version__)&amp;#34; Tensorflow: 2.3.1 Pytorch: 1.7.0  </description>
    </item>
    
    <item>
      <title>Multi-layer perceptron (MLP) from scratch</title>
      <link>https://othrif.github.io/machine_learning/basics/mlp.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/basics/mlp.html</guid>
      <description>MLP API import numpy as np import sys class NeuralNetMLP(object): &amp;#34;&amp;#34;&amp;#34; Feedforward neural network / Multi-layer perceptron classifier. Parameters ------------ n_hidden : int (default: 30) Number of hidden units. l2 : float (default: 0.) Lambda value for L2-regularization. No regularization if l2=0. (default) epochs : int (default: 100) Number of passes over the training set. eta : float (default: 0.001) Learning rate. shuffle : bool (default: True) Shuffles training data every epoch if True to prevent circles.</description>
    </item>
    
    <item>
      <title>Profiling GPU with Nvidia tools</title>
      <link>https://othrif.github.io/machine_learning/basics/cuda_profiling.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/basics/cuda_profiling.html</guid>
      <description>Using Nsight systems To install Nsight systems:
 Download from Nvidia Make executable with chmod a+x NsightSystems-linux-public-2020.5.1.85-5ee086b.run install with ./NsightSystems-linux-public-2020.5.1.85-5ee086b.run Add to path export PATH=&amp;quot;/home/othrif/setup/nsight-systems-2020.5.1/target-linux-x64:$PATH&amp;quot; Run profiling tool with: nsys profile ./program  Using nvida-smi nvidia-smi -l 1 &amp;gt;&amp;gt; gpu-stats.out </description>
    </item>
    
    <item>
      <title>Standard includes</title>
      <link>https://othrif.github.io/machine_learning/basics/includes.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/basics/includes.html</guid>
      <description>Basic includes for TensorFlow import os import pathlib import matplotlib.pyplot as plt import numpy as np import seaborn as sns import tensorflow as tf from tensorflow.keras.layers.experimental import preprocessing from tensorflow.keras import layers from tensorflow.keras import models from IPython import display # Set seed for experiment reproducibility seed = 42 tf.random.set_seed(seed) np.random.seed(seed) </description>
    </item>
    
  </channel>
</rss>