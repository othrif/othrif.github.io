<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Othmane Rifki</title>
    <link>https://othrif.github.io/machine_learning/scikitlearn/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Apr 2020 14:41:32 +0200</lastBuildDate>
    
        <atom:link href="https://othrif.github.io/machine_learning/scikitlearn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Bagging in ensemble methods</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/bagging_ensemble.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/bagging_ensemble.html</guid>
      <description>import pandas as pd df_wine = pd.read_csv(&amp;#39;https://archive.ics.uci.edu/ml/&amp;#39; &amp;#39;machine-learning-databases/wine/wine.data&amp;#39;, header=None) df_wine.columns = [&amp;#39;Class label&amp;#39;, &amp;#39;Alcohol&amp;#39;, &amp;#39;Malic acid&amp;#39;, &amp;#39;Ash&amp;#39;, &amp;#39;Alcalinity of ash&amp;#39;, &amp;#39;Magnesium&amp;#39;, &amp;#39;Total phenols&amp;#39;, &amp;#39;Flavanoids&amp;#39;, &amp;#39;Nonflavanoid phenols&amp;#39;, &amp;#39;Proanthocyanins&amp;#39;, &amp;#39;Color intensity&amp;#39;, &amp;#39;Hue&amp;#39;, &amp;#39;OD280/OD315 of diluted wines&amp;#39;, &amp;#39;Proline&amp;#39;] # if the Wine dataset is temporarily unavailable from the # UCI machine learning repository, un-comment the following line # of code to load the dataset from a local path: # df_wine = pd.read_csv(&amp;#39;wine.data&amp;#39;, header=None) # drop 1 class df_wine = df_wine[df_wine[&amp;#39;Class label&amp;#39;] !</description>
    </item>
    
    <item>
      <title>Boosting in ensemble methods</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/boosting_ensemble.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/boosting_ensemble.html</guid>
      <description>import pandas as pd df_wine = pd.read_csv(&amp;#39;https://archive.ics.uci.edu/ml/&amp;#39; &amp;#39;machine-learning-databases/wine/wine.data&amp;#39;, header=None) df_wine.columns = [&amp;#39;Class label&amp;#39;, &amp;#39;Alcohol&amp;#39;, &amp;#39;Malic acid&amp;#39;, &amp;#39;Ash&amp;#39;, &amp;#39;Alcalinity of ash&amp;#39;, &amp;#39;Magnesium&amp;#39;, &amp;#39;Total phenols&amp;#39;, &amp;#39;Flavanoids&amp;#39;, &amp;#39;Nonflavanoid phenols&amp;#39;, &amp;#39;Proanthocyanins&amp;#39;, &amp;#39;Color intensity&amp;#39;, &amp;#39;Hue&amp;#39;, &amp;#39;OD280/OD315 of diluted wines&amp;#39;, &amp;#39;Proline&amp;#39;] # if the Wine dataset is temporarily unavailable from the # UCI machine learning repository, un-comment the following line # of code to load the dataset from a local path: # df_wine = pd.read_csv(&amp;#39;wine.data&amp;#39;, header=None) # drop 1 class df_wine = df_wine[df_wine[&amp;#39;Class label&amp;#39;] !</description>
    </item>
    
    <item>
      <title>Combining classifiers via majority vote</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/majoirty_vote.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/majoirty_vote.html</guid>
      <description>Implementing a majority vote classifier There are two ways to determine the majority vote classification using:
 Class label Class probability  Class label import numpy as np np.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6])) 1  Class probability ex = np.array([[0.9, 0.1], [0.8, 0.2], [0.4, 0.6]]) p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6]) p array([0.58, 0.42])  np.argmax(p) 0  Majority classifier class from sklearn.base import BaseEstimator from sklearn.base import ClassifierMixin from sklearn.</description>
    </item>
    
    <item>
      <title>Confusion matrix, ROC curve, evaluation metrics</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/evaluation_metrics.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/evaluation_metrics.html</guid>
      <description>Basic pipeline import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split df = pd.read_csv(&amp;#39;https://archive.ics.uci.edu/ml/&amp;#39; &amp;#39;machine-learning-databases&amp;#39; &amp;#39;/breast-cancer-wisconsin/wdbc.data&amp;#39;, header=None) X = df.loc[:, 2:].values y = df.loc[:, 1].values le = LabelEncoder() y = le.fit_transform(y) X_train, X_test, y_train, y_test = \ train_test_split(X, y, test_size=0.20, stratify=y, random_state=1) Pipeline from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC pipe_svc = make_pipeline(StandardScaler(), SVC(random_state=1)) Confusion matrix from sklearn.metrics import confusion_matrix pipe_svc.</description>
    </item>
    
    <item>
      <title>Decision Boundary</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/decision_boundary.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/decision_boundary.html</guid>
      <description>from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt import numpy as np def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # setup marker generator and color map markers = (&amp;#39;s&amp;#39;, &amp;#39;x&amp;#39;, &amp;#39;o&amp;#39;, &amp;#39;^&amp;#39;, &amp;#39;v&amp;#39;) colors = (&amp;#39;red&amp;#39;, &amp;#39;blue&amp;#39;, &amp;#39;lightgreen&amp;#39;, &amp;#39;gray&amp;#39;, &amp;#39;cyan&amp;#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.</description>
    </item>
    
    <item>
      <title>Decision tree and random forest</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/decision_tree.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/decision_tree.html</guid>
      <description>Decision tree from sklearn.tree import DecisionTreeClassifier tree_model = DecisionTreeClassifier(criterion=&amp;#39;gini&amp;#39;, max_depth=4, random_state=1) Basic pipeline Collect data from sklearn import datasets import numpy as np iris = datasets.load_iris() X = iris.data[:, [2, 3]] y = iris.target print(&amp;#39;Class labels:&amp;#39;, np.unique(y)) Class labels: [0 1 2]  Split data from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=1, stratify=y) print(&amp;#39;Labels count in y:&amp;#39;, np.bincount(y)) print(&amp;#39;Labels count in y_train:&amp;#39;, np.bincount(y_train)) print(&amp;#39;Labels count in y_test:&amp;#39;, np.</description>
    </item>
    
    <item>
      <title>Feature scaling</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/scaling.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/scaling.html</guid>
      <description>Standardize vs Normalize import numpy as np ex = np.array([0, 1, 2, 3, 4, 5]) # standardize print(&amp;#39;standardized:&amp;#39;, (ex - ex.mean()) / ex.std()) # normalize print(&amp;#39;normalized:&amp;#39;, (ex - ex.min()) / (ex.max() - ex.min())) standardized: [-1.46385011 -0.87831007 -0.29277002 0.29277002 0.87831007 1.46385011] normalized: [0. 0.2 0.4 0.6 0.8 1. ]  X_train = np.random.randn(200, 2) X_test = np.random.randn(200, 2) Standardize from sklearn.preprocessing import MinMaxScaler mms = MinMaxScaler() X_train_norm = mms.fit_transform(X_train) X_test_norm = mms.</description>
    </item>
    
    <item>
      <title>K-fold cross-validation</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/kfold.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/kfold.html</guid>
      <description>Basic pipeline import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split df = pd.read_csv(&amp;#39;https://archive.ics.uci.edu/ml/&amp;#39; &amp;#39;machine-learning-databases&amp;#39; &amp;#39;/breast-cancer-wisconsin/wdbc.data&amp;#39;, header=None) X = df.loc[:, 2:].values y = df.loc[:, 1].values le = LabelEncoder() y = le.fit_transform(y) X_train, X_test, y_train, y_test = \ train_test_split(X, y, test_size=0.20, stratify=y, random_state=1) Combining transformers and estimators in a pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline pipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression(random_state=1, solver=&amp;#39;lbfgs&amp;#39;)) pipe_lr.</description>
    </item>
    
    <item>
      <title>K-nearest neighbors (knn)</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/knn.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/knn.html</guid>
      <description>Basic pipeline from sklearn import datasets import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler # Collect data iris = datasets.load_iris() X = iris.data[:, [2, 3]] y = iris.target # Split data X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=1, stratify=y) # Standardize features sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train) X_test_std = sc.transform(X_test) knn Algorithm from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=5, p=2, metric=&amp;#39;minkowski&amp;#39;) knn.</description>
    </item>
    
    <item>
      <title>Learning and validation curves</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/validation_curves.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/validation_curves.html</guid>
      <description>Basic pipeline import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split df = pd.read_csv(&amp;#39;https://archive.ics.uci.edu/ml/&amp;#39; &amp;#39;machine-learning-databases&amp;#39; &amp;#39;/breast-cancer-wisconsin/wdbc.data&amp;#39;, header=None) X = df.loc[:, 2:].values y = df.loc[:, 1].values le = LabelEncoder() y = le.fit_transform(y) X_train, X_test, y_train, y_test = \ train_test_split(X, y, test_size=0.20, stratify=y, random_state=1) Learning curve Vary the sample size
import matplotlib.pyplot as plt from sklearn.model_selection import learning_curve pipe_lr = make_pipeline(StandardScaler(), LogisticRegression(penalty=&amp;#39;l2&amp;#39;, random_state=1, solver=&amp;#39;lbfgs&amp;#39;, max_iter=10000)) train_sizes, train_scores, test_scores =\ learning_curve(estimator=pipe_lr, X=X_train, y=y_train, train_sizes=np.</description>
    </item>
    
    <item>
      <title>Linear regression and its scores: MSE and R^2</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/linear_regression.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/linear_regression.html</guid>
      <description>Prepare data import pandas as pd from sklearn.preprocessing import StandardScaler df = pd.read_csv(&amp;#39;https://raw.githubusercontent.com/rasbt/&amp;#39; &amp;#39;python-machine-learning-book-3rd-edition/&amp;#39; &amp;#39;master/ch10/housing.data.txt&amp;#39;, header=None, sep=&amp;#39;\s+&amp;#39;) df.columns = [&amp;#39;CRIM&amp;#39;, &amp;#39;ZN&amp;#39;, &amp;#39;INDUS&amp;#39;, &amp;#39;CHAS&amp;#39;, &amp;#39;NOX&amp;#39;, &amp;#39;RM&amp;#39;, &amp;#39;AGE&amp;#39;, &amp;#39;DIS&amp;#39;, &amp;#39;RAD&amp;#39;, &amp;#39;TAX&amp;#39;, &amp;#39;PTRATIO&amp;#39;, &amp;#39;B&amp;#39;, &amp;#39;LSTAT&amp;#39;, &amp;#39;MEDV&amp;#39;] X = df[[&amp;#39;RM&amp;#39;]].values y = df[&amp;#39;MEDV&amp;#39;].values import matplotlib.pyplot as plt def lin_regplot(X, y, model): plt.scatter(X, y, c=&amp;#39;steelblue&amp;#39;, edgecolor=&amp;#39;white&amp;#39;, s=70) plt.plot(X, model.predict(X), color=&amp;#39;black&amp;#39;, lw=2) return Using scikit-learn from sklearn.linear_model import LinearRegression slr = LinearRegression() slr.fit(X, y) y_pred = slr.</description>
    </item>
    
    <item>
      <title>Linear scikit-learn classifiers</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/linear_classifiers.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/linear_classifiers.html</guid>
      <description>Basic linear supervised learning algorithms  Perceptron Linear regression Support vector Machine  from sklearn.linear_model import SGDClassifier ppn = SGDClassifier(loss=&amp;#39;perceptron&amp;#39;) lr = SGDClassifier(loss=&amp;#39;log&amp;#39;) svm = SGDClassifier(loss=&amp;#39;hinge&amp;#39;) Basic pipeline Collect data from sklearn import datasets import numpy as np iris = datasets.load_iris() X = iris.data[:, [2, 3]] y = iris.target print(&amp;#39;Class labels:&amp;#39;, np.unique(y)) Class labels: [0 1 2]  Split data from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.</description>
    </item>
    
    <item>
      <title>Logistic regression with L1 norm</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/logistic_regression.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/logistic_regression.html</guid>
      <description>Prepare data from sklearn import datasets import numpy as np # Collect data iris = datasets.load_iris() X = iris.data[:, [2, 3]] y = iris.target # Split data from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=1, stratify=y) # Standardize data from sklearn.preprocessing import StandardScaler sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train) X_test_std = sc.transform(X_test) L1 regulariztion Increase or decrease C to make the regulariztion effect stronger or weaker, respectively.</description>
    </item>
    
    <item>
      <title>Non-linear SVM kernels</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/kernel_svm.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/kernel_svm.html</guid>
      <description>Solving non-linear problems using a kernel SVM Kernel methods create nonlinear combinations of the original features to project them into a higher-dimensional space via a mapping $\phi$ where it becomes linearly seperable.
Kernel function: $\kappa \left( x^{(i)}, x^{(j)} \right) = \phi\left(x^{(i)}\right)^{T} \phi\left(x^{(j)}\right) $
One of the most widely used kernels is the Gaussian kernel: $\kappa \left( x^{(i)}, x^{(j)} \right) = exp\left( -\gamma || x^{(i)} - x^{(j)} ||^2 \right)$
Kernel can be interpretted as a similarity measure: 1 if similar, 0 if very dissimilar.</description>
    </item>
    
    <item>
      <title>Optimizing the precision and recall of a classification model</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/optimize_precision_recall.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/optimize_precision_recall.html</guid>
      <description>Basic pipeline import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split df = pd.read_csv(&amp;#39;https://archive.ics.uci.edu/ml/&amp;#39; &amp;#39;machine-learning-databases&amp;#39; &amp;#39;/breast-cancer-wisconsin/wdbc.data&amp;#39;, header=None) X = df.loc[:, 2:].values y = df.loc[:, 1].values le = LabelEncoder() y = le.fit_transform(y) X_train, X_test, y_train, y_test = \ train_test_split(X, y, test_size=0.20, stratify=y, random_state=1) from sklearn.model_selection import GridSearchCV from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC pipe_svc = make_pipeline(StandardScaler(), SVC(random_state=1)) pipe_svc.fit(X_train, y_train) y_pred = pipe_svc.</description>
    </item>
    
    <item>
      <title>PCA</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/pca.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/pca.html</guid>
      <description>Prep data # Get data import numpy as np import pandas as pd df_wine = pd.read_csv(&amp;#39;https://archive.ics.uci.edu/ml/&amp;#39; &amp;#39;machine-learning-databases/wine/wine.data&amp;#39;, header=None) df_wine.columns = [&amp;#39;Class label&amp;#39;, &amp;#39;Alcohol&amp;#39;, &amp;#39;Malic acid&amp;#39;, &amp;#39;Ash&amp;#39;, &amp;#39;Alcalinity of ash&amp;#39;, &amp;#39;Magnesium&amp;#39;, &amp;#39;Total phenols&amp;#39;, &amp;#39;Flavanoids&amp;#39;, &amp;#39;Nonflavanoid phenols&amp;#39;, &amp;#39;Proanthocyanins&amp;#39;, &amp;#39;Color intensity&amp;#39;, &amp;#39;Hue&amp;#39;, &amp;#39;OD280/OD315 of diluted wines&amp;#39;, &amp;#39;Proline&amp;#39;] # Split data from sklearn.model_selection import train_test_split X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values X_train, X_test, y_train, y_test = \ train_test_split(X, y, test_size=0.3, stratify=y, random_state=0) # Standardize data from sklearn.</description>
    </item>
    
    <item>
      <title>Polynomial regression</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/polynomial_regression.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/polynomial_regression.html</guid>
      <description>Turning a linear regression model into a curve - polynomial regression import numpy as np X = np.array([258.0, 270.0, 294.0, 320.0, 342.0, 368.0, 396.0, 446.0, 480.0, 586.0])\ [:, np.newaxis] y = np.array([236.4, 234.4, 252.8, 298.6, 314.2, 342.2, 360.8, 368.0, 391.2, 390.8]) Fit linear features from sklearn.linearuyuhjjuy7765_model import LinearRegression lr = LinearRegression() # fit linear features lr.fit(X, y) X_fit = np.arange(250, 600, 10)[:, np.newaxis] y_lin_fit = lr.predict(X_fit) LinearRegression()  Fit quadratic features from sklearn.</description>
    </item>
    
    <item>
      <title>Resampling for class imbalance</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/class_imbalance.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/class_imbalance.html</guid>
      <description>Basic pipeline import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split df = pd.read_csv(&amp;#39;https://archive.ics.uci.edu/ml/&amp;#39; &amp;#39;machine-learning-databases&amp;#39; &amp;#39;/breast-cancer-wisconsin/wdbc.data&amp;#39;, header=None) X = df.loc[:, 2:].values y = df.loc[:, 1].values le = LabelEncoder() y = le.fit_transform(y) X_train, X_test, y_train, y_test = \ train_test_split(X, y, test_size=0.20, stratify=y, random_state=1) print(&amp;#39;Prior to imbalance&amp;#39;) print(f&amp;#39;Class 0: {X[y == 0].shape[0]}&amp;#39;) print(f&amp;#39;Class 1: {X[y == 1].shape[0]}&amp;#39;) Prior to imbalance Class 0: 357 Class 1: 212  Create imbalanced set X_imb = np.</description>
    </item>
    
    <item>
      <title>ROC curve for multiclass problem</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/roc_multiclass.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/roc_multiclass.html</guid>
      <description>from sklearn.metrics import roc_curve, auc from sklearn import datasets from sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC from sklearn.preprocessing import label_binarize from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt iris = datasets.load_iris() X, y = iris.data, iris.target y = label_binarize(y, classes=[0,1,2]) n_classes = 3 # shuffle and split training and test sets X_train, X_test, y_train, y_test =\ train_test_split(X, y, test_size=0.33, random_state=0) # classifier clf = OneVsRestClassifier(LinearSVC(random_state=0)) y_score = clf.fit(X_train, y_train).decision_function(X_test) # Compute ROC curve and ROC area for each class fpr = dict() tpr = dict() roc_auc = dict() for i in range(n_classes): fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i]) roc_auc[i] = auc(fpr[i], tpr[i]) # Plot of a ROC curve for a specific class for i in range(n_classes): plt.</description>
    </item>
    
    <item>
      <title>Tuning hyperparameters via grid search</title>
      <link>https://othrif.github.io/machine_learning/scikitlearn/grid_search.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/scikitlearn/grid_search.html</guid>
      <description>Basic pipeline import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split df = pd.read_csv(&amp;#39;https://archive.ics.uci.edu/ml/&amp;#39; &amp;#39;machine-learning-databases&amp;#39; &amp;#39;/breast-cancer-wisconsin/wdbc.data&amp;#39;, header=None) X = df.loc[:, 2:].values y = df.loc[:, 1].values le = LabelEncoder() y = le.fit_transform(y) X_train, X_test, y_train, y_test = \ train_test_split(X, y, test_size=0.20, stratify=y, random_state=1) finding best parameters from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.model_selection import GridSearchCV from sklearn.svm import SVC pipe_svc = make_pipeline(StandardScaler(), SVC(random_state=1)) param_range = [0.</description>
    </item>
    
  </channel>
</rss>