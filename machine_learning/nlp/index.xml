<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Othmane Rifki</title>
    <link>https://othrif.github.io/machine_learning/nlp/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Apr 2020 14:41:32 +0200</lastBuildDate>
    
        <atom:link href="https://othrif.github.io/machine_learning/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Bag of words</title>
      <link>https://othrif.github.io/machine_learning/nlp/bag_of_words.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/nlp/bag_of_words.html</guid>
      <description>Transforming documents into feature vectors By calling the fit_transform method on CountVectorizer, we just constructed the vocabulary of the bag-of-words model and transformed the following three sentences into sparse feature vectors
import numpy as np from sklearn.feature_extraction.text import CountVectorizer count = CountVectorizer() docs = np.array([ &amp;#39;The sun is shining&amp;#39;, &amp;#39;The weather is sweet&amp;#39;, &amp;#39;The sun is shining, the weather is sweet, and one and one is two&amp;#39;]) bag = count.fit_transform(docs) Content of the vocabulary:</description>
    </item>
    
    <item>
      <title>Embeding a ML model into a Web Application</title>
      <link>https://othrif.github.io/machine_learning/nlp/embedml.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/nlp/embedml.html</guid>
      <description>Unpickle the classifier import pickle import re import os from movieclassifier.vectorizer import vect /Users/othrif/github/notes/content/machine_learning/nlp/movieclassifier/vectorizer.py  clf = pickle.load(open(os.path.join(&amp;#39;movieclassifier/pkl_objects&amp;#39;, &amp;#39;classifier.pkl&amp;#39;), &amp;#39;rb&amp;#39;)) import numpy as np label = {0:&amp;#39;negative&amp;#39;, 1:&amp;#39;positive&amp;#39;} example = [&amp;#34;I love this movie. It&amp;#39;s amazing.&amp;#34;] X = vect.transform(example) print(&amp;#39;Prediction: %s\nProbability: %.2f%%&amp;#39; %\ (label[clf.predict(X)[0]], np.max(clf.predict_proba(X))*100)) Prediction: positive Probability: 95.01%  X = vect.transform([&amp;#34;The work that they did with the move is really not up to the standards I expect&amp;#34;]) print(&amp;#39;Prediction: %s\nProbability: %.</description>
    </item>
    
    <item>
      <title>Online algorithms and out-of-core learning</title>
      <link>https://othrif.github.io/machine_learning/nlp/online_algorithm.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/nlp/online_algorithm.html</guid>
      <description>Get dataset import os import urllib.request import gzip import pandas as pd source = &amp;#39;https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz&amp;#39; target = &amp;#39;/tmp/movie_data.csv.gz&amp;#39; if not os.path.isfile(target): print (&amp;#34;download start!&amp;#34;) filename, headers = urllib.request.urlretrieve(source, filename=target) print (&amp;#34;download complete!&amp;#34;) if not os.path.isfile(&amp;#39;/tmp/movie_data.csv&amp;#39;): with gzip.open(target, &amp;#39;rb&amp;#39;) as in_f, \ open(&amp;#39;/tmp/movie_data.csv&amp;#39;, &amp;#39;wb&amp;#39;) as out_f: out_f.write(in_f.read()) print (target + &amp;#34; extraction complete!&amp;#34;) Build tokenizer and streamer import numpy as np import re from nltk.corpus import stopwords stop = stopwords.words(&amp;#39;english&amp;#39;) def tokenizer(text): &amp;#39;&amp;#39;&amp;#39; Cleans the unprocessed text data and seperate it into word tokens while removing stop words.</description>
    </item>
    
    <item>
      <title>Processing documents into tokens (tokenization and stop words)</title>
      <link>https://othrif.github.io/machine_learning/nlp/tokenize.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/nlp/tokenize.html</guid>
      <description>Tokenization from nltk.stem.porter import PorterStemmer porter = PorterStemmer() def tokenizer(text): return text.split() def tokenizer_porter(text): return [porter.stem(word) for word in text.split()] tokenizer(&amp;#39;runners like running and thus they run&amp;#39;) [&#39;runners&#39;, &#39;like&#39;, &#39;running&#39;, &#39;and&#39;, &#39;thus&#39;, &#39;they&#39;, &#39;run&#39;]  tokenizer_porter(&amp;#39;runners like running and thus they run&amp;#39;) [&#39;runner&#39;, &#39;like&#39;, &#39;run&#39;, &#39;and&#39;, &#39;thu&#39;, &#39;they&#39;, &#39;run&#39;]  Stop words import nltk nltk.download(&amp;#39;stopwords&amp;#39;) [nltk_data] Downloading package stopwords to [nltk_data] /Users/othrif/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. True  from nltk.corpus import stopwords stop = stopwords.</description>
    </item>
    
    <item>
      <title>Regular expressions</title>
      <link>https://othrif.github.io/machine_learning/nlp/regex.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/nlp/regex.html</guid>
      <description>Extremely useful to understand what the pattern means: https://regexr.com/
re.search() pattern match = re.search(pat, str) Basic patterns   a, X, 9, &amp;lt; &amp;ndash; ordinary characters just match themselves exactly. Meta-characters don&amp;rsquo;t: . ^ $ * + ? { \ | ( )
  . (a period) &amp;ndash; matches any single character except newline &amp;lsquo;\n&amp;rsquo;
  \w &amp;ndash; (lowercase w) matches a &amp;ldquo;word&amp;rdquo; character: a letter or digit or underbar [a-zA-Z0-9_].</description>
    </item>
    
    <item>
      <title>Sentiment analysis in text</title>
      <link>https://othrif.github.io/machine_learning/nlp/sentiment_analysis.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/nlp/sentiment_analysis.html</guid>
      <description>Get dataset import os import urllib.request import tarfile import pandas as pd source = &amp;#39;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&amp;#39; target = &amp;#39;/tmp/aclImdb_v1.tar.gz&amp;#39; if not os.path.isdir(&amp;#39;/tmp/aclImdb&amp;#39;) and not os.path.isfile(&amp;#39;/tmp/aclImdb_v1.tar.gz&amp;#39;): urllib.request.urlretrieve(source, target) if not os.path.isdir(&amp;#39;/tmp/aclImdb&amp;#39;): with tarfile.open(target, &amp;#39;r:gz&amp;#39;) as tar: tar.extractall(path=&amp;#39;/tmp/&amp;#39;) # Build dataframe basepath = &amp;#39;/tmp/aclImdb&amp;#39; labels = {&amp;#39;pos&amp;#39;: 1, &amp;#39;neg&amp;#39;: 0} df = pd.DataFrame() for s in (&amp;#39;test&amp;#39;, &amp;#39;train&amp;#39;): for l in (&amp;#39;pos&amp;#39;, &amp;#39;neg&amp;#39;): path = os.path.join(basepath, s, l) for file in sorted(os.listdir(path)): with open(os.</description>
    </item>
    
    <item>
      <title>Setting up a SQLite database for data storage</title>
      <link>https://othrif.github.io/machine_learning/nlp/movieclassifier/sqlite.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/nlp/movieclassifier/sqlite.html</guid>
      <description>import sqlite3 import os conn = sqlite3.connect(&amp;#39;reviews.sqlite&amp;#39;) c = conn.cursor() c.execute(&amp;#39;DROP TABLE IF EXISTS review_db&amp;#39;) c.execute(&amp;#39;CREATE TABLE review_db (review TEXT, sentiment INTEGER, date TEXT)&amp;#39;) example1 = &amp;#39;I love this movie&amp;#39; c.execute(&amp;#34;INSERT INTO review_db (review, sentiment, date) VALUES (?, ?, DATETIME(&amp;#39;now&amp;#39;))&amp;#34;, (example1, 1)) example2 = &amp;#39;I disliked this movie&amp;#39; c.execute(&amp;#34;INSERT INTO review_db (review, sentiment, date) VALUES (?, ?, DATETIME(&amp;#39;now&amp;#39;))&amp;#34;, (example2, 0)) conn.commit() conn.close() conn = sqlite3.connect(&amp;#39;reviews.sqlite&amp;#39;) c = conn.cursor() c.execute(&amp;#34;SELECT * FROM review_db WHERE date BETWEEN &amp;#39;2017-01-01 10:10:10&amp;#39; AND DATETIME(&amp;#39;now&amp;#39;)&amp;#34;) results = c.</description>
    </item>
    
    <item>
      <title>Topic modeling with Latent Dirichlet Allocation</title>
      <link>https://othrif.github.io/machine_learning/nlp/latent_dirichlet_allocation.html</link>
      <pubDate>Sun, 12 Apr 2020 14:41:32 +0200</pubDate>
      
      <guid>https://othrif.github.io/machine_learning/nlp/latent_dirichlet_allocation.html</guid>
      <description>Get dataset import os import urllib.request import gzip import pandas as pd source = &amp;#39;https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz&amp;#39; target = &amp;#39;/tmp/movie_data.csv.gz&amp;#39; if not os.path.isfile(target): print (&amp;#34;download start!&amp;#34;) filename, headers = urllib.request.urlretrieve(source, filename=target) print (&amp;#34;download complete!&amp;#34;) if not os.path.isfile(&amp;#39;/tmp/movie_data.csv&amp;#39;): with gzip.open(target, &amp;#39;rb&amp;#39;) as in_f, \ open(&amp;#39;/tmp/movie_data.csv&amp;#39;, &amp;#39;wb&amp;#39;) as out_f: out_f.write(in_f.read()) print (target + &amp;#34; extraction complete!&amp;#34;) df = pd.read_csv(target, encoding=&amp;#39;utf-8&amp;#39;) df.head(3) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  from sklearn.</description>
    </item>
    
  </channel>
</rss>