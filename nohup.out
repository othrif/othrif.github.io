[I 2021-04-19 07:25:39.469 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-04-19 07:25:39.691 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-04-19 07:25:39.691 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-04-19 07:25:39.691 ServerApp] nbclassic | extension was successfully linked.
[I 2021-04-19 07:25:39.719 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-04-19 07:25:39.720 ServerApp] The port 8889 is already in use, trying another port.
[W 2021-04-19 07:25:39.724 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-04-19 07:25:39.725 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-04-19 07:25:39.725 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-04-19 07:25:39.728 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-04-19 07:25:39.732 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-04-19 07:25:39.732 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-04-19 07:25:39.732 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-04-19 07:25:39.732 ServerApp] http://localhost:8890/lab?token=bcae1e756a3931c43dde679a50710cb247f2cfb496a4db20
[I 2021-04-19 07:25:39.732 ServerApp]  or http://127.0.0.1:8890/lab?token=bcae1e756a3931c43dde679a50710cb247f2cfb496a4db20
[I 2021-04-19 07:25:39.732 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-04-19 07:25:39.743 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-4758-open.html
    Or copy and paste one of these URLs:
        http://localhost:8890/lab?token=bcae1e756a3931c43dde679a50710cb247f2cfb496a4db20
     or http://127.0.0.1:8890/lab?token=bcae1e756a3931c43dde679a50710cb247f2cfb496a4db20
[I 2021-04-19 07:25:43.874 LabApp] Build is up to date
-\|/[I 2021-04-19 08:33:48.317 ServerApp] Kernel started: e246c542-c482-4621-bf9d-e242a2bf117a
[I 2021-04-19 08:33:50.305 ServerApp] Copying machine_learning/scikitlearn/knn.ipynb to /machine_learning/scikitlearn
[W 2021-04-19 08:33:51.852 ServerApp] Got events for closed stream None
[I 2021-04-19 08:41:14.774 ServerApp] Kernel started: eac08d90-ca4d-4d58-808e-d41b0526c690
[I 2021-04-19 08:41:17.069 ServerApp] Kernel started: df493b87-5355-436a-a5b2-2025b15bb082
[I 2021-04-19 08:41:19.030 ServerApp] Kernel started: 72316ac7-271e-4692-938d-15d3764e523d
[I 2021-04-19 08:41:21.920 ServerApp] Copying machine_learning/scikitlearn/scaling.ipynb to /machine_learning/scikitlearn
[I 2021-04-19 08:41:31.954 ServerApp] Kernel started: 98be9d10-4574-46d5-b963-7c2e7aa0f7d0
[I 2021-04-19 08:41:33.232 ServerApp] Starting buffering for eac08d90-ca4d-4d58-808e-d41b0526c690:befad1f4-09cf-4927-a79c-7e91fbc2d36f
[I 2021-04-19 08:41:33.620 ServerApp] Starting buffering for df493b87-5355-436a-a5b2-2025b15bb082:e5c553bb-39be-4dad-8c41-a7795f8e5e75
[I 2021-04-19 08:41:34.177 ServerApp] Starting buffering for 72316ac7-271e-4692-938d-15d3764e523d:5cdc1d24-0cd4-4cdf-b416-a619e03543c0
[I 2021-04-19 08:41:47.466 ServerApp] Kernel started: 860aade1-37aa-4993-92f0-3cacc367d17e
[I 2021-04-19 08:41:49.367 ServerApp] Copying python/numpy/where.ipynb to /python/numpy
[I 2021-04-19 08:41:55.561 ServerApp] Kernel started: bd485f61-fc1a-4ba8-90b9-7553fbac7ac7
[I 2021-04-19 08:41:56.592 ServerApp] Starting buffering for 860aade1-37aa-4993-92f0-3cacc367d17e:583a6095-2030-4194-b0cd-fcd86f0a2d15
[I 2021-04-19 08:43:55.514 ServerApp] Saving file at /python/numpy/cumsum.ipynb
[I 2021-04-19 08:45:25.617 ServerApp] Saving file at /python/numpy/cumsum.ipynb
[I 2021-04-19 08:45:42.025 ServerApp] Copying python/numpy/cumsum.ipynb to /python/numpy
[I 2021-04-19 08:45:53.583 ServerApp] Kernel started: 34f99d0f-6d2f-4f93-bcef-27f3dc63a3c8
[I 2021-04-19 08:45:54.593 ServerApp] Starting buffering for bd485f61-fc1a-4ba8-90b9-7553fbac7ac7:e4a7dc37-2f6c-4024-bf5e-ed335d505b35
[I 2021-04-19 08:47:53.516 ServerApp] Saving file at /python/numpy/cov_eigen.ipynb
[I 2021-04-19 08:49:53.549 ServerApp] Saving file at /python/numpy/cov_eigen.ipynb
[I 2021-04-19 08:51:53.585 ServerApp] Saving file at /python/numpy/cov_eigen.ipynb
[I 2021-04-19 08:52:08.940 ServerApp] Saving file at /python/numpy/cov_eigen.ipynb
[I 2021-04-19 08:54:08.973 ServerApp] Saving file at /python/numpy/cov_eigen.ipynb
[I 2021-04-19 08:56:09.022 ServerApp] Saving file at /python/numpy/cov_eigen.ipynb
[I 2021-04-19 08:58:09.055 ServerApp] Saving file at /python/numpy/cov_eigen.ipynb
[I 2021-04-19 08:58:54.523 ServerApp] Saving file at /python/numpy/cov_eigen.ipynb
[I 2021-04-19 08:58:56.775 ServerApp] Saving file at /python/numpy/cov_eigen.ipynb
[I 2021-04-19 08:59:31.867 ServerApp] Saving file at /machine_learning/scikitlearn/pca.ipynb
[I 2021-04-19 09:01:31.905 ServerApp] Saving file at /machine_learning/scikitlearn/pca.ipynb
[I 2021-04-19 09:03:32.578 ServerApp] Saving file at /machine_learning/scikitlearn/pca.ipynb
[I 2021-04-19 09:06:26.068 ServerApp] Starting buffering for e246c542-c482-4621-bf9d-e242a2bf117a:010342ab-1c4d-445b-9bb0-d45e7946696e
[I 2021-04-19 09:06:26.069 ServerApp] Starting buffering for 34f99d0f-6d2f-4f93-bcef-27f3dc63a3c8:efefc1f4-27d5-4c0c-b3c2-a0de243010ca
[I 2021-04-19 09:06:26.069 ServerApp] Starting buffering for 98be9d10-4574-46d5-b963-7c2e7aa0f7d0:7c8db534-6384-46ed-9e6b-510bc497961d
[I 2021-04-19 09:26:31.550 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-04-19 09:26:31.753 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-04-19 09:26:31.753 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-04-19 09:26:31.753 ServerApp] nbclassic | extension was successfully linked.
[I 2021-04-19 09:26:31.785 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-04-19 09:26:31.786 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-04-19 09:26:31.786 ServerApp] The port 8890 is already in use, trying another port.
[W 2021-04-19 09:26:31.791 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-04-19 09:26:31.793 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-04-19 09:26:31.793 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-04-19 09:26:31.796 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-04-19 09:26:31.799 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-04-19 09:26:31.799 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-04-19 09:26:31.799 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-04-19 09:26:31.799 ServerApp] http://localhost:8891/lab?token=b203d457b9cffa4c6e53386315bce8baee4e8d66aa78d232
[I 2021-04-19 09:26:31.799 ServerApp]  or http://127.0.0.1:8891/lab?token=b203d457b9cffa4c6e53386315bce8baee4e8d66aa78d232
[I 2021-04-19 09:26:31.799 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-04-19 09:26:31.815 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-5937-open.html
    Or copy and paste one of these URLs:
        http://localhost:8891/lab?token=b203d457b9cffa4c6e53386315bce8baee4e8d66aa78d232
     or http://127.0.0.1:8891/lab?token=b203d457b9cffa4c6e53386315bce8baee4e8d66aa78d232
[I 2021-04-19 09:26:36.290 LabApp] Build is up to date
-\|/-[I 2021-04-19 09:26:37.957 ServerApp] Kernel started: 4841e842-ce0f-490e-ba3f-6622b4cc5584
[I 2021-04-19 09:26:37.977 ServerApp] Kernel started: 472d0485-6c2b-4cf1-ac74-5ad0270ba0c2
\|/-\|/-\[I 2021-04-19 09:26:52.357 ServerApp] Kernel started: 813d0eb9-e0c0-41a0-8176-348d80e9d8a8
[I 2021-04-19 09:26:54.319 ServerApp] Copying python/basics/id_object.ipynb to /python/basics
[I 2021-04-19 09:27:01.294 ServerApp] Kernel started: dcfcdfd4-30b0-407f-9829-70b0c4246e7d
[I 2021-04-19 09:27:02.172 ServerApp] Starting buffering for 813d0eb9-e0c0-41a0-8176-348d80e9d8a8:0a007e45-77c3-4661-8e6b-58b71ff5bce1
[I 2021-04-19 09:27:03.231 ServerApp] Starting buffering for 4841e842-ce0f-490e-ba3f-6622b4cc5584:5f91151b-061b-469b-b325-15e5f1cdb3f1
[I 2021-04-19 09:27:21.985 ServerApp] Saving file at /python/basics/mostfrequent.ipynb
[I 2021-04-19 09:27:40.768 ServerApp] Saving file at /python/basics/mostfrequent.ipynb
[I 2021-04-19 09:31:15.195 ServerApp] Saving file at /python/basics/mostfrequent.ipynb
[I 2021-04-19 09:31:16.105 ServerApp] Starting buffering for dcfcdfd4-30b0-407f-9829-70b0c4246e7d:c0a7d4a4-1bbc-4cba-a4c7-6762109c8b67
[I 2021-04-19 09:31:16.106 ServerApp] Starting buffering for 472d0485-6c2b-4cf1-ac74-5ad0270ba0c2:f5736593-4448-48d6-a1d3-3d8131a512db
[I 2021-04-20 09:42:26.302 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-04-20 09:42:26.778 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-04-20 09:42:26.778 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-04-20 09:42:26.779 ServerApp] nbclassic | extension was successfully linked.
[I 2021-04-20 09:42:26.826 ServerApp] The port 8888 is already in use, trying another port.
[W 2021-04-20 09:42:26.835 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-04-20 09:42:26.837 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-04-20 09:42:26.837 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-04-20 09:42:26.840 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-04-20 09:42:26.847 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-04-20 09:42:26.847 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-04-20 09:42:26.847 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-04-20 09:42:26.847 ServerApp] http://localhost:8889/lab?token=82a84faeb6dc4435d103ef6ebd6568711d0f1f353a175ab6
[I 2021-04-20 09:42:26.847 ServerApp]  or http://127.0.0.1:8889/lab?token=82a84faeb6dc4435d103ef6ebd6568711d0f1f353a175ab6
[I 2021-04-20 09:42:26.847 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-04-20 09:42:26.870 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-11771-open.html
    Or copy and paste one of these URLs:
        http://localhost:8889/lab?token=82a84faeb6dc4435d103ef6ebd6568711d0f1f353a175ab6
     or http://127.0.0.1:8889/lab?token=82a84faeb6dc4435d103ef6ebd6568711d0f1f353a175ab6
[I 2021-04-20 09:42:31.626 LabApp] Build is up to date
[W 2021-04-20 09:42:32.752 ServerApp] 404 GET /api/contents/time-series-deep-AR.ipynb?content=0&1618926152732 (::1): No such file or directory: time-series-deep-AR.ipynb
[W 2021-04-20 09:42:32.752 ServerApp] No such file or directory: time-series-deep-AR.ipynb
[W 2021-04-20 09:42:32.753 ServerApp] 404 GET /api/contents/time-series-deep-AR.ipynb?content=0&1618926152732 (::1) 3.64ms referer=http://localhost:8889/lab
-\|/-\|/-\|/-[I 2021-04-20 09:42:45.681 ServerApp] Kernel started: f70c34ad-e248-4d8f-adfa-d1ed28d8bdd4
[I 2021-04-20 09:42:47.359 ServerApp] Copying python/basics/keywords.ipynb to /python/basics
[W 2021-04-20 09:42:49.259 ServerApp] Got events for closed stream None
[I 2021-04-20 09:42:56.702 ServerApp] Kernel started: 54e94978-7f0a-4823-ba5c-0e9b019554c2
[I 2021-04-20 09:44:28.160 ServerApp] Kernel interrupted: 54e94978-7f0a-4823-ba5c-0e9b019554c2
[I 2021-04-20 09:44:56.593 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 09:46:56.625 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 09:48:56.666 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 09:50:56.537 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 09:51:07.223 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 09:52:56.495 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 09:53:01.389 ServerApp] Copying python/basics/timeit.ipynb to /python/basics
[I 2021-04-20 09:53:08.629 ServerApp] Kernel started: ce2092fe-17e4-47ab-9653-49cf1ecf9305
[I 2021-04-20 09:53:10.091 ServerApp] Starting buffering for 54e94978-7f0a-4823-ba5c-0e9b019554c2:263a5ae4-cfb8-4862-afeb-b57354a1caf3
[I 2021-04-20 09:54:00.306 ServerApp] Saving file at /python/basics/logging.ipynb
[I 2021-04-20 09:54:02.905 ServerApp] Starting buffering for ce2092fe-17e4-47ab-9653-49cf1ecf9305:d044d525-e08c-4366-bfc6-c46636324370
[I 2021-04-20 09:54:03.932 ServerApp] Starting buffering for f70c34ad-e248-4d8f-adfa-d1ed28d8bdd4:62db7834-5b52-49b9-b973-d976b956012f
[I 2021-04-20 09:57:00.791 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-04-20 09:57:00.988 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-04-20 09:57:00.988 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-04-20 09:57:00.989 ServerApp] nbclassic | extension was successfully linked.
[I 2021-04-20 09:57:01.021 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-04-20 09:57:01.022 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-04-20 09:57:01.022 ServerApp] The port 8890 is already in use, trying another port.
[I 2021-04-20 09:57:01.023 ServerApp] The port 8891 is already in use, trying another port.
[W 2021-04-20 09:57:01.027 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-04-20 09:57:01.029 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-04-20 09:57:01.029 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-04-20 09:57:01.032 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-04-20 09:57:01.036 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-04-20 09:57:01.037 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-04-20 09:57:01.037 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-04-20 09:57:01.037 ServerApp] http://localhost:8892/lab?token=c79957c3c4dcf67d7ec3ef69d9204bc066c62e8940e5cb58
[I 2021-04-20 09:57:01.037 ServerApp]  or http://127.0.0.1:8892/lab?token=c79957c3c4dcf67d7ec3ef69d9204bc066c62e8940e5cb58
[I 2021-04-20 09:57:01.037 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-04-20 09:57:01.051 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-12384-open.html
    Or copy and paste one of these URLs:
        http://localhost:8892/lab?token=c79957c3c4dcf67d7ec3ef69d9204bc066c62e8940e5cb58
     or http://127.0.0.1:8892/lab?token=c79957c3c4dcf67d7ec3ef69d9204bc066c62e8940e5cb58
[I 2021-04-20 09:57:06.107 LabApp] Build is up to date
-\|/-\|[I 2021-04-20 09:57:17.060 ServerApp] Kernel started: bb30ae78-ff7c-488b-b5f2-2f7b65f016c1
[W 2021-04-20 09:57:20.677 ServerApp] Got events for closed stream None
[I 2021-04-20 09:59:16.899 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 10:01:16.939 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 11:13:17.132 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 11:13:58.762 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 11:14:05.233 ServerApp] Saving file at /python/basics/timeit.ipynb
[I 2021-04-20 11:14:05.781 ServerApp] Starting buffering for bb30ae78-ff7c-488b-b5f2-2f7b65f016c1:e5dfd9b8-525f-4b3a-8c6e-ef35aece0216
[I 2021-04-20 12:17:24.223 ServerApp] Shutting down on /api/shutdown request.
[I 2021-04-20 12:17:24.225 ServerApp] Shutting down 8 kernels
[I 2021-04-20 12:17:24.644 ServerApp] Kernel shutdown: e246c542-c482-4621-bf9d-e242a2bf117a
[I 2021-04-20 12:17:24.749 ServerApp] Kernel shutdown: eac08d90-ca4d-4d58-808e-d41b0526c690
[I 2021-04-20 12:17:24.751 ServerApp] Kernel shutdown: df493b87-5355-436a-a5b2-2025b15bb082
[I 2021-04-20 12:17:24.751 ServerApp] Kernel shutdown: 72316ac7-271e-4692-938d-15d3764e523d
[I 2021-04-20 12:17:24.856 ServerApp] Kernel shutdown: 98be9d10-4574-46d5-b963-7c2e7aa0f7d0
[I 2021-04-20 12:17:24.857 ServerApp] Kernel shutdown: 860aade1-37aa-4993-92f0-3cacc367d17e
[I 2021-04-20 12:17:24.857 ServerApp] Kernel shutdown: bd485f61-fc1a-4ba8-90b9-7553fbac7ac7
[I 2021-04-20 12:17:24.858 ServerApp] Kernel shutdown: 34f99d0f-6d2f-4f93-bcef-27f3dc63a3c8
[I 2021-04-20 12:17:37.864 ServerApp] Shutting down on /api/shutdown request.
[I 2021-04-20 12:17:37.865 ServerApp] Shutting down 1 kernel
[I 2021-04-20 12:17:38.072 ServerApp] Kernel shutdown: bb30ae78-ff7c-488b-b5f2-2f7b65f016c1
[I 2021-04-20 12:17:43.708 ServerApp] Shutting down on /api/shutdown request.
[I 2021-04-20 12:17:43.709 ServerApp] Shutting down 3 kernels
[I 2021-04-20 12:17:44.024 ServerApp] Kernel shutdown: f70c34ad-e248-4d8f-adfa-d1ed28d8bdd4
[I 2021-04-20 12:17:44.130 ServerApp] Kernel shutdown: 54e94978-7f0a-4823-ba5c-0e9b019554c2
[I 2021-04-20 12:17:44.134 ServerApp] Kernel shutdown: ce2092fe-17e4-47ab-9653-49cf1ecf9305
[I 2021-04-20 12:17:56.557 ServerApp] Shutting down on /api/shutdown request.
[I 2021-04-20 12:17:56.559 ServerApp] Shutting down 4 kernels
[I 2021-04-20 12:17:56.875 ServerApp] Kernel shutdown: 4841e842-ce0f-490e-ba3f-6622b4cc5584
[I 2021-04-20 12:17:56.877 ServerApp] Kernel shutdown: 472d0485-6c2b-4cf1-ac74-5ad0270ba0c2
[I 2021-04-20 12:17:56.878 ServerApp] Kernel shutdown: 813d0eb9-e0c0-41a0-8176-348d80e9d8a8
[I 2021-04-20 12:17:56.879 ServerApp] Kernel shutdown: dcfcdfd4-30b0-407f-9829-70b0c4246e7d
[I 2021-04-20 12:18:06.762 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-04-20 12:18:07.016 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-04-20 12:18:07.016 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-04-20 12:18:07.016 ServerApp] nbclassic | extension was successfully linked.
[W 2021-04-20 12:18:07.060 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-04-20 12:18:07.061 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-04-20 12:18:07.061 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-04-20 12:18:07.064 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-04-20 12:18:07.070 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-04-20 12:18:07.070 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-04-20 12:18:07.070 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-04-20 12:18:07.071 ServerApp] http://localhost:8888/lab?token=76abf89ffafa2bfd0107613c3f7d8dac9a0b4bccfd497e5a
[I 2021-04-20 12:18:07.071 ServerApp]  or http://127.0.0.1:8888/lab?token=76abf89ffafa2bfd0107613c3f7d8dac9a0b4bccfd497e5a
[I 2021-04-20 12:18:07.071 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-04-20 12:18:07.082 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-14293-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/lab?token=76abf89ffafa2bfd0107613c3f7d8dac9a0b4bccfd497e5a
     or http://127.0.0.1:8888/lab?token=76abf89ffafa2bfd0107613c3f7d8dac9a0b4bccfd497e5a
[I 2021-04-20 12:18:12.550 LabApp] Build is up to date
-\|/-\|/[I 2021-04-20 12:18:14.791 ServerApp] Kernel started: b7d35cf8-3267-43bb-86f4-084637729a32
-\[W 2021-04-20 12:18:18.456 ServerApp] Got events for closed stream None
[I 2021-04-20 12:18:18.881 ServerApp] Kernel started: 1557d283-9a1d-42fd-bbd1-4521f820718e
[I 2021-04-20 12:18:20.665 ServerApp] Copying python/pandas/df_numpy.ipynb to /python/pandas
[I 2021-04-20 12:18:35.371 ServerApp] Kernel started: 4062e9b1-428e-441b-a801-adb0b863be82
[I 2021-04-20 12:18:36.241 ServerApp] Starting buffering for 1557d283-9a1d-42fd-bbd1-4521f820718e:2d03f662-92bf-4769-bc78-f0d557fc0f6c
[I 2021-04-20 12:19:18.528 ServerApp] Saving file at /python/pandas/split_text.ipynb
[I 2021-04-20 12:20:07.194 ServerApp] Saving file at /python/pandas/split_text.ipynb
[I 2021-04-20 12:20:08.508 ServerApp] Starting buffering for b7d35cf8-3267-43bb-86f4-084637729a32:a89eeb61-9966-44e5-a7fe-d1a6530770eb
[I 2021-04-20 12:20:08.510 ServerApp] Starting buffering for 4062e9b1-428e-441b-a801-adb0b863be82:a1eab408-d424-46ac-aab1-a33f711fd7fc
[I 2021-04-21 09:27:01.285 ServerApp] Shutting down on /api/shutdown request.
[I 2021-04-21 09:27:01.289 ServerApp] Shutting down 3 kernels
[I 2021-04-21 09:27:01.602 ServerApp] Kernel shutdown: b7d35cf8-3267-43bb-86f4-084637729a32
[I 2021-04-21 09:27:01.605 ServerApp] Kernel shutdown: 1557d283-9a1d-42fd-bbd1-4521f820718e
[I 2021-04-21 09:27:01.606 ServerApp] Kernel shutdown: 4062e9b1-428e-441b-a801-adb0b863be82
[I 2021-04-25 12:55:36.863 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-04-25 12:55:37.287 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-04-25 12:55:37.287 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-04-25 12:55:37.287 ServerApp] nbclassic | extension was successfully linked.
[W 2021-04-25 12:55:37.341 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-04-25 12:55:37.343 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-04-25 12:55:37.343 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-04-25 12:55:37.347 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-04-25 12:55:37.352 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-04-25 12:55:37.352 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-04-25 12:55:37.352 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-04-25 12:55:37.352 ServerApp] http://localhost:8888/lab?token=9e85970a66176307ea6edfe482cdcc4fd6a71a235d8323f3
[I 2021-04-25 12:55:37.352 ServerApp]  or http://127.0.0.1:8888/lab?token=9e85970a66176307ea6edfe482cdcc4fd6a71a235d8323f3
[I 2021-04-25 12:55:37.352 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-04-25 12:55:37.367 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-2434-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/lab?token=9e85970a66176307ea6edfe482cdcc4fd6a71a235d8323f3
     or http://127.0.0.1:8888/lab?token=9e85970a66176307ea6edfe482cdcc4fd6a71a235d8323f3
[I 2021-04-25 12:55:42.174 LabApp] Build is up to date
[W 2021-04-25 12:55:43.089 ServerApp] 404 GET /api/contents/time-series-deep-AR.ipynb?content=0&1619369743073 (::1): No such file or directory: time-series-deep-AR.ipynb
[W 2021-04-25 12:55:43.090 ServerApp] No such file or directory: time-series-deep-AR.ipynb
[W 2021-04-25 12:55:43.092 ServerApp] 404 GET /api/contents/time-series-deep-AR.ipynb?content=0&1619369743073 (::1) 16.55ms referer=http://localhost:8888/lab
-\|/-\|/[I 2021-04-26 09:29:25.885 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-04-26 09:29:26.310 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-04-26 09:29:26.310 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-04-26 09:29:26.310 ServerApp] nbclassic | extension was successfully linked.
[I 2021-04-26 09:29:26.356 ServerApp] The port 8888 is already in use, trying another port.
[W 2021-04-26 09:29:26.364 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-04-26 09:29:26.365 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-04-26 09:29:26.365 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-04-26 09:29:26.368 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-04-26 09:29:26.374 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-04-26 09:29:26.374 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-04-26 09:29:26.374 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-04-26 09:29:26.374 ServerApp] http://localhost:8889/lab?token=078c2a837781de733a0ed550d17f229c6ed73bc9f553d728
[I 2021-04-26 09:29:26.374 ServerApp]  or http://127.0.0.1:8889/lab?token=078c2a837781de733a0ed550d17f229c6ed73bc9f553d728
[I 2021-04-26 09:29:26.374 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-04-26 09:29:26.391 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-7586-open.html
    Or copy and paste one of these URLs:
        http://localhost:8889/lab?token=078c2a837781de733a0ed550d17f229c6ed73bc9f553d728
     or http://127.0.0.1:8889/lab?token=078c2a837781de733a0ed550d17f229c6ed73bc9f553d728
[I 2021-04-26 09:29:31.217 LabApp] Build is up to date
[W 2021-04-26 09:29:32.195 ServerApp] 404 GET /api/contents/ch05.ipynb?content=0&1619443772187 (::1): No such file or directory: ch05.ipynb
[W 2021-04-26 09:29:32.197 ServerApp] No such file or directory: ch05.ipynb
[W 2021-04-26 09:29:32.197 ServerApp] 404 GET /api/contents/ch05.ipynb?content=0&1619443772187 (::1) 3.19ms referer=http://localhost:8889/lab
-\|/-\|/-[I 2021-04-26 09:29:41.068 ServerApp] Kernel started: b429413f-c049-4ff8-bb19-2015aa6b8974
[I 2021-04-26 09:29:43.104 ServerApp] Copying python/pandas/traintestsplit.ipynb to /python/pandas
[W 2021-04-26 09:29:44.734 ServerApp] Got events for closed stream None
[I 2021-04-26 09:30:02.205 ServerApp] Kernel started: 6eb0fa49-250e-44d4-8109-3f4d8a9e7eec
[I 2021-04-26 09:32:02.125 ServerApp] Saving file at /python/pandas/list_unique.ipynb
[I 2021-04-26 09:32:10.777 ServerApp] Saving file at /python/pandas/list_unique.ipynb
[I 2021-04-26 09:45:28.338 ServerApp] Copying python/pandas/list_unique.ipynb to /python/pandas
[I 2021-04-26 09:45:35.278 ServerApp] Kernel started: ce60cbfc-1c74-4539-962c-7589a46df494
[I 2021-04-26 09:47:35.140 ServerApp] Saving file at /python/pandas/column-names.ipynb
[I 2021-04-26 09:51:35.768 ServerApp] Saving file at /python/pandas/column-names.ipynb
[I 2021-04-26 09:53:35.805 ServerApp] Saving file at /python/pandas/column-names.ipynb
[I 2021-04-26 09:55:35.860 ServerApp] Saving file at /python/pandas/column-names.ipynb
[I 2021-04-26 09:55:48.082 ServerApp] Saving file at /python/pandas/column-names.ipynb
[I 2021-04-26 09:56:17.173 ServerApp] Starting buffering for ce60cbfc-1c74-4539-962c-7589a46df494:0675591a-d395-48b8-baf9-315edb6f9f70
[I 2021-04-26 09:56:17.174 ServerApp] Starting buffering for 6eb0fa49-250e-44d4-8109-3f4d8a9e7eec:25d32b4b-a9d2-4fdd-86dd-8b2c9749c6d8
[I 2021-04-26 09:56:17.175 ServerApp] Starting buffering for b429413f-c049-4ff8-bb19-2015aa6b8974:08361149-3722-434b-af88-b6bbb620c0fc
[I 2021-04-28 15:23:57.491 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-04-28 15:23:57.930 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-04-28 15:23:57.930 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-04-28 15:23:57.930 ServerApp] nbclassic | extension was successfully linked.
[W 2021-04-28 15:23:57.987 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-04-28 15:23:57.988 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-04-28 15:23:57.988 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-04-28 15:23:57.992 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-04-28 15:23:57.998 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-04-28 15:23:57.999 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-04-28 15:23:57.999 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-04-28 15:23:57.999 ServerApp] http://localhost:8888/lab?token=4e2d214c64abca49a3588f0c0e1c468c70bf0c71a1118308
[I 2021-04-28 15:23:57.999 ServerApp]  or http://127.0.0.1:8888/lab?token=4e2d214c64abca49a3588f0c0e1c468c70bf0c71a1118308
[I 2021-04-28 15:23:57.999 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-04-28 15:23:58.017 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-47819-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/lab?token=4e2d214c64abca49a3588f0c0e1c468c70bf0c71a1118308
     or http://127.0.0.1:8888/lab?token=4e2d214c64abca49a3588f0c0e1c468c70bf0c71a1118308
[I 2021-04-28 15:24:02.396 LabApp] Build is up to date
[W 2021-04-28 15:24:03.317 ServerApp] 404 GET /api/contents/properties_stft.ipynb?content=0&1619637843311 (::1): No such file or directory: properties_stft.ipynb
[W 2021-04-28 15:24:03.317 ServerApp] No such file or directory: properties_stft.ipynb
[W 2021-04-28 15:24:03.319 ServerApp] 404 GET /api/contents/properties_stft.ipynb?content=0&1619637843311 (::1) 5.44ms referer=http://localhost:8888/lab
[W 2021-04-28 15:24:03.321 ServerApp] 404 GET /api/contents/my_simple_audio-lstm.ipynb?content=0&1619637843312 (::1): No such file or directory: my_simple_audio-lstm.ipynb
[W 2021-04-28 15:24:03.322 ServerApp] No such file or directory: my_simple_audio-lstm.ipynb
[W 2021-04-28 15:24:03.322 ServerApp] 404 GET /api/contents/my_simple_audio-lstm.ipynb?content=0&1619637843312 (::1) 1.85ms referer=http://localhost:8888/lab
-\|/-\|/-\|/-\|/-\|/-[I 2021-04-28 15:24:15.796 ServerApp] Kernel started: 2ed23f16-6fdf-4fa5-ac3b-91654a46ada8
[W 2021-04-28 15:24:19.555 ServerApp] Got events for closed stream None
[I 2021-04-28 15:24:54.602 ServerApp] Saving file at /machine_learning/preprocessing_voice/total_audio.ipynb
[I 2021-04-28 15:24:56.231 ServerApp] Starting buffering for 2ed23f16-6fdf-4fa5-ac3b-91654a46ada8:bd87d80c-ba41-4d9f-a26b-584e8e879459
[I 2021-04-30 07:56:53.447 ServerApp] Shutting down on /api/shutdown request.
[I 2021-04-30 07:56:53.455 ServerApp] Shutting down 1 kernel
[I 2021-04-30 07:56:53.874 ServerApp] Kernel shutdown: 2ed23f16-6fdf-4fa5-ac3b-91654a46ada8
[I 2021-05-03 10:52:31.888 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-05-03 10:52:32.420 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-05-03 10:52:32.420 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-05-03 10:52:32.420 ServerApp] nbclassic | extension was successfully linked.
[I 2021-05-03 10:52:32.472 ServerApp] The port 8888 is already in use, trying another port.
[W 2021-05-03 10:52:32.481 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-05-03 10:52:32.482 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-05-03 10:52:32.482 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-05-03 10:52:32.486 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-05-03 10:52:32.492 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-05-03 10:52:32.493 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-05-03 10:52:32.493 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-05-03 10:52:32.493 ServerApp] http://localhost:8889/lab?token=bd529774dc3b78d1f1c74f13d6b0f8ca6f62d9d930917305
[I 2021-05-03 10:52:32.493 ServerApp]  or http://127.0.0.1:8889/lab?token=bd529774dc3b78d1f1c74f13d6b0f8ca6f62d9d930917305
[I 2021-05-03 10:52:32.493 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-05-03 10:52:32.509 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-7391-open.html
    Or copy and paste one of these URLs:
        http://localhost:8889/lab?token=bd529774dc3b78d1f1c74f13d6b0f8ca6f62d9d930917305
     or http://127.0.0.1:8889/lab?token=bd529774dc3b78d1f1c74f13d6b0f8ca6f62d9d930917305
[I 2021-05-03 10:52:37.016 LabApp] Build is up to date
[W 2021-05-03 10:52:37.909 ServerApp] 404 GET /api/contents/ch5_feature_selection/ch05.ipynb?content=0&1620053557902 (::1): No such file or directory: ch5_feature_selection/ch05.ipynb
[W 2021-05-03 10:52:37.909 ServerApp] No such file or directory: ch5_feature_selection/ch05.ipynb
[W 2021-05-03 10:52:37.910 ServerApp] 404 GET /api/contents/ch5_feature_selection/ch05.ipynb?content=0&1620053557902 (::1) 1.78ms referer=http://localhost:8889/lab
[W 2021-05-03 10:52:38.547 ServerApp] 404 GET /api/contents/ch5_feature_selection?1620053558543 (::1): No such file or directory: ch5_feature_selection
[W 2021-05-03 10:52:38.548 ServerApp] No such file or directory: ch5_feature_selection
[W 2021-05-03 10:52:38.548 ServerApp] 404 GET /api/contents/ch5_feature_selection?1620053558543 (::1) 1.76ms referer=http://localhost:8889/lab
-\|/-\|/-\[I 2021-05-03 10:52:47.886 ServerApp] Kernel started: 021fe1e0-4e22-4d22-a0d6-6579d23d4360
[W 2021-05-03 10:52:51.530 ServerApp] Got events for closed stream None
[I 2021-05-03 10:53:37.540 ServerApp] Saving file at /machine_learning/scikitlearn/pca.ipynb
[I 2021-05-03 10:53:38.964 ServerApp] Saving file at /machine_learning/scikitlearn/pca.ipynb
[I 2021-05-03 10:53:45.317 ServerApp] Saving file at /machine_learning/scikitlearn/pca.ipynb
[I 2021-05-03 11:15:24.722 ServerApp] Saving file at /machine_learning/scikitlearn/pca.ipynb
[I 2021-05-03 11:15:24.982 ServerApp] Starting buffering for 021fe1e0-4e22-4d22-a0d6-6579d23d4360:879ded84-6abd-486d-a0cd-c82b9e9f289c
[C 2021-05-05 08:03:00.313 ServerApp] received signal 15, stopping
[I 2021-05-05 08:03:00.341 ServerApp] Shutting down 1 kernel
[I 2021-05-05 08:03:00.885 ServerApp] Kernel shutdown: 021fe1e0-4e22-4d22-a0d6-6579d23d4360
[I 2021-05-10 11:46:17.107 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-05-10 11:46:17.371 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-05-10 11:46:17.371 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-05-10 11:46:17.371 ServerApp] nbclassic | extension was successfully linked.
[W 2021-05-10 11:46:17.433 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-05-10 11:46:17.434 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-05-10 11:46:17.434 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-05-10 11:46:17.437 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-05-10 11:46:17.443 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-05-10 11:46:17.443 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-05-10 11:46:17.444 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-05-10 11:46:17.444 ServerApp] http://localhost:8888/lab?token=a4309521998f3e85afb00233e959ae677eae75aaaa4473e7
[I 2021-05-10 11:46:17.444 ServerApp]  or http://127.0.0.1:8888/lab?token=a4309521998f3e85afb00233e959ae677eae75aaaa4473e7
[I 2021-05-10 11:46:17.444 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-05-10 11:46:17.460 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-32195-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/lab?token=a4309521998f3e85afb00233e959ae677eae75aaaa4473e7
     or http://127.0.0.1:8888/lab?token=a4309521998f3e85afb00233e959ae677eae75aaaa4473e7
[I 2021-05-10 11:46:21.611 LabApp] Build is up to date
[W 2021-05-10 11:46:22.487 ServerApp] 404 GET /api/contents/starter_kit.ipynb?content=0&1620661582478 (::1): No such file or directory: starter_kit.ipynb
[W 2021-05-10 11:46:22.487 ServerApp] No such file or directory: starter_kit.ipynb
[W 2021-05-10 11:46:22.488 ServerApp] 404 GET /api/contents/starter_kit.ipynb?content=0&1620661582478 (::1) 1.91ms referer=http://localhost:8888/lab
[W 2021-05-10 11:46:22.489 ServerApp] 404 GET /api/contents/README.md?content=0&1620661582479 (::1): No such file or directory: README.md
[W 2021-05-10 11:46:22.490 ServerApp] No such file or directory: README.md
[W 2021-05-10 11:46:22.491 ServerApp] 404 GET /api/contents/README.md?content=0&1620661582479 (::1) 1.73ms referer=http://localhost:8888/lab
-\|/-\|[I 2021-05-10 11:47:15.963 ServerApp] Saving file at /linux/basics/ip_address.md
[I 2021-05-12 16:44:45.740 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-05-12 16:44:46.181 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-05-12 16:44:46.181 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-05-12 16:44:46.181 ServerApp] nbclassic | extension was successfully linked.
[I 2021-05-12 16:44:46.226 ServerApp] The port 8888 is already in use, trying another port.
[W 2021-05-12 16:44:46.234 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-05-12 16:44:46.235 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-05-12 16:44:46.235 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-05-12 16:44:46.238 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-05-12 16:44:46.245 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-05-12 16:44:46.245 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-05-12 16:44:46.245 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-05-12 16:44:46.245 ServerApp] http://localhost:8889/lab?token=41690eb2dc1dd95a25589edbd567bcbf30b370dfb2eadbc9
[I 2021-05-12 16:44:46.245 ServerApp]  or http://127.0.0.1:8889/lab?token=41690eb2dc1dd95a25589edbd567bcbf30b370dfb2eadbc9
[I 2021-05-12 16:44:46.245 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-05-12 16:44:46.261 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-34754-open.html
    Or copy and paste one of these URLs:
        http://localhost:8889/lab?token=41690eb2dc1dd95a25589edbd567bcbf30b370dfb2eadbc9
     or http://127.0.0.1:8889/lab?token=41690eb2dc1dd95a25589edbd567bcbf30b370dfb2eadbc9
[I 2021-05-12 16:44:50.945 LabApp] Build is up to date
[W 2021-05-12 16:44:51.906 ServerApp] 404 GET /api/contents/eda.ipynb?content=0&1620852291899 (::1): No such file or directory: eda.ipynb
[W 2021-05-12 16:44:51.906 ServerApp] No such file or directory: eda.ipynb
[W 2021-05-12 16:44:51.907 ServerApp] 404 GET /api/contents/eda.ipynb?content=0&1620852291899 (::1) 2.14ms referer=http://localhost:8889/lab
[W 2021-05-12 16:44:51.909 ServerApp] 404 GET /api/contents/solutions.answers_only.051221.ipynb?content=0&1620852291900 (::1): No such file or directory: solutions.answers_only.051221.ipynb
[W 2021-05-12 16:44:51.910 ServerApp] No such file or directory: solutions.answers_only.051221.ipynb
[W 2021-05-12 16:44:51.910 ServerApp] 404 GET /api/contents/solutions.answers_only.051221.ipynb?content=0&1620852291900 (::1) 2.54ms referer=http://localhost:8889/lab
[W 2021-05-12 16:44:51.912 ServerApp] 404 GET /api/contents/solutions.ipynb?content=0&1620852291902 (::1): No such file or directory: solutions.ipynb
[W 2021-05-12 16:44:51.912 ServerApp] No such file or directory: solutions.ipynb
[W 2021-05-12 16:44:51.912 ServerApp] 404 GET /api/contents/solutions.ipynb?content=0&1620852291902 (::1) 3.90ms referer=http://localhost:8889/lab
-\|/-\|/[I 2021-05-12 16:45:10.455 ServerApp] Copying linux/basics/ip_address.md to /linux/basics
[I 2021-05-12 16:46:02.456 ServerApp] Saving file at /linux/basics/speedtest.md
[I 2021-06-21 16:18:45.811 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-06-21 16:18:46.524 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-06-21 16:18:46.524 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-06-21 16:18:46.524 ServerApp] nbclassic | extension was successfully linked.
[W 2021-06-21 16:18:46.637 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-06-21 16:18:46.639 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-06-21 16:18:46.639 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-06-21 16:18:46.644 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-06-21 16:18:46.651 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-06-21 16:18:46.651 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-06-21 16:18:46.651 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-06-21 16:18:46.651 ServerApp] http://localhost:8888/lab?token=73cdecfc018080aab0b7552c973bd936798625f54ea8a595
[I 2021-06-21 16:18:46.651 ServerApp]  or http://127.0.0.1:8888/lab?token=73cdecfc018080aab0b7552c973bd936798625f54ea8a595
[I 2021-06-21 16:18:46.651 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-06-21 16:18:46.697 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-62140-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/lab?token=73cdecfc018080aab0b7552c973bd936798625f54ea8a595
     or http://127.0.0.1:8888/lab?token=73cdecfc018080aab0b7552c973bd936798625f54ea8a595
[I 2021-06-21 16:18:51.872 LabApp] Build is up to date
[W 2021-06-21 16:18:52.875 ServerApp] 404 GET /api/contents/my_simple_audio-lstm_dawson.ipynb?content=0&1624306732867 (::1): No such file or directory: my_simple_audio-lstm_dawson.ipynb
[W 2021-06-21 16:18:52.876 ServerApp] No such file or directory: my_simple_audio-lstm_dawson.ipynb
[W 2021-06-21 16:18:52.876 ServerApp] 404 GET /api/contents/my_simple_audio-lstm_dawson.ipynb?content=0&1624306732867 (::1) 1.74ms referer=http://localhost:8888/lab
-\|/-\|/-\|/-\|[I 2021-06-21 16:19:04.922 ServerApp] Kernel started: fd72d261-467d-40c7-93d9-1085de3382f5
[I 2021-06-21 16:19:07.364 ServerApp] Copying python/basics/combinatorics.ipynb to /python/basics
[W 2021-06-21 16:19:08.622 ServerApp] Got events for closed stream None
[I 2021-06-21 16:19:17.344 ServerApp] Kernel started: 0fe48775-48b7-42ec-b1d2-177ff9d1660c
[I 2021-06-21 16:19:54.344 ServerApp] Saving file at /python/basics/exception.ipynb
[I 2021-06-21 16:19:56.187 ServerApp] Saving file at /python/basics/exception.ipynb
[I 2021-06-21 16:19:56.622 ServerApp] Starting buffering for 0fe48775-48b7-42ec-b1d2-177ff9d1660c:21e7b8f9-47f6-4991-a5b3-3a3f193cb3aa
[I 2021-06-21 16:19:56.623 ServerApp] Starting buffering for fd72d261-467d-40c7-93d9-1085de3382f5:5b0b5e91-4877-4bf7-8da0-dcdd74a49b33
[I 2021-07-15 09:29:13.710 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-07-15 09:29:14.161 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-07-15 09:29:14.161 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-07-15 09:29:14.161 ServerApp] nbclassic | extension was successfully linked.
[W 2021-07-15 09:29:14.222 ServerApp] jupyter_nbextensions_configurator | extension failed loading with message: 'nbextensions_path'
[I 2021-07-15 09:29:14.224 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.8.5/lib/python3.8/site-packages/jupyterlab
[I 2021-07-15 09:29:14.224 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.8.5/share/jupyter/lab
[I 2021-07-15 09:29:14.228 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-15 09:29:14.235 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-15 09:29:14.236 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-15 09:29:14.236 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2021-07-15 09:29:14.236 ServerApp] http://localhost:8888/lab?token=eea3a17c53c6b3dff6c4f9d93db535f6979372fb81718fe4
[I 2021-07-15 09:29:14.236 ServerApp]  or http://127.0.0.1:8888/lab?token=eea3a17c53c6b3dff6c4f9d93db535f6979372fb81718fe4
[I 2021-07-15 09:29:14.236 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-15 09:29:14.257 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-6107-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/lab?token=eea3a17c53c6b3dff6c4f9d93db535f6979372fb81718fe4
     or http://127.0.0.1:8888/lab?token=eea3a17c53c6b3dff6c4f9d93db535f6979372fb81718fe4
[I 2021-07-15 09:29:20.354 LabApp] Build is up to date
[W 2021-07-15 09:29:21.435 ServerApp] 404 GET /api/contents/keywods_analysis.ipynb?content=0&1626355761426 (::1): No such file or directory: keywods_analysis.ipynb
[W 2021-07-15 09:29:21.436 ServerApp] No such file or directory: keywods_analysis.ipynb
[W 2021-07-15 09:29:21.437 ServerApp] 404 GET /api/contents/keywods_analysis.ipynb?content=0&1626355761426 (::1) 3.33ms referer=http://localhost:8888/lab
[W 2021-07-15 09:29:21.439 ServerApp] 404 GET /api/contents/stress_test.ipynb?content=0&1626355761427 (::1): No such file or directory: stress_test.ipynb
[W 2021-07-15 09:29:21.439 ServerApp] No such file or directory: stress_test.ipynb
[W 2021-07-15 09:29:21.440 ServerApp] 404 GET /api/contents/stress_test.ipynb?content=0&1626355761427 (::1) 2.02ms referer=http://localhost:8888/lab
-\|/-\|/-\|/[I 2021-07-15 09:31:18.803 ServerApp] Kernel started: 74723d3a-b188-4a8a-8e1a-748c6942936c
[W 2021-07-15 09:31:22.758 ServerApp] Got events for closed stream None
[I 2021-07-15 09:31:33.112 ServerApp] Copying scripting/bash.ipynb to /scripting
[I 2021-07-15 09:31:40.040 ServerApp] Kernel started: a78f8d73-8ca3-4e99-bcc5-e88ead7d8fd7
[I 2021-07-15 09:32:14.664 ServerApp] Saving file at /scripting/encrypt.ipynb
[I 2021-07-15 09:32:17.580 ServerApp] Starting buffering for a78f8d73-8ca3-4e99-bcc5-e88ead7d8fd7:d7c6d0f8-dc4e-4a0b-a97a-b71e020a08f5
[I 2021-07-15 09:32:17.582 ServerApp] Starting buffering for 74723d3a-b188-4a8a-8e1a-748c6942936c:dc36aee7-478d-439c-addf-f592e5f2b0de
[I 2021-07-16 09:50:17.198 ServerApp] jupyterlab | extension was successfully linked.
[W 2021-07-16 09:50:18.444 ServerApp] The module 'jupyter_nbextensions_configurator' could not be found. Are you sure the extension is installed?
[I 2021-07-16 09:50:18.444 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-16 09:50:18.522 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-16 09:50:18.526 LabApp] JupyterLab extension loaded from /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/lib/python3.9/site-packages/jupyterlab
[I 2021-07-16 09:50:18.526 LabApp] JupyterLab application directory is /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/share/jupyter/lab
[I 2021-07-16 09:50:18.532 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-16 09:50:18.543 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-16 09:50:18.549 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-16 09:50:18.550 ServerApp] Jupyter Server 1.7.0 is running at:
[I 2021-07-16 09:50:18.550 ServerApp] http://localhost:8889/lab?token=3019fa10e75af2b297524fdee7cde7dd5681ae39e700d82f
[I 2021-07-16 09:50:18.550 ServerApp]     http://127.0.0.1:8889/lab?token=3019fa10e75af2b297524fdee7cde7dd5681ae39e700d82f
[I 2021-07-16 09:50:18.550 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-16 09:50:18.570 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-33193-open.html
    Or copy and paste one of these URLs:
        http://localhost:8889/lab?token=3019fa10e75af2b297524fdee7cde7dd5681ae39e700d82f
        http://127.0.0.1:8889/lab?token=3019fa10e75af2b297524fdee7cde7dd5681ae39e700d82f
[I 2021-07-16 09:50:23.075 LabApp] Build is up to date
[W 2021-07-16 09:50:23.382 ServerApp] 404 GET /api/contents/my_simple_audio.ipynb?content=0&1626443423375 (::1): No such file or directory: my_simple_audio.ipynb
[W 2021-07-16 09:50:23.382 ServerApp] No such file or directory: my_simple_audio.ipynb
[W 2021-07-16 09:50:23.383 ServerApp] 404 GET /api/contents/my_simple_audio.ipynb?content=0&1626443423375 (::1) 4.09ms referer=http://localhost:8889/lab
[I 2021-07-16 09:51:52.003 ServerApp] Kernel started: ad3d4f93-3b9e-4dd2-83fb-bfb692712ed9
[I 2021-07-16 09:51:53.719 ServerApp] Copying python/numpy/angle.ipynb to /python/numpy
[W 2021-07-16 09:51:56.106 ServerApp] Got events for closed stream None
[I 2021-07-16 09:52:01.612 ServerApp] Kernel started: 05a6d6e9-b1e9-43c6-b2d6-ef2d19d70b4c
[I 2021-07-16 09:54:01.519 ServerApp] Saving file at /python/numpy/majorityvote.ipynb
[I 2021-07-16 09:56:01.553 ServerApp] Saving file at /python/numpy/majorityvote.ipynb
[I 2021-07-16 09:58:01.585 ServerApp] Saving file at /python/numpy/majorityvote.ipynb
[I 2021-07-16 09:59:22.897 ServerApp] Saving file at /python/numpy/majorityvote.ipynb
[I 2021-07-16 09:59:25.996 ServerApp] Saving file at /python/numpy/majorityvote.ipynb
[I 2021-07-16 09:59:28.577 ServerApp] Starting buffering for 05a6d6e9-b1e9-43c6-b2d6-ef2d19d70b4c:05adebfe-29a5-4fc6-bea1-028ae3dd2696
[I 2021-07-16 09:59:28.579 ServerApp] Starting buffering for ad3d4f93-3b9e-4dd2-83fb-bfb692712ed9:39eb8586-4265-48ab-a22b-0e37f4647b66
[I 2021-07-16 13:00:40.535 ServerApp] jupyterlab | extension was successfully linked.
[W 2021-07-16 13:00:40.754 ServerApp] The module 'jupyter_nbextensions_configurator' could not be found. Are you sure the extension is installed?
[I 2021-07-16 13:00:40.754 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-16 13:00:40.791 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-16 13:00:40.792 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-07-16 13:00:40.797 LabApp] JupyterLab extension loaded from /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/lib/python3.9/site-packages/jupyterlab
[I 2021-07-16 13:00:40.797 LabApp] JupyterLab application directory is /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/share/jupyter/lab
[I 2021-07-16 13:00:40.801 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-16 13:00:40.807 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-16 13:00:40.811 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-16 13:00:40.811 ServerApp] Jupyter Server 1.7.0 is running at:
[I 2021-07-16 13:00:40.811 ServerApp] http://localhost:8890/lab?token=e89eec88a40f4a9be2fd052b5e06322f2921acd68cc06065
[I 2021-07-16 13:00:40.811 ServerApp]     http://127.0.0.1:8890/lab?token=e89eec88a40f4a9be2fd052b5e06322f2921acd68cc06065
[I 2021-07-16 13:00:40.811 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-16 13:00:40.827 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-47146-open.html
    Or copy and paste one of these URLs:
        http://localhost:8890/lab?token=e89eec88a40f4a9be2fd052b5e06322f2921acd68cc06065
        http://127.0.0.1:8890/lab?token=e89eec88a40f4a9be2fd052b5e06322f2921acd68cc06065
[I 2021-07-16 13:00:44.652 LabApp] Build is up to date
[I 2021-07-16 13:01:04.856 ServerApp] Kernel started: b3d0b3e5-8e77-4455-9ea7-4533906fe574
[W 2021-07-16 13:01:08.771 ServerApp] Got events for closed stream None
[I 2021-07-16 13:01:11.086 ServerApp] Copying python/pandas/column-names.ipynb to /python/pandas
[I 2021-07-16 13:01:23.645 ServerApp] Kernel started: 55b73a2a-a001-4616-9c7a-1f1a15661f10
[I 2021-07-16 13:02:39.842 ServerApp] Saving file at /python/pandas/column-rename.ipynb
[I 2021-07-16 13:02:41.080 ServerApp] Starting buffering for 55b73a2a-a001-4616-9c7a-1f1a15661f10:5cb2b538-7997-4589-abc3-1e009e39e6a5
[I 2021-07-16 13:02:41.082 ServerApp] Starting buffering for b3d0b3e5-8e77-4455-9ea7-4533906fe574:bf765ef1-cd43-4f10-98ec-3582aeefbb38
[I 2021-07-16 13:05:48.872 ServerApp] jupyterlab | extension was successfully linked.
[W 2021-07-16 13:05:49.095 ServerApp] The module 'jupyter_nbextensions_configurator' could not be found. Are you sure the extension is installed?
[I 2021-07-16 13:05:49.095 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-16 13:05:49.134 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-16 13:05:49.135 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-07-16 13:05:49.136 ServerApp] The port 8890 is already in use, trying another port.
[I 2021-07-16 13:05:49.140 LabApp] JupyterLab extension loaded from /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/lib/python3.9/site-packages/jupyterlab
[I 2021-07-16 13:05:49.140 LabApp] JupyterLab application directory is /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/share/jupyter/lab
[I 2021-07-16 13:05:49.143 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-16 13:05:49.147 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-16 13:05:49.150 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-16 13:05:49.150 ServerApp] Jupyter Server 1.7.0 is running at:
[I 2021-07-16 13:05:49.150 ServerApp] http://localhost:8891/lab?token=a37b663ceb6b94fc6fa7c13cedf707338b1239c720ba77dd
[I 2021-07-16 13:05:49.150 ServerApp]     http://127.0.0.1:8891/lab?token=a37b663ceb6b94fc6fa7c13cedf707338b1239c720ba77dd
[I 2021-07-16 13:05:49.150 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-16 13:05:49.165 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-47255-open.html
    Or copy and paste one of these URLs:
        http://localhost:8891/lab?token=a37b663ceb6b94fc6fa7c13cedf707338b1239c720ba77dd
        http://127.0.0.1:8891/lab?token=a37b663ceb6b94fc6fa7c13cedf707338b1239c720ba77dd
[I 2021-07-16 13:05:53.119 LabApp] Build is up to date
[I 2021-07-16 13:12:39.747 ServerApp] Kernel started: ee7da5e0-d9bb-42ed-8df4-569ce774433d
[W 2021-07-16 13:12:43.451 ServerApp] Got events for closed stream None
[I 2021-07-16 13:13:14.170 ServerApp] Kernel started: b024a2d4-60fb-441c-ae4f-a81f5ddc63e7
[I 2021-07-16 13:15:14.095 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 13:17:14.143 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 15:22:09.822 ServerApp] Starting buffering for b024a2d4-60fb-441c-ae4f-a81f5ddc63e7:f0a24817-4257-412c-bbdc-8eff51e3eae2
[I 2021-07-16 15:22:09.840 ServerApp] Starting buffering for ee7da5e0-d9bb-42ed-8df4-569ce774433d:f806d650-e138-490e-b730-7f94f2040578
[I 2021-07-16 15:22:09.846 ServerApp] Restoring connection for b024a2d4-60fb-441c-ae4f-a81f5ddc63e7:f0a24817-4257-412c-bbdc-8eff51e3eae2
[I 2021-07-16 15:35:22.007 ServerApp] Starting buffering for ee7da5e0-d9bb-42ed-8df4-569ce774433d:fe6ce6a8-bcb2-475c-b317-2de3333b3d54
[I 2021-07-16 15:35:22.010 ServerApp] Starting buffering for b024a2d4-60fb-441c-ae4f-a81f5ddc63e7:f0a24817-4257-412c-bbdc-8eff51e3eae2
[I 2021-07-16 15:35:22.111 ServerApp] Restoring connection for b024a2d4-60fb-441c-ae4f-a81f5ddc63e7:f0a24817-4257-412c-bbdc-8eff51e3eae2
[I 2021-07-16 16:17:34.387 ServerApp] jupyterlab | extension was successfully linked.
[W 2021-07-16 16:17:34.762 ServerApp] The module 'jupyter_nbextensions_configurator' could not be found. Are you sure the extension is installed?
[I 2021-07-16 16:17:34.762 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-16 16:17:34.818 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-16 16:17:34.819 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-07-16 16:17:34.820 ServerApp] The port 8890 is already in use, trying another port.
[I 2021-07-16 16:17:34.820 ServerApp] The port 8891 is already in use, trying another port.
[I 2021-07-16 16:17:34.824 LabApp] JupyterLab extension loaded from /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/lib/python3.9/site-packages/jupyterlab
[I 2021-07-16 16:17:34.824 LabApp] JupyterLab application directory is /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/share/jupyter/lab
[I 2021-07-16 16:17:34.828 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-16 16:17:34.835 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-16 16:17:34.840 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-16 16:17:34.841 ServerApp] Jupyter Server 1.7.0 is running at:
[I 2021-07-16 16:17:34.841 ServerApp] http://localhost:8892/lab?token=2da5eb28fd81001724836f144ff146744628a6800a5af13a
[I 2021-07-16 16:17:34.841 ServerApp]     http://127.0.0.1:8892/lab?token=2da5eb28fd81001724836f144ff146744628a6800a5af13a
[I 2021-07-16 16:17:34.841 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-16 16:17:34.865 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-74729-open.html
    Or copy and paste one of these URLs:
        http://localhost:8892/lab?token=2da5eb28fd81001724836f144ff146744628a6800a5af13a
        http://127.0.0.1:8892/lab?token=2da5eb28fd81001724836f144ff146744628a6800a5af13a
[I 2021-07-16 16:17:38.874 LabApp] Build is up to date
[I 2021-07-16 16:17:40.101 ServerApp] Kernel started: d737b3fe-6cb3-4aaa-8f8e-51dd22f2868d
[I 2021-07-16 16:17:40.135 ServerApp] Kernel started: 312aa8be-951e-4ea4-8e9b-5b1091c7ce9f
[W 2021-07-16 16:17:44.300 ServerApp] Got events for closed stream None
[I 2021-07-16 16:19:10.748 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 16:19:32.105 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 16:19:39.034 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 16:19:41.084 ServerApp] Starting buffering for 312aa8be-951e-4ea4-8e9b-5b1091c7ce9f:4d293b0c-5089-48de-8a44-d0353f2fe3fd
[I 2021-07-16 16:19:41.086 ServerApp] Starting buffering for d737b3fe-6cb3-4aaa-8f8e-51dd22f2868d:46354e38-a0c8-424a-a32b-c9766777245a
[I 2021-07-16 16:23:28.254 ServerApp] Starting buffering for b024a2d4-60fb-441c-ae4f-a81f5ddc63e7:f0a24817-4257-412c-bbdc-8eff51e3eae2
[I 2021-07-16 16:23:28.255 ServerApp] Starting buffering for ee7da5e0-d9bb-42ed-8df4-569ce774433d:f806d650-e138-490e-b730-7f94f2040578
[I 2021-07-16 16:36:06.152 ServerApp] jupyterlab | extension was successfully linked.
[W 2021-07-16 16:36:06.598 ServerApp] The module 'jupyter_nbextensions_configurator' could not be found. Are you sure the extension is installed?
[I 2021-07-16 16:36:06.599 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-16 16:36:06.657 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-16 16:36:06.658 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-07-16 16:36:06.659 ServerApp] The port 8890 is already in use, trying another port.
[I 2021-07-16 16:36:06.660 ServerApp] The port 8891 is already in use, trying another port.
[I 2021-07-16 16:36:06.661 ServerApp] The port 8892 is already in use, trying another port.
[I 2021-07-16 16:36:06.666 LabApp] JupyterLab extension loaded from /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/lib/python3.9/site-packages/jupyterlab
[I 2021-07-16 16:36:06.666 LabApp] JupyterLab application directory is /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/share/jupyter/lab
[I 2021-07-16 16:36:06.670 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-16 16:36:06.678 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-16 16:36:06.681 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-16 16:36:06.682 ServerApp] Jupyter Server 1.7.0 is running at:
[I 2021-07-16 16:36:06.682 ServerApp] http://localhost:8800/lab?token=8ce783793ac2120507d6e5ecc27a3157f3b53214176decf8
[I 2021-07-16 16:36:06.682 ServerApp]     http://127.0.0.1:8800/lab?token=8ce783793ac2120507d6e5ecc27a3157f3b53214176decf8
[I 2021-07-16 16:36:06.682 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-16 16:36:06.702 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-77380-open.html
    Or copy and paste one of these URLs:
        http://localhost:8800/lab?token=8ce783793ac2120507d6e5ecc27a3157f3b53214176decf8
        http://127.0.0.1:8800/lab?token=8ce783793ac2120507d6e5ecc27a3157f3b53214176decf8
[I 2021-07-16 16:36:12.347 LabApp] Build is up to date
[I 2021-07-16 16:36:14.208 ServerApp] Kernel started: baec47cf-6a24-4e84-8d3a-24439e60ca63
[I 2021-07-16 16:36:14.303 ServerApp] Kernel started: 69735fba-d62b-4c72-8078-add14e9e185d
[W 2021-07-16 16:36:18.504 ServerApp] Got events for closed stream None
[I 2021-07-16 16:40:15.070 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 16:42:15.119 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 16:44:15.165 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 16:46:15.189 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 16:46:40.945 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 16:47:01.860 ServerApp] Saving file at /python/basics/pyenv.ipynb
[I 2021-07-16 16:47:10.423 ServerApp] Starting buffering for 69735fba-d62b-4c72-8078-add14e9e185d:066c4b52-e803-4f20-87ab-678edfa00345
[I 2021-07-16 16:47:10.425 ServerApp] Starting buffering for baec47cf-6a24-4e84-8d3a-24439e60ca63:987d56a2-3da6-466a-9024-6fc8da204502
[I 2021-07-18 19:18:23.146 ServerApp] jupyterlab | extension was successfully linked.
[W 2021-07-18 19:18:23.565 ServerApp] The module 'jupyter_nbextensions_configurator' could not be found. Are you sure the extension is installed?
[I 2021-07-18 19:18:23.566 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-18 19:18:23.626 LabApp] JupyterLab extension loaded from /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/lib/python3.9/site-packages/jupyterlab
[I 2021-07-18 19:18:23.626 LabApp] JupyterLab application directory is /usr/local/Cellar/jupyterlab/3.0.16_1/libexec/share/jupyter/lab
[I 2021-07-18 19:18:23.630 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-18 19:18:23.638 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-18 19:18:23.644 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-18 19:18:23.644 ServerApp] Jupyter Server 1.7.0 is running at:
[I 2021-07-18 19:18:23.644 ServerApp] http://localhost:8888/lab?token=d430ceb988e3419d5cedd23f7a760a2f963c6730feb1240e
[I 2021-07-18 19:18:23.644 ServerApp]     http://127.0.0.1:8888/lab?token=d430ceb988e3419d5cedd23f7a760a2f963c6730feb1240e
[I 2021-07-18 19:18:23.645 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-18 19:18:23.668 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-5828-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/lab?token=d430ceb988e3419d5cedd23f7a760a2f963c6730feb1240e
        http://127.0.0.1:8888/lab?token=d430ceb988e3419d5cedd23f7a760a2f963c6730feb1240e
[I 2021-07-18 19:18:27.527 LabApp] Build is up to date
[W 2021-07-18 19:18:27.718 ServerApp] 404 GET /api/contents/kws.ipynb?content=0&1626650307713 (::1): No such file or directory: kws.ipynb
[W 2021-07-18 19:18:27.718 ServerApp] No such file or directory: kws.ipynb
[W 2021-07-18 19:18:27.719 ServerApp] 404 GET /api/contents/kws.ipynb?content=0&1626650307713 (::1) 2.39ms referer=http://localhost:8888/lab
[I 2021-07-19 11:50:30.862 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-07-19 11:50:31.079 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-07-19 11:50:31.080 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-07-19 11:50:31.080 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-19 11:50:31.134 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-19 11:50:31.135 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-07-19 11:50:31.135 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-07-19 11:50:31.136 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-07-19 11:50:31.136 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-07-19 11:50:31.140 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-19 11:50:31.141 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-19 11:50:31.142 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-07-19 11:50:31.142 ServerApp] The port 8890 is already in use, trying another port.
[I 2021-07-19 11:50:31.143 ServerApp] The port 8891 is already in use, trying another port.
[I 2021-07-19 11:50:31.144 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-19 11:50:31.144 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-07-19 11:50:31.144 ServerApp] http://localhost:8892/lab?token=143841c5974f2b1a044c6a831b325ac12d4b624d6bfd3cb9
[I 2021-07-19 11:50:31.144 ServerApp]  or http://127.0.0.1:8892/lab?token=143841c5974f2b1a044c6a831b325ac12d4b624d6bfd3cb9
[I 2021-07-19 11:50:31.144 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-19 11:50:31.163 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-17916-open.html
    Or copy and paste one of these URLs:
        http://localhost:8892/lab?token=143841c5974f2b1a044c6a831b325ac12d4b624d6bfd3cb9
     or http://127.0.0.1:8892/lab?token=143841c5974f2b1a044c6a831b325ac12d4b624d6bfd3cb9
[I 2021-07-19 11:50:35.224 LabApp] Build is up to date
[W 2021-07-19 11:50:35.533 ServerApp] 404 GET /api/contents/Untitled.ipynb?content=0&1626709835528 (::1): No such file or directory: Untitled.ipynb
[W 2021-07-19 11:50:35.533 ServerApp] No such file or directory: Untitled.ipynb
[W 2021-07-19 11:50:35.533 ServerApp] 404 GET /api/contents/Untitled.ipynb?content=0&1626709835528 (::1) 1.95ms referer=http://localhost:8892/lab
[W 2021-07-19 11:50:35.723 ServerApp] Config option `template_path` not recognized by `ExporterCollapsibleHeadings`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.725 ServerApp] Config option `template_path` not recognized by `ExporterCollapsibleHeadings`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.746 ServerApp] Config option `template_path` not recognized by `ExporterCollapsibleHeadings`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.771 ServerApp] Config option `template_path` not recognized by `ExporterCollapsibleHeadings`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.853 ServerApp] Config option `template_path` not recognized by `TocExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.864 ServerApp] Config option `template_path` not recognized by `TocExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.866 ServerApp] Config option `template_path` not recognized by `TocExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.877 ServerApp] Config option `template_path` not recognized by `TocExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.913 ServerApp] Config option `template_path` not recognized by `LenvsHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.914 ServerApp] Config option `template_path` not recognized by `LenvsHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.932 ServerApp] Config option `template_path` not recognized by `LenvsHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.936 ServerApp] Config option `template_path` not recognized by `LenvsHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.979 ServerApp] Config option `template_path` not recognized by `LenvsTocHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:35.985 ServerApp] Config option `template_path` not recognized by `LenvsTocHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.001 ServerApp] Config option `template_path` not recognized by `LenvsTocHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.006 ServerApp] Config option `template_path` not recognized by `LenvsTocHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.120 ServerApp] Config option `template_path` not recognized by `LenvsLatexExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.121 ServerApp] Config option `template_path` not recognized by `LenvsLatexExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.151 ServerApp] Config option `template_path` not recognized by `LenvsLatexExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.157 ServerApp] Config option `template_path` not recognized by `LenvsLatexExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.525 ServerApp] Config option `template_path` not recognized by `LenvsSlidesExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.530 ServerApp] Config option `template_path` not recognized by `LenvsSlidesExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.539 ServerApp] Config option `template_path` not recognized by `LenvsSlidesExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.541 ServerApp] Config option `template_path` not recognized by `LenvsSlidesExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.773 ServerApp] Config option `template_path` not recognized by `ExporterCollapsibleHeadings`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.789 ServerApp] Config option `template_path` not recognized by `ExporterCollapsibleHeadings`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.830 ServerApp] Config option `template_path` not recognized by `TocExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.842 ServerApp] Config option `template_path` not recognized by `TocExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.862 ServerApp] Config option `template_path` not recognized by `LenvsHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.889 ServerApp] Config option `template_path` not recognized by `LenvsHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.920 ServerApp] Config option `template_path` not recognized by `LenvsTocHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:36.947 ServerApp] Config option `template_path` not recognized by `LenvsTocHTMLExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:37.004 ServerApp] Config option `template_path` not recognized by `LenvsLatexExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:37.025 ServerApp] Config option `template_path` not recognized by `LenvsLatexExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:37.270 ServerApp] Config option `template_path` not recognized by `LenvsSlidesExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[W 2021-07-19 11:50:37.289 ServerApp] Config option `template_path` not recognized by `LenvsSlidesExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?
[I 2021-07-19 11:51:14.589 ServerApp] Kernel started: c5ecafd6-522e-4586-842e-4cf9cfc1c875
[I 2021-07-19 11:51:16.245 ServerApp] Copying python/basics/exception.ipynb to /python/basics
[I 2021-07-19 11:51:22.806 ServerApp] Kernel started: 5ba54386-d946-442d-8a37-2e329e233781
[I 2021-07-19 11:51:26.335 ServerApp] Saving file at /python/basics/exception.ipynb
[I 2021-07-19 11:53:22.710 ServerApp] Saving file at /python/basics/jupyter.ipynb
[I 2021-07-19 11:55:22.752 ServerApp] Saving file at /python/basics/jupyter.ipynb
[I 2021-07-19 11:57:15.476 ServerApp] Saving file at /python/basics/jupyter.ipynb
[I 2021-07-19 11:57:17.053 ServerApp] Starting buffering for 5ba54386-d946-442d-8a37-2e329e233781:2d24ee92-9781-46ef-b61f-cde88924f9da
[I 2021-07-19 11:57:17.054 ServerApp] Starting buffering for c5ecafd6-522e-4586-842e-4cf9cfc1c875:ebd06af8-bd21-4ff8-b9cc-99c38d1deb71
[I 2021-07-19 12:00:01.946 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-07-19 12:00:02.151 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-07-19 12:00:02.152 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-07-19 12:00:02.152 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-19 12:00:02.192 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-19 12:00:02.193 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-07-19 12:00:02.193 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-07-19 12:00:02.194 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-07-19 12:00:02.194 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-07-19 12:00:02.197 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-19 12:00:02.198 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-19 12:00:02.199 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-07-19 12:00:02.200 ServerApp] The port 8890 is already in use, trying another port.
[I 2021-07-19 12:00:02.201 ServerApp] The port 8891 is already in use, trying another port.
[I 2021-07-19 12:00:02.201 ServerApp] The port 8892 is already in use, trying another port.
[I 2021-07-19 12:00:02.202 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-19 12:00:02.202 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-07-19 12:00:02.202 ServerApp] http://localhost:8966/lab?token=d4a4d3b5ac81e73a170dae1e9d76c37e62a339f87ca328c7
[I 2021-07-19 12:00:02.203 ServerApp]  or http://127.0.0.1:8966/lab?token=d4a4d3b5ac81e73a170dae1e9d76c37e62a339f87ca328c7
[I 2021-07-19 12:00:02.203 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-19 12:00:02.217 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-20794-open.html
    Or copy and paste one of these URLs:
        http://localhost:8966/lab?token=d4a4d3b5ac81e73a170dae1e9d76c37e62a339f87ca328c7
     or http://127.0.0.1:8966/lab?token=d4a4d3b5ac81e73a170dae1e9d76c37e62a339f87ca328c7
[I 2021-07-19 12:00:06.146 LabApp] Build is up to date
[W 2021-07-19 12:00:06.307 ServerApp] 404 GET /api/contents/demo_setup.ipynb?content=0&1626710406300 (::1): No such file or directory: demo_setup.ipynb
[W 2021-07-19 12:00:06.307 ServerApp] No such file or directory: demo_setup.ipynb
[W 2021-07-19 12:00:06.308 ServerApp] 404 GET /api/contents/demo_setup.ipynb?content=0&1626710406300 (::1) 2.43ms referer=http://localhost:8966/lab
[I 2021-07-19 12:00:18.595 ServerApp] Kernel started: 23e81bba-a1a0-4b93-bc00-0b672a69f592
[W 2021-07-19 12:00:22.214 ServerApp] Got events for closed stream None
[I 2021-07-19 12:00:27.087 ServerApp] Saving file at /python/basics/jupyter.ipynb
[I 2021-07-19 12:00:34.118 ServerApp] Starting buffering for 23e81bba-a1a0-4b93-bc00-0b672a69f592:57a440b6-1d4f-4980-87dd-a4c0158d4736
[I 2021-07-19 14:24:09.617 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-07-19 14:24:10.139 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-07-19 14:24:10.139 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-07-19 14:24:10.139 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-19 14:24:10.188 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-19 14:24:10.189 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-07-19 14:24:10.189 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-07-19 14:24:10.191 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-07-19 14:24:10.191 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-07-19 14:24:10.197 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-19 14:24:10.198 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-19 14:24:10.198 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-07-19 14:24:10.200 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-19 14:24:10.200 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-07-19 14:24:10.200 ServerApp] http://localhost:8890/lab?token=12adf52a9df3a37f8918f29328088b1d393689a5fc2a7eef
[I 2021-07-19 14:24:10.200 ServerApp]  or http://127.0.0.1:8890/lab?token=12adf52a9df3a37f8918f29328088b1d393689a5fc2a7eef
[I 2021-07-19 14:24:10.201 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-19 14:24:10.218 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-30593-open.html
    Or copy and paste one of these URLs:
        http://localhost:8890/lab?token=12adf52a9df3a37f8918f29328088b1d393689a5fc2a7eef
     or http://127.0.0.1:8890/lab?token=12adf52a9df3a37f8918f29328088b1d393689a5fc2a7eef
[I 2021-07-19 14:24:14.927 LabApp] Build is up to date
[W 2021-07-19 14:24:15.166 ServerApp] 404 GET /api/contents/kws.ipynb?content=0&1626719055161 (::1): No such file or directory: kws.ipynb
[W 2021-07-19 14:24:15.167 ServerApp] No such file or directory: kws.ipynb
[W 2021-07-19 14:24:15.168 ServerApp] 404 GET /api/contents/kws.ipynb?content=0&1626719055161 (::1) 3.43ms referer=http://localhost:8890/lab
[I 2021-07-19 14:25:19.800 ServerApp] Kernel started: f7ad4e37-6c44-4142-9747-a5c01845b142
[W 2021-07-19 14:25:23.603 ServerApp] Got events for closed stream None
[I 2021-07-19 14:27:19.601 ServerApp] Saving file at /scripting/bash.ipynb
[I 2021-07-19 14:29:19.627 ServerApp] Saving file at /scripting/bash.ipynb
[I 2021-07-19 14:53:35.665 ServerApp] Saving file at /scripting/bash.ipynb
[I 2021-07-19 15:17:05.026 ServerApp] Starting buffering for f7ad4e37-6c44-4142-9747-a5c01845b142:9d3a9c0a-af06-46b5-8f4c-acbaad5f1d48
[I 2021-07-21 09:50:47.646 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-07-21 09:50:48.115 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-07-21 09:50:48.116 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-07-21 09:50:48.116 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-21 09:50:48.183 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-21 09:50:48.184 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-07-21 09:50:48.184 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-07-21 09:50:48.186 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-07-21 09:50:48.186 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-07-21 09:50:48.191 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-21 09:50:48.192 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-21 09:50:48.193 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-21 09:50:48.193 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-07-21 09:50:48.193 ServerApp] http://localhost:8889/lab?token=f13092ae539c89577556231058a8691f8fdc23f45d7d98e5
[I 2021-07-21 09:50:48.193 ServerApp]  or http://127.0.0.1:8889/lab?token=f13092ae539c89577556231058a8691f8fdc23f45d7d98e5
[I 2021-07-21 09:50:48.193 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-21 09:50:48.212 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-90235-open.html
    Or copy and paste one of these URLs:
        http://localhost:8889/lab?token=f13092ae539c89577556231058a8691f8fdc23f45d7d98e5
     or http://127.0.0.1:8889/lab?token=f13092ae539c89577556231058a8691f8fdc23f45d7d98e5
[I 2021-07-21 09:50:52.322 LabApp] Build is up to date
[W 2021-07-21 09:50:52.624 ServerApp] 404 GET /api/contents/custom_training_walkthrough.ipynb?content=0&1626875452614 (::1): No such file or directory: custom_training_walkthrough.ipynb
[W 2021-07-21 09:50:52.625 ServerApp] No such file or directory: custom_training_walkthrough.ipynb
[W 2021-07-21 09:50:52.626 ServerApp] 404 GET /api/contents/custom_training_walkthrough.ipynb?content=0&1626875452614 (::1) 3.01ms referer=http://localhost:8889/lab
[I 2021-07-21 09:50:56.053 ServerApp] Kernel started: f52b126d-fe76-42bc-ad25-f54ecb72ae2e
[W 2021-07-21 09:50:59.744 ServerApp] Got events for closed stream None
[I 2021-07-21 09:51:15.948 ServerApp] Saving file at /scripting/bash.ipynb
[I 2021-07-21 09:51:17.090 ServerApp] Saving file at /scripting/bash.ipynb
[I 2021-07-21 09:51:18.639 ServerApp] Saving file at /scripting/bash.ipynb
[I 2021-07-21 09:51:20.257 ServerApp] Starting buffering for f52b126d-fe76-42bc-ad25-f54ecb72ae2e:7359d9f3-7918-47ab-b04b-051d700aed83
[I 2021-07-25 21:46:26.920 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-07-25 21:46:27.392 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-07-25 21:46:27.392 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-07-25 21:46:27.392 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-25 21:46:27.470 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-25 21:46:27.471 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-07-25 21:46:27.471 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-07-25 21:46:27.473 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-07-25 21:46:27.473 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-07-25 21:46:27.478 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-25 21:46:27.479 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-25 21:46:27.480 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-07-25 21:46:27.481 ServerApp] The port 8890 is already in use, trying another port.
[I 2021-07-25 21:46:27.482 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-25 21:46:27.482 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-07-25 21:46:27.482 ServerApp] http://localhost:8891/lab?token=c47c02b01bf4769b2ac80c67335a638d342d6ddd6060d434
[I 2021-07-25 21:46:27.482 ServerApp]  or http://127.0.0.1:8891/lab?token=c47c02b01bf4769b2ac80c67335a638d342d6ddd6060d434
[I 2021-07-25 21:46:27.482 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-25 21:46:27.505 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-51787-open.html
    Or copy and paste one of these URLs:
        http://localhost:8891/lab?token=c47c02b01bf4769b2ac80c67335a638d342d6ddd6060d434
     or http://127.0.0.1:8891/lab?token=c47c02b01bf4769b2ac80c67335a638d342d6ddd6060d434
[I 2021-07-25 21:46:32.869 LabApp] Build is up to date
[W 2021-07-25 21:54:33.908 ServerApp] 404 GET /api/contents/compile_scored_movie_aws.ipynb?content=0&1627264473901 (::1): No such file or directory: compile_scored_movie_aws.ipynb
[W 2021-07-25 21:54:33.908 ServerApp] No such file or directory: compile_scored_movie_aws.ipynb
[W 2021-07-25 21:54:33.909 ServerApp] 404 GET /api/contents/compile_scored_movie_aws.ipynb?content=0&1627264473901 (::1) 5.45ms referer=http://localhost:8891/lab
[I 2021-07-25 21:54:47.700 ServerApp] Kernel started: 3648499b-13c8-442e-b272-073a83ed2888
[W 2021-07-25 21:54:51.369 ServerApp] Got events for closed stream None
[I 2021-07-25 21:54:56.003 ServerApp] Kernel started: 812ec183-84c3-4b6d-b6c7-54c03bb0b38d
[I 2021-07-25 21:56:47.522 ServerApp] Saving file at /machine_learning/scikitlearn/kfold.ipynb
[I 2021-07-25 21:56:56.928 ServerApp] Saving file at /machine_learning/scikitlearn/knn.ipynb
[I 2021-07-25 21:58:22.608 ServerApp] Saving file at /machine_learning/scikitlearn/kfold.ipynb
[I 2021-07-25 21:58:25.004 ServerApp] Starting buffering for 812ec183-84c3-4b6d-b6c7-54c03bb0b38d:09e75fb9-7f71-47bd-bb71-ce4f77fb0017
[I 2021-07-25 22:20:23.621 ServerApp] Copying machine_learning/scikitlearn/kfold.ipynb to /machine_learning/scikitlearn
[I 2021-07-25 22:20:39.895 ServerApp] Kernel started: b82b5c0b-ea06-4514-a8a6-cfe799506d9b
[I 2021-07-25 22:22:40.685 ServerApp] Saving file at /machine_learning/scikitlearn/validation_curves.ipynb
[I 2021-07-25 22:54:07.925 ServerApp] Starting buffering for b82b5c0b-ea06-4514-a8a6-cfe799506d9b:bd9ba7fb-70c3-4242-a459-fbf765c86c4e
[I 2021-07-25 22:54:07.931 ServerApp] Starting buffering for 3648499b-13c8-442e-b272-073a83ed2888:a51558f6-1f47-4840-ac9b-6a6b68636e7f
[I 2021-07-25 22:54:08.039 ServerApp] Restoring connection for b82b5c0b-ea06-4514-a8a6-cfe799506d9b:bd9ba7fb-70c3-4242-a459-fbf765c86c4e
[I 2021-07-25 22:54:08.060 ServerApp] Restoring connection for 3648499b-13c8-442e-b272-073a83ed2888:a51558f6-1f47-4840-ac9b-6a6b68636e7f
[I 2021-07-26 09:16:04.319 ServerApp] Starting buffering for 3648499b-13c8-442e-b272-073a83ed2888:a51558f6-1f47-4840-ac9b-6a6b68636e7f
[I 2021-07-26 09:16:04.324 ServerApp] Starting buffering for b82b5c0b-ea06-4514-a8a6-cfe799506d9b:bd9ba7fb-70c3-4242-a459-fbf765c86c4e
[I 2021-07-26 09:16:04.526 ServerApp] Restoring connection for 3648499b-13c8-442e-b272-073a83ed2888:a51558f6-1f47-4840-ac9b-6a6b68636e7f
[I 2021-07-26 09:16:04.555 ServerApp] Restoring connection for b82b5c0b-ea06-4514-a8a6-cfe799506d9b:bd9ba7fb-70c3-4242-a459-fbf765c86c4e
[I 2021-07-26 09:44:31.057 ServerApp] Saving file at /machine_learning/scikitlearn/kfold.ipynb
[I 2021-07-26 09:44:43.316 ServerApp] Saving file at /machine_learning/scikitlearn/kfold.ipynb
[I 2021-07-26 09:45:55.560 ServerApp] Saving file at /machine_learning/scikitlearn/validation_curves.ipynb
[I 2021-07-26 09:47:55.612 ServerApp] Saving file at /machine_learning/scikitlearn/validation_curves.ipynb
[I 2021-07-26 09:49:55.663 ServerApp] Saving file at /machine_learning/scikitlearn/validation_curves.ipynb
[I 2021-07-26 09:51:31.607 ServerApp] Saving file at /machine_learning/scikitlearn/validation_curves.ipynb
[I 2021-07-26 09:52:40.719 ServerApp] Saving file at /machine_learning/scikitlearn/validation_curves.ipynb
[I 2021-07-26 09:53:35.447 ServerApp] Saving file at /machine_learning/scikitlearn/validation_curves.ipynb
[I 2021-07-26 09:53:42.030 ServerApp] Copying machine_learning/scikitlearn/validation_curves.ipynb to /machine_learning/scikitlearn
[I 2021-07-26 09:54:31.497 ServerApp] Kernel started: 43d559eb-bab7-4b14-bda9-dc017db35df0
[I 2021-07-26 09:54:32.098 ServerApp] Starting buffering for b82b5c0b-ea06-4514-a8a6-cfe799506d9b:bd9ba7fb-70c3-4242-a459-fbf765c86c4e
[I 2021-07-26 09:56:31.214 ServerApp] Saving file at /machine_learning/scikitlearn/grid_search.ipynb
[I 2021-07-26 09:57:00.288 ServerApp] Saving file at /machine_learning/scikitlearn/grid_search.ipynb
[I 2021-07-26 10:36:35.560 ServerApp] Saving file at /machine_learning/scikitlearn/grid_search.ipynb
[I 2021-07-26 10:36:41.621 ServerApp] Copying machine_learning/scikitlearn/grid_search.ipynb to /machine_learning/scikitlearn
[I 2021-07-26 10:36:52.978 ServerApp] Kernel started: 5514a46f-c32e-4c52-9dbe-506bac5581f7
[I 2021-07-26 10:36:53.765 ServerApp] Starting buffering for 43d559eb-bab7-4b14-bda9-dc017db35df0:9539e4ef-f585-4dfb-8e3b-dabd40bf8d6d
[I 2021-07-26 10:38:52.830 ServerApp] Saving file at /machine_learning/scikitlearn/evaluation_metrics.ipynb
[I 2021-07-26 10:40:52.868 ServerApp] Saving file at /machine_learning/scikitlearn/evaluation_metrics.ipynb
[I 2021-07-26 10:44:52.910 ServerApp] Saving file at /machine_learning/scikitlearn/evaluation_metrics.ipynb
[I 2021-07-26 10:46:52.940 ServerApp] Saving file at /machine_learning/scikitlearn/evaluation_metrics.ipynb
[I 2021-07-26 10:48:53.023 ServerApp] Saving file at /machine_learning/scikitlearn/evaluation_metrics.ipynb
[I 2021-07-26 10:50:53.083 ServerApp] Saving file at /machine_learning/scikitlearn/evaluation_metrics.ipynb
[I 2021-07-26 11:03:58.474 ServerApp] Copying machine_learning/scikitlearn/evaluation_metrics.ipynb to /machine_learning/scikitlearn
[I 2021-07-26 11:04:12.829 ServerApp] Kernel started: ae94f4be-7a46-4638-a23c-49db0a68c232
[I 2021-07-26 11:06:00.936 ServerApp] Kernel restarted: ae94f4be-7a46-4638-a23c-49db0a68c232
[I 2021-07-26 11:06:00.944 ServerApp] Starting buffering for ae94f4be-7a46-4638-a23c-49db0a68c232:7f8e260a-f481-4e05-bfb1-9f1c38820740
[I 2021-07-26 11:06:00.950 ServerApp] Restoring connection for ae94f4be-7a46-4638-a23c-49db0a68c232:7f8e260a-f481-4e05-bfb1-9f1c38820740
[I 2021-07-26 11:06:12.535 ServerApp] Saving file at /machine_learning/scikitlearn/optimize_precision_recall.ipynb
[I 2021-07-26 11:06:55.938 ServerApp] Saving file at /machine_learning/scikitlearn/optimize_precision_recall.ipynb
[I 2021-07-26 11:08:55.978 ServerApp] Saving file at /machine_learning/scikitlearn/optimize_precision_recall.ipynb
[I 2021-07-26 11:09:47.811 ServerApp] Kernel restarted: ae94f4be-7a46-4638-a23c-49db0a68c232
[I 2021-07-26 11:09:47.818 ServerApp] Starting buffering for ae94f4be-7a46-4638-a23c-49db0a68c232:7f8e260a-f481-4e05-bfb1-9f1c38820740
[I 2021-07-26 11:09:47.823 ServerApp] Restoring connection for ae94f4be-7a46-4638-a23c-49db0a68c232:7f8e260a-f481-4e05-bfb1-9f1c38820740
[I 2021-07-26 11:10:56.029 ServerApp] Saving file at /machine_learning/scikitlearn/optimize_precision_recall.ipynb
[I 2021-07-26 11:18:58.505 ServerApp] Saving file at /machine_learning/scikitlearn/optimize_precision_recall.ipynb
[I 2021-07-26 11:20:58.545 ServerApp] Saving file at /machine_learning/scikitlearn/optimize_precision_recall.ipynb
[I 2021-07-26 11:22:13.784 ServerApp] Kernel restarted: ae94f4be-7a46-4638-a23c-49db0a68c232
[I 2021-07-26 11:22:13.793 ServerApp] Starting buffering for ae94f4be-7a46-4638-a23c-49db0a68c232:7f8e260a-f481-4e05-bfb1-9f1c38820740
[I 2021-07-26 11:22:13.800 ServerApp] Restoring connection for ae94f4be-7a46-4638-a23c-49db0a68c232:7f8e260a-f481-4e05-bfb1-9f1c38820740
[I 2021-07-26 11:22:58.586 ServerApp] Saving file at /machine_learning/scikitlearn/optimize_precision_recall.ipynb
[I 2021-07-26 11:24:07.827 ServerApp] Saving file at /machine_learning/scikitlearn/optimize_precision_recall.ipynb
[I 2021-07-26 11:24:29.860 ServerApp] Kernel restarted: ae94f4be-7a46-4638-a23c-49db0a68c232
[I 2021-07-26 11:24:29.881 ServerApp] Starting buffering for ae94f4be-7a46-4638-a23c-49db0a68c232:7f8e260a-f481-4e05-bfb1-9f1c38820740
[I 2021-07-26 11:24:29.887 ServerApp] Restoring connection for ae94f4be-7a46-4638-a23c-49db0a68c232:7f8e260a-f481-4e05-bfb1-9f1c38820740
[I 2021-07-26 11:26:07.863 ServerApp] Saving file at /machine_learning/scikitlearn/optimize_precision_recall.ipynb
[I 2021-07-26 13:47:47.586 ServerApp] Saving file at /machine_learning/scikitlearn/optimize_precision_recall.ipynb
[I 2021-07-26 13:56:33.546 ServerApp] Starting buffering for 5514a46f-c32e-4c52-9dbe-506bac5581f7:42c3b5e7-d372-4ca8-ac87-082d3ec42a4f
[I 2021-07-26 13:56:33.986 ServerApp] Starting buffering for ae94f4be-7a46-4638-a23c-49db0a68c232:7f8e260a-f481-4e05-bfb1-9f1c38820740
[I 2021-07-26 13:56:34.742 ServerApp] Starting buffering for 43d559eb-bab7-4b14-bda9-dc017db35df0:17900474-8aff-4c2a-9113-f43b3c5bdd85
[I 2021-07-26 13:56:38.361 ServerApp] Copying machine_learning/scikitlearn/optimize_precision_recall.ipynb to /machine_learning/scikitlearn
[I 2021-07-26 13:56:53.839 ServerApp] Kernel started: d3ab8fdf-0057-4605-9e0f-1f28ef5f771c
[I 2021-07-26 13:58:53.693 ServerApp] Saving file at /machine_learning/scikitlearn/class_imbalance.ipynb
[I 2021-07-26 14:00:53.733 ServerApp] Saving file at /machine_learning/scikitlearn/class_imbalance.ipynb
[I 2021-07-26 14:02:53.787 ServerApp] Saving file at /machine_learning/scikitlearn/class_imbalance.ipynb
[I 2021-07-26 14:03:08.468 ServerApp] Saving file at /machine_learning/scikitlearn/class_imbalance.ipynb
[I 2021-07-26 14:03:15.257 ServerApp] Saving file at /machine_learning/scikitlearn/class_imbalance.ipynb
[I 2021-07-26 14:03:17.842 ServerApp] Starting buffering for d3ab8fdf-0057-4605-9e0f-1f28ef5f771c:a2c8240d-9f9e-4126-a80b-8585ff763cef
[I 2021-07-26 14:03:17.843 ServerApp] Starting buffering for 3648499b-13c8-442e-b272-073a83ed2888:a51558f6-1f47-4840-ac9b-6a6b68636e7f
[I 2021-07-28 20:41:00.654 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-07-28 20:41:01.103 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-07-28 20:41:01.104 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-07-28 20:41:01.104 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-28 20:41:01.179 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-28 20:41:01.180 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-07-28 20:41:01.180 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-07-28 20:41:01.182 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-07-28 20:41:01.182 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-07-28 20:41:01.188 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-28 20:41:01.189 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-28 20:41:01.190 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-07-28 20:41:01.190 ServerApp] The port 8890 is already in use, trying another port.
[I 2021-07-28 20:41:01.191 ServerApp] The port 8891 is already in use, trying another port.
[I 2021-07-28 20:41:01.191 ServerApp] The port 8892 is already in use, trying another port.
[I 2021-07-28 20:41:01.192 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-28 20:41:01.192 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-07-28 20:41:01.192 ServerApp] http://localhost:8913/lab?token=a777c12e65552a79fe8c4fc5a947cb0d29ed011f47dc2a8f
[I 2021-07-28 20:41:01.192 ServerApp]  or http://127.0.0.1:8913/lab?token=a777c12e65552a79fe8c4fc5a947cb0d29ed011f47dc2a8f
[I 2021-07-28 20:41:01.192 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-28 20:41:01.208 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-1582-open.html
    Or copy and paste one of these URLs:
        http://localhost:8913/lab?token=a777c12e65552a79fe8c4fc5a947cb0d29ed011f47dc2a8f
     or http://127.0.0.1:8913/lab?token=a777c12e65552a79fe8c4fc5a947cb0d29ed011f47dc2a8f
[I 2021-07-28 20:41:05.498 LabApp] Build is up to date
[W 2021-07-28 20:41:05.851 ServerApp] 404 GET /api/contents/data/all_deepgram_analysis_2021_06_24-results.csv?content=0&1627519265846 (::1): No such file or directory: data/all_deepgram_analysis_2021_06_24-results.csv
[W 2021-07-28 20:41:05.851 ServerApp] No such file or directory: data/all_deepgram_analysis_2021_06_24-results.csv
[W 2021-07-28 20:41:05.852 ServerApp] 404 GET /api/contents/data/all_deepgram_analysis_2021_06_24-results.csv?content=0&1627519265846 (::1) 1.92ms referer=http://localhost:8913/lab
[W 2021-07-28 20:41:05.855 ServerApp] 404 GET /api/contents/listen_fp_fn.ipynb?content=0&1627519265848 (::1): No such file or directory: listen_fp_fn.ipynb
[W 2021-07-28 20:41:05.855 ServerApp] No such file or directory: listen_fp_fn.ipynb
[W 2021-07-28 20:41:05.856 ServerApp] 404 GET /api/contents/listen_fp_fn.ipynb?content=0&1627519265848 (::1) 2.34ms referer=http://localhost:8913/lab
[W 2021-07-28 20:41:05.859 ServerApp] 404 GET /api/contents/format_score_label.ipynb?content=0&1627519265850 (::1): No such file or directory: format_score_label.ipynb
[W 2021-07-28 20:41:05.860 ServerApp] No such file or directory: format_score_label.ipynb
[W 2021-07-28 20:41:05.860 ServerApp] 404 GET /api/contents/format_score_label.ipynb?content=0&1627519265850 (::1) 2.94ms referer=http://localhost:8913/lab
[W 2021-07-28 20:41:05.862 ServerApp] 404 GET /api/contents/analyze_results.ipynb?content=0&1627519265852 (::1): No such file or directory: analyze_results.ipynb
[W 2021-07-28 20:41:05.863 ServerApp] No such file or directory: analyze_results.ipynb
[W 2021-07-28 20:41:05.863 ServerApp] 404 GET /api/contents/analyze_results.ipynb?content=0&1627519265852 (::1) 2.43ms referer=http://localhost:8913/lab
[I 2021-07-28 20:41:26.595 ServerApp] Kernel started: 151fe06b-734c-49f2-9908-367821b1ec35
[W 2021-07-28 20:41:30.425 ServerApp] Got events for closed stream None
[I 2021-07-28 20:41:43.552 ServerApp] Saving file at /scripting/bash.ipynb
[I 2021-07-28 20:41:46.667 ServerApp] Saving file at /scripting/bash.ipynb
[I 2021-07-28 20:41:48.085 ServerApp] Starting buffering for 151fe06b-734c-49f2-9908-367821b1ec35:d75b8303-4c0f-4bcc-b373-69fb1e453bdb
[I 2021-07-29 10:25:03.420 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-07-29 10:25:03.890 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-07-29 10:25:03.890 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-07-29 10:25:03.890 ServerApp] nbclassic | extension was successfully linked.
[I 2021-07-29 10:25:03.958 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-07-29 10:25:03.959 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-07-29 10:25:03.959 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-07-29 10:25:03.961 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-07-29 10:25:03.961 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-07-29 10:25:03.966 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-07-29 10:25:03.968 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-07-29 10:25:03.969 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-07-29 10:25:03.969 ServerApp] The port 8890 is already in use, trying another port.
[I 2021-07-29 10:25:03.970 ServerApp] The port 8891 is already in use, trying another port.
[I 2021-07-29 10:25:03.971 ServerApp] The port 8892 is already in use, trying another port.
[I 2021-07-29 10:25:03.972 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-07-29 10:25:03.972 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-07-29 10:25:03.972 ServerApp] http://localhost:8987/lab?token=c920375ddde33bc212c693946b5dc46374ed0cfcd6e9bccc
[I 2021-07-29 10:25:03.972 ServerApp]  or http://127.0.0.1:8987/lab?token=c920375ddde33bc212c693946b5dc46374ed0cfcd6e9bccc
[I 2021-07-29 10:25:03.972 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-07-29 10:25:04.001 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-5396-open.html
    Or copy and paste one of these URLs:
        http://localhost:8987/lab?token=c920375ddde33bc212c693946b5dc46374ed0cfcd6e9bccc
     or http://127.0.0.1:8987/lab?token=c920375ddde33bc212c693946b5dc46374ed0cfcd6e9bccc
[I 2021-07-29 10:25:08.822 LabApp] Build is up to date
[I 2021-07-29 10:25:10.790 ServerApp] Kernel started: 87ec4ba5-75a6-4ee9-96b4-4d3e7426f206
[W 2021-07-29 10:25:14.457 ServerApp] Got events for closed stream None
[I 2021-07-29 10:25:23.827 ServerApp] Kernel started: f9ac450d-e881-48fa-849c-a8395c2c4a7b
[I 2021-07-29 10:25:26.646 ServerApp] Copying machine_learning/aws/transcribe.ipynb to /machine_learning/aws
[I 2021-07-29 10:25:34.768 ServerApp] Kernel started: 4e58d7fd-e657-4107-aeb8-21a4cadf54ea
[I 2021-07-29 10:25:56.625 ServerApp] Saving file at /machine_learning/aws/size_s3.ipynb
[I 2021-07-29 10:25:58.013 ServerApp] Saving file at /machine_learning/aws/size_s3.ipynb
[I 2021-07-29 10:25:58.224 ServerApp] Saving file at /machine_learning/aws/size_s3.ipynb
[I 2021-07-29 10:26:00.673 ServerApp] Starting buffering for 4e58d7fd-e657-4107-aeb8-21a4cadf54ea:9ee82ba0-d646-4ca5-9843-298698ec3120
[I 2021-07-29 10:26:00.674 ServerApp] Starting buffering for f9ac450d-e881-48fa-849c-a8395c2c4a7b:6e23ef5b-0f74-41f0-a53f-1c7d7bc9ad01
[I 2021-07-29 10:26:00.677 ServerApp] Starting buffering for 87ec4ba5-75a6-4ee9-96b4-4d3e7426f206:3437db1b-3181-475e-8285-2406196e86ce
[I 2021-08-14 07:52:11.830 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-08-14 07:52:12.404 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-08-14 07:52:12.404 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-08-14 07:52:12.405 ServerApp] nbclassic | extension was successfully linked.
[I 2021-08-14 07:52:12.485 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-08-14 07:52:12.486 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-08-14 07:52:12.486 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-08-14 07:52:12.488 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-08-14 07:52:12.488 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-08-14 07:52:12.493 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-08-14 07:52:12.494 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-08-14 07:52:12.494 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-08-14 07:52:12.495 ServerApp] http://localhost:8888/lab?token=66b28cbf4e7ace5fff37d261ac15559b49892ba45d33c591
[I 2021-08-14 07:52:12.495 ServerApp]  or http://127.0.0.1:8888/lab?token=66b28cbf4e7ace5fff37d261ac15559b49892ba45d33c591
[I 2021-08-14 07:52:12.495 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-08-14 07:52:12.521 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-13237-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/lab?token=66b28cbf4e7ace5fff37d261ac15559b49892ba45d33c591
     or http://127.0.0.1:8888/lab?token=66b28cbf4e7ace5fff37d261ac15559b49892ba45d33c591
[I 2021-08-14 07:52:17.524 LabApp] Build is up to date
[W 2021-08-14 07:52:17.852 ServerApp] 404 GET /api/contents/leaderboard_tocsin.ipynb?content=0&1628952737845 (::1): No such file or directory: leaderboard_tocsin.ipynb
[W 2021-08-14 07:52:17.853 ServerApp] No such file or directory: leaderboard_tocsin.ipynb
[W 2021-08-14 07:52:17.853 ServerApp] 404 GET /api/contents/leaderboard_tocsin.ipynb?content=0&1628952737845 (::1) 2.59ms referer=http://localhost:8888/lab
[W 2021-08-14 07:52:17.862 ServerApp] 404 GET /api/contents/process.ipynb?content=0&1628952737848 (::1): No such file or directory: process.ipynb
[W 2021-08-14 07:52:17.862 ServerApp] No such file or directory: process.ipynb
[W 2021-08-14 07:52:17.863 ServerApp] 404 GET /api/contents/process.ipynb?content=0&1628952737848 (::1) 6.97ms referer=http://localhost:8888/lab
[I 2021-08-15 09:25:39.494 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-08-15 09:25:39.831 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-08-15 09:25:39.831 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-08-15 09:25:39.831 ServerApp] nbclassic | extension was successfully linked.
[I 2021-08-15 09:25:39.891 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-08-15 09:25:39.892 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-08-15 09:25:39.892 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-08-15 09:25:39.893 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-08-15 09:25:39.893 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-08-15 09:25:39.899 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-08-15 09:25:39.900 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-08-15 09:25:39.901 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-08-15 09:25:39.901 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-08-15 09:25:39.901 ServerApp] http://localhost:8889/lab?token=f3e47f56ac8ca193cabfe3ccde7bfa7572079e6a31db1146
[I 2021-08-15 09:25:39.901 ServerApp]  or http://127.0.0.1:8889/lab?token=f3e47f56ac8ca193cabfe3ccde7bfa7572079e6a31db1146
[I 2021-08-15 09:25:39.901 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-08-15 09:25:39.922 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-21630-open.html
    Or copy and paste one of these URLs:
        http://localhost:8889/lab?token=f3e47f56ac8ca193cabfe3ccde7bfa7572079e6a31db1146
     or http://127.0.0.1:8889/lab?token=f3e47f56ac8ca193cabfe3ccde7bfa7572079e6a31db1146
[I 2021-08-15 09:25:44.119 LabApp] Build is up to date
[I 2021-08-15 09:25:45.358 ServerApp] Kernel started: c3182739-a04e-4065-848a-df2e2ed21b21
[W 2021-08-15 09:25:49.124 ServerApp] Got events for closed stream None
[I 2021-08-15 09:45:02.631 ServerApp] Creating new directory in /machine_learning
[I 2021-08-15 09:45:16.216 ServerApp] Copying machine_learning/aws/_index.md to /machine_learning/nlp
[I 2021-08-15 09:45:25.340 ServerApp] Copying machine_learning/jupyter/close_jupyter_server.ipynb to /machine_learning/nlp
[I 2021-08-15 09:45:33.430 ServerApp] Kernel started: 64e52793-1747-405b-aee0-e1fd7c116fa6
[I 2021-08-15 09:47:33.323 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 09:49:33.353 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 12:38:46.505 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 12:40:46.564 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 12:41:19.950 ServerApp] Kernel interrupted: 64e52793-1747-405b-aee0-e1fd7c116fa6
[I 2021-08-15 12:41:22.959 ServerApp] Kernel restarted: 64e52793-1747-405b-aee0-e1fd7c116fa6
[I 2021-08-15 12:41:22.966 ServerApp] Starting buffering for 64e52793-1747-405b-aee0-e1fd7c116fa6:6041307c-0bae-409e-8037-f54e27c52981
[I 2021-08-15 12:41:22.972 ServerApp] Restoring connection for 64e52793-1747-405b-aee0-e1fd7c116fa6:6041307c-0bae-409e-8037-f54e27c52981
[I 2021-08-15 12:41:51.583 ServerApp] Kernel interrupted: 64e52793-1747-405b-aee0-e1fd7c116fa6
[I 2021-08-15 12:42:46.606 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 12:44:46.631 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 12:46:46.666 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 17:55:23.584 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 17:57:19.928 ServerApp] Kernel restarted: 64e52793-1747-405b-aee0-e1fd7c116fa6
[I 2021-08-15 17:57:19.933 ServerApp] Starting buffering for 64e52793-1747-405b-aee0-e1fd7c116fa6:6041307c-0bae-409e-8037-f54e27c52981
[I 2021-08-15 17:57:19.941 ServerApp] Restoring connection for 64e52793-1747-405b-aee0-e1fd7c116fa6:6041307c-0bae-409e-8037-f54e27c52981
[I 2021-08-15 17:57:23.618 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 17:59:23.651 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 18:29:23.772 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 18:29:24.556 ServerApp] Copying machine_learning/nlp/sentiment_analysis.ipynb to /machine_learning/nlp
[I 2021-08-15 18:29:31.612 ServerApp] Kernel started: 7302a974-e96e-4066-bed8-524889e7f548
[I 2021-08-15 18:31:23.809 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-15 18:31:31.416 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-15 18:55:32.298 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-15 18:57:32.336 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-15 18:59:32.372 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-15 19:01:32.414 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-15 19:03:32.451 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-15 19:17:32.501 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-15 19:19:32.546 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-15 19:21:32.583 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-15 19:23:32.632 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:18:43.227 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:20:43.302 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:22:43.356 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:26:43.401 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:28:43.450 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:30:43.495 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:32:43.553 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:34:43.612 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:36:43.681 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:40:43.735 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:43:37.595 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 07:43:40.092 ServerApp] Starting buffering for 7302a974-e96e-4066-bed8-524889e7f548:8d63cbdd-d2d0-46c6-a97f-cbbdeac45df0
[I 2021-08-16 07:43:40.093 ServerApp] Starting buffering for 64e52793-1747-405b-aee0-e1fd7c116fa6:6041307c-0bae-409e-8037-f54e27c52981
[I 2021-08-16 07:43:40.094 ServerApp] Starting buffering for c3182739-a04e-4065-848a-df2e2ed21b21:1827b2ec-340e-459d-a468-68ef7933c87d
[I 2021-08-16 07:52:20.258 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-08-16 07:52:20.767 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-08-16 07:52:20.767 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-08-16 07:52:20.767 ServerApp] nbclassic | extension was successfully linked.
[I 2021-08-16 07:52:20.849 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-08-16 07:52:20.850 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-08-16 07:52:20.850 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-08-16 07:52:20.851 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-08-16 07:52:20.851 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-08-16 07:52:20.855 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-08-16 07:52:20.857 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-08-16 07:52:20.857 ServerApp] The port 8889 is already in use, trying another port.
[I 2021-08-16 07:52:20.858 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-08-16 07:52:20.859 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-08-16 07:52:20.859 ServerApp] http://localhost:8890/lab?token=30976d6876b700f6fc6d50decb05e930dbc9cc6187f4faf6
[I 2021-08-16 07:52:20.859 ServerApp]  or http://127.0.0.1:8890/lab?token=30976d6876b700f6fc6d50decb05e930dbc9cc6187f4faf6
[I 2021-08-16 07:52:20.859 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-08-16 07:52:20.881 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-29352-open.html
    Or copy and paste one of these URLs:
        http://localhost:8890/lab?token=30976d6876b700f6fc6d50decb05e930dbc9cc6187f4faf6
     or http://127.0.0.1:8890/lab?token=30976d6876b700f6fc6d50decb05e930dbc9cc6187f4faf6
[I 2021-08-16 07:52:25.568 LabApp] Build is up to date
[C 2021-08-16 07:53:06.617 ServerApp] received signal 15, stopping
[I 2021-08-16 07:53:06.621 ServerApp] Shutting down 3 kernels
[I 2021-08-16 07:53:06.623 ServerApp] Kernel shutdown: c3182739-a04e-4065-848a-df2e2ed21b21
[I 2021-08-16 07:53:06.625 ServerApp] Kernel shutdown: 64e52793-1747-405b-aee0-e1fd7c116fa6
[I 2021-08-16 07:53:06.628 ServerApp] Kernel shutdown: 7302a974-e96e-4066-bed8-524889e7f548
[I 2021-08-16 07:53:06.937 ServerApp] Shutting down 0 terminals
[I 2021-08-16 07:53:42.454 ServerApp] Kernel started: 74a5dd87-0c38-402e-a33c-e6cc9581a9b8
[I 2021-08-16 07:53:42.479 ServerApp] Kernel started: 8628a09c-f11b-46cd-8c08-f1fd4b84c884
[W 2021-08-16 07:53:46.263 ServerApp] Got events for closed stream None
[I 2021-08-16 07:54:26.661 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 07:56:26.706 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[W 2021-08-16 07:57:07.307 ServerApp] delete /machine_learning/nlp/aclImdb
[I 2021-08-16 07:57:14.874 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 07:57:29.783 ServerApp] Kernel restarted: 74a5dd87-0c38-402e-a33c-e6cc9581a9b8
[I 2021-08-16 07:57:29.790 ServerApp] Starting buffering for 74a5dd87-0c38-402e-a33c-e6cc9581a9b8:b46cb395-cbd0-46fd-99c2-504ae01c215d
[I 2021-08-16 07:57:29.803 ServerApp] Restoring connection for 74a5dd87-0c38-402e-a33c-e6cc9581a9b8:b46cb395-cbd0-46fd-99c2-504ae01c215d
[I 2021-08-16 07:59:14.916 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 08:11:44.411 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 08:17:46.313 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 08:19:47.220 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 08:25:51.053 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 08:27:51.586 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 08:56:45.942 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[W 2021-08-16 08:58:03.341 ServerApp] delete /machine_learning/nlp/aclImdb
[I 2021-08-16 08:58:46.259 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 09:00:46.316 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 09:02:45.933 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 09:02:46.350 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 09:06:46.394 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 09:08:46.459 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 09:10:47.302 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 09:20:49.172 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 09:27:16.442 ServerApp] Saving file at /machine_learning/nlp/regex.ipynb
[I 2021-08-16 09:28:52.950 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 09:30:53.002 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 09:52:54.206 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 11:05:00.806 ServerApp] Copying machine_learning/nlp/sentiment_analysis.ipynb to /machine_learning/nlp
[I 2021-08-16 11:05:08.883 ServerApp] Kernel started: ce7ddbda-14bc-43e5-9dc6-7ce6301c3fdd
[I 2021-08-16 11:05:19.902 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 11:07:08.645 ServerApp] Saving file at /machine_learning/nlp/bag_of_words.ipynb
[I 2021-08-16 11:11:08.691 ServerApp] Saving file at /machine_learning/nlp/bag_of_words.ipynb
[I 2021-08-16 11:13:08.732 ServerApp] Saving file at /machine_learning/nlp/bag_of_words.ipynb
[I 2021-08-16 11:15:08.786 ServerApp] Saving file at /machine_learning/nlp/bag_of_words.ipynb
[I 2021-08-16 11:17:08.819 ServerApp] Saving file at /machine_learning/nlp/bag_of_words.ipynb
[I 2021-08-16 11:19:08.860 ServerApp] Saving file at /machine_learning/nlp/bag_of_words.ipynb
[I 2021-08-16 11:33:08.896 ServerApp] Saving file at /machine_learning/nlp/bag_of_words.ipynb
[I 2021-08-16 11:33:26.214 ServerApp] Saving file at /machine_learning/nlp/bag_of_words.ipynb
[I 2021-08-16 11:33:37.754 ServerApp] Saving file at /machine_learning/nlp/bag_of_words.ipynb
[I 2021-08-16 11:33:39.469 ServerApp] Copying machine_learning/nlp/bag_of_words.ipynb to /machine_learning/nlp
[I 2021-08-16 11:33:50.585 ServerApp] Kernel started: 5ff8769f-3ebb-4c45-864e-3be8123505a6
[I 2021-08-16 11:35:11.011 ServerApp] Saving file at /machine_learning/nlp/tokenize.ipynb
[I 2021-08-16 11:37:19.944 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 11:39:20.014 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 11:41:20.062 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 11:43:20.146 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 11:45:20.221 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 11:45:22.009 ServerApp] Saving file at /machine_learning/nlp/bag_of_words.ipynb
[I 2021-08-16 11:47:20.291 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 11:49:20.376 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 11:51:20.453 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 11:53:20.528 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 11:55:20.598 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 11:57:20.659 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 11:59:20.732 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 12:00:34.133 ServerApp] Copying machine_learning/nlp/sentiment_analysis.ipynb to /machine_learning/nlp
[I 2021-08-16 12:00:36.091 ServerApp] Starting buffering for ce7ddbda-14bc-43e5-9dc6-7ce6301c3fdd:da6a072f-fe6d-4d80-abaa-fa276a820b6c
[I 2021-08-16 12:00:36.491 ServerApp] Starting buffering for 5ff8769f-3ebb-4c45-864e-3be8123505a6:13b1a2ef-a44f-44c8-9cd5-ace96aa15018
[I 2021-08-16 12:00:42.439 ServerApp] Kernel started: b1fe91fb-6ce3-45e1-b7e1-197d587825b6
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:02:41.468 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:03:20.818 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 12:04:41.566 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 12:06:41.644 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:09:20.889 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341550>; total time=   9.0s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11c341550>; total time= 4.6min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341550>; total time=   6.2s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11c341430>; total time= 4.4min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11c341700>; total time= 4.8min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11c341160>; total time=   8.3s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11c3414c0>; total time= 4.2min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11c3418b0>; total time= 4.3min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341820>; total time=   6.2s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11c341550>; total time= 4.7min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341430>; total time=   8.4s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341700>; total time=   8.5s
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:13:20.959 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:15:21.032 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:17:21.102 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:19:21.175 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:21:21.243 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:25:21.318 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x111b83550>; total time=   9.1s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x111b83550>; total time=   8.7s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x111b83550>; total time= 4.7min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x111b83550>; total time= 4.4min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x111b83700>; total time=   5.7s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x111b838b0>; total time=   7.1s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x111b83160>; total time= 4.8min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x111b83550>; total time=   7.3s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x111b83700>; total time=   7.7s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x111b838b0>; total time=   6.4s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x111b83160>; total time=   6.2s
[I 2021-08-16 12:27:21.389 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x10efa0550>; total time=   9.2s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x10efa0550>; total time= 4.6min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x10efa0550>; total time=   6.2s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x10efa0430>; total time=   6.8s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x10efa0700>; total time= 4.4min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x10efa0160>; total time=   5.6s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x10efa04c0>; total time=   6.6s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x10efa08b0>; total time= 4.8min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10efa0820>; total time= 4.2min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10efa0550>; total time=   7.5s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10efa0430>; total time= 4.4min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x10efa0700>; total time= 4.7min
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:31:21.466 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341160>; total time=  12.8s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11c3414c0>; total time= 4.7min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c3418b0>, vect__use_idf=False; total time=   5.7s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341820>, vect__use_idf=False; total time=   5.6s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341550>, vect__use_idf=False; total time=   6.0s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341430>, vect__use_idf=False; total time=   5.9s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11c341700>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11c341160>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11c3414c0>, vect__use_idf=False; total time=  15.0s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11c3418b0>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11c341820>, vect__use_idf=False; total time= 4.1min
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x120ec1550>; total time= 4.7min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec1550>; total time=   6.0s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec1670>; total time=   6.3s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x120ec1430>; total time= 4.4min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x120ec1700>; total time= 4.8min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x120ec15e0>; total time=   7.8s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x120ec14c0>; total time= 4.3min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x120ec1160>; total time=   7.4s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x120ec1550>; total time=   7.7s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x120ec1670>; total time= 4.4min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x120ec1430>; total time=   6.8s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x120ec1700>; total time=   7.7s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x120ec15e0>; total time=   8.6s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x120ec14c0>; total time= 5.0min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x120ec1160>; total time= 4.7min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x120ec1550>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec1670>, vect__use_idf=False; total time=   7.6s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec1430>, vect__use_idf=False; total time=   7.5s
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:35:21.553 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:37:21.624 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:39:21.703 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:41:21.795 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
/Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', "it'", 'onc', 'onli', 'ourselv', "she'", "should'v", 'themselv', 'thi', 'veri', 'wa', 'whi', "you'r", "you'v", 'yourselv'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
[I 2021-08-16 12:43:21.865 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 12:49:21.871 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x1174c5550>; total time= 4.7min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x1174c5550>; total time= 4.4min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x1174c5670>; total time= 4.8min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x1174c5430>; total time=   7.8s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x1174c5700>; total time=   6.8s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x1174c55e0>; total time=   7.6s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x1174c54c0>; total time= 4.2min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x1174c5160>; total time= 4.3min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x1174c5550>; total time= 4.7min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x1174c5670>; total time= 4.9min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x1174c5430>; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x1174c5700>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x1174c55e0>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x1174c54c0>, vect__use_idf=False; total time=   5.8s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x1174c5160>, vect__use_idf=False; total time=   6.1s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x1174c5550>, vect__use_idf=False; total time=   6.4s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x1174c5670>, vect__use_idf=False; total time=   6.0s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x1174c5430>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x1174c5700>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x1174c55e0>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x1174c54c0>, vect__use_idf=False; total time= 3.9min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x1174c5160>, vect__use_idf=False; total time= 4.1min
[I 2021-08-16 12:51:21.933 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x10efa0160>; total time=  13.5s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x10efa04c0>; total time= 4.7min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10efa08b0>; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10efa0820>, vect__use_idf=False; total time=   9.0s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10efa0550>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x10efa0430>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x10efa0700>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x10efa0160>, vect__use_idf=False; total time=   8.0s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x10efa04c0>, vect__use_idf=False; total time=  10.4s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x10efa08b0>, vect__use_idf=False; total time=   8.1s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x10efa0820>, vect__use_idf=False; total time=   9.2s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x10efa0550>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10efa0430>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10efa0700>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x10efa0160>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11cdf4550>; total time=   9.2s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11cdf4550>; total time=   8.7s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11cdf4550>; total time=   8.8s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11cdf4550>; total time= 4.6min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11cdf4550>; total time=   7.2s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11cdf4160>; total time=   6.5s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11cdf4820>; total time=   6.8s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11cdf4670>; total time=   7.2s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11cdf4790>; total time=   7.3s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11cdf45e0>; total time= 4.6min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11cdf4430>; total time= 4.7min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11cdf4550>; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11cdf4670>; total time= 4.4min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11cdf4160>; total time=   8.2s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11cdf4430>; total time= 4.9min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11cdf4550>; total time= 4.7min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11cdf4670>, vect__use_idf=False; total time=   5.9s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11cdf4160>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11cdf4430>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11cdf4550>, vect__use_idf=False; total time=  13.8s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11cdf4670>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11cdf4160>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11cdf4430>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11cdf4550>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11cdf4670>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11cdf4160>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341550>, vect__use_idf=False; total time=   8.8s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11c341430>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341700>, vect__use_idf=False; total time=   6.1s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341160>, vect__use_idf=False; total time=   5.9s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c3414c0>, vect__use_idf=False; total time=   5.6s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c3418b0>, vect__use_idf=False; total time=   5.6s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x11c341820>, vect__use_idf=False; total time=   5.5s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x11c341550>, vect__use_idf=False; total time= 3.9min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11c341430>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11c341700>, vect__use_idf=False; total time=  14.9s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11c341160>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x115eee550>; total time= 4.7min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x115eee550>; total time= 4.4min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x115eee670>; total time= 4.8min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x115eee430>; total time= 4.2min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x115eee700>; total time= 4.3min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x115eee5e0>; total time=   6.0s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x115eee4c0>; total time=   5.8s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x115eee160>; total time=   6.0s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x115eee550>; total time=   5.8s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x115eee670>; total time= 4.7min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x115eee430>; total time= 4.9min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x115eee700>; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee5e0>, vect__use_idf=False; total time=   7.5s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee4c0>, vect__use_idf=False; total time=   7.9s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee160>, vect__use_idf=False; total time=   8.5s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x115eee550>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x115eee670>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x115eee430>, vect__use_idf=False; total time=   6.1s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x115eee700>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x115eee5e0>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x115eee4c0>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee160>, vect__use_idf=False; total time=  11.2s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee550>, vect__use_idf=False; total time=   6.8s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee670>, vect__use_idf=False; total time=   7.2s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee430>, vect__use_idf=False; total time=   7.1s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee700>, vect__use_idf=False; total time=   7.1s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x115eee5e0>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee4c0>, vect__use_idf=False; total time=  14.4s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee160>, vect__use_idf=False; total time=  13.9s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x115eee550>, vect__use_idf=False; total time=  14.6s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x115eee670>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x111b83550>; total time=   6.0s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x111b83700>; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x111b838b0>; total time= 4.4min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x111b83160>; total time=   8.8s
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x111b83550>; total time= 5.0min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x111b83700>; total time=  11.0s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x111b838b0>; total time=   9.8s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x111b83160>; total time=   9.8s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x111b83550>; total time=   9.8s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x111b83700>; total time=  10.1s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x111b838b0>; total time= 4.6min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x111b83160>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x111b83550>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x111b83700>, vect__use_idf=False; total time= 4.3min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x111b838b0>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x111b83160>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x111b83550>, vect__use_idf=False; total time= 3.9min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x111b83700>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x111b838b0>, vect__use_idf=False; total time=  14.6s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x111b83160>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec1700>, vect__use_idf=False; total time=  11.4s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec15e0>, vect__use_idf=False; total time=   7.7s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec14c0>, vect__use_idf=False; total time=   7.3s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x120ec1160>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x120ec1550>, vect__use_idf=False; total time= 4.3min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x120ec1670>, vect__use_idf=False; total time=   8.5s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x120ec1430>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x120ec1700>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x120ec15e0>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec14c0>, vect__use_idf=False; total time=   8.5s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec1160>, vect__use_idf=False; total time=   8.6s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec1550>, vect__use_idf=False; total time=   8.8s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec1670>, vect__use_idf=False; total time=   9.0s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x120ec1430>, vect__use_idf=False; total time=   8.8s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x120ec1700>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x120ec15e0>, vect__use_idf=False; total time= 3.6min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x123ee9550>; total time=   9.1s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9550>; total time=   7.7s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9550>; total time=   9.3s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x123ee9550>; total time= 4.6min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x123ee9550>; total time= 4.4min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x123ee9160>; total time=   6.3s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x123ee9790>; total time= 4.8min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x123ee9820>; total time= 4.2min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9550>; total time=   6.9s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9160>; total time=   7.6s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x123ee9790>; total time= 4.4min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x123ee9820>; total time= 4.8min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer at 0x123ee9550>; total time=  11.3s
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x123ee9160>; total time= 4.7min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x123ee9790>; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9820>, vect__use_idf=False; total time=   8.3s
[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x123ee9550>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9160>, vect__use_idf=False; total time=  11.9s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9790>, vect__use_idf=False; total time=  13.7s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9820>, vect__use_idf=False; total time=  16.6s
[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x123ee9550>, vect__use_idf=False; total time= 4.2min
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9160>, vect__use_idf=False; total time=  10.0s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9790>, vect__use_idf=False; total time=  10.2s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9820>, vect__use_idf=False; total time=   8.8s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9550>, vect__use_idf=False; total time=   8.5s
[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x123ee9160>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9790>, vect__use_idf=False; total time=  14.4s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9820>, vect__use_idf=False; total time=  14.1s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9550>, vect__use_idf=False; total time=  13.7s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9160>, vect__use_idf=False; total time=  19.0s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x123ee9790>, vect__use_idf=False; total time=  14.4s
[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x123ee9820>, vect__use_idf=False; total time= 4.1min
[CV] END clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x123ee9550>, vect__use_idf=False; total time= 3.9min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"], vect__tokenizer=<function tokenizer_porter at 0x123ee9160>, vect__use_idf=False; total time= 4.0min
[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x123ee9790>, vect__use_idf=False; total time= 3.3min
[I 2021-08-16 12:53:22.001 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 13:16:41.719 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:20:41.825 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:22:41.877 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:23:22.457 ServerApp] Saving file at /machine_learning/nlp/sentiment_analysis.ipynb
[I 2021-08-16 13:24:41.968 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:26:42.060 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:28:42.121 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:30:42.173 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:32:42.221 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:34:42.272 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[W 2021-08-16 13:35:41.623 ServerApp] IOPub data rate exceeded.
    The Jupyter server will temporarily stop sending output
    to the client in order to avoid crashing it.
    To change this limit, set the config variable
    `--ServerApp.iopub_data_rate_limit`.
    
    Current values:
    ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)
    ServerApp.rate_limit_window=3.0 (secs)
    
[W 2021-08-16 13:35:44.356 ServerApp] iopub messages resumed
[W 2021-08-16 13:35:44.585 ServerApp] IOPub data rate exceeded.
    The Jupyter server will temporarily stop sending output
    to the client in order to avoid crashing it.
    To change this limit, set the config variable
    `--ServerApp.iopub_data_rate_limit`.
    
    Current values:
    ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)
    ServerApp.rate_limit_window=3.0 (secs)
    
[W 2021-08-16 13:35:46.633 ServerApp] iopub messages resumed
[W 2021-08-16 13:35:47.051 ServerApp] IOPub data rate exceeded.
    The Jupyter server will temporarily stop sending output
    to the client in order to avoid crashing it.
    To change this limit, set the config variable
    `--ServerApp.iopub_data_rate_limit`.
    
    Current values:
    ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)
    ServerApp.rate_limit_window=3.0 (secs)
    
[W 2021-08-16 13:35:47.528 ServerApp] iopub messages resumed
[W 2021-08-16 13:35:48.178 ServerApp] IOPub data rate exceeded.
    The Jupyter server will temporarily stop sending output
    to the client in order to avoid crashing it.
    To change this limit, set the config variable
    `--ServerApp.iopub_data_rate_limit`.
    
    Current values:
    ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)
    ServerApp.rate_limit_window=3.0 (secs)
    
[W 2021-08-16 13:35:51.121 ServerApp] iopub messages resumed
[I 2021-08-16 13:36:42.303 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:38:42.295 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:40:42.332 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:40:57.257 ServerApp] Kernel interrupted: b1fe91fb-6ce3-45e1-b7e1-197d587825b6
[I 2021-08-16 13:42:42.385 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:42:46.147 ServerApp] Kernel restarted: b1fe91fb-6ce3-45e1-b7e1-197d587825b6
[I 2021-08-16 13:42:46.154 ServerApp] Starting buffering for b1fe91fb-6ce3-45e1-b7e1-197d587825b6:0b20cebc-4977-42d4-9df6-0b81279d4c97
[I 2021-08-16 13:42:46.160 ServerApp] Restoring connection for b1fe91fb-6ce3-45e1-b7e1-197d587825b6:0b20cebc-4977-42d4-9df6-0b81279d4c97
[W 2021-08-16 13:42:50.432 ServerApp] delete /machine_learning/nlp/movie_data.csv
[I 2021-08-16 13:44:42.429 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:46:42.476 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:48:42.518 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:50:42.570 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 13:52:42.620 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 17:18:59.117 ServerApp] Copying machine_learning/nlp/online_algorithm.ipynb to /machine_learning/nlp
[I 2021-08-16 17:19:23.005 ServerApp] Kernel started: a89b21ad-576a-40bc-8b0e-478f744172e5
[I 2021-08-16 17:19:34.222 ServerApp] Starting buffering for 74a5dd87-0c38-402e-a33c-e6cc9581a9b8:b46cb395-cbd0-46fd-99c2-504ae01c215d
[I 2021-08-16 17:19:37.024 ServerApp] Saving file at /machine_learning/nlp/online_algorithm.ipynb
[I 2021-08-16 17:19:38.790 ServerApp] Starting buffering for b1fe91fb-6ce3-45e1-b7e1-197d587825b6:0b20cebc-4977-42d4-9df6-0b81279d4c97
[I 2021-08-16 17:21:22.701 ServerApp] Saving file at /machine_learning/nlp/latent_dirichlet_allocation.ipynb
[I 2021-08-16 17:23:22.741 ServerApp] Saving file at /machine_learning/nlp/latent_dirichlet_allocation.ipynb
[I 2021-08-16 17:29:22.789 ServerApp] Saving file at /machine_learning/nlp/latent_dirichlet_allocation.ipynb
[I 2021-08-16 17:33:22.764 ServerApp] Saving file at /machine_learning/nlp/latent_dirichlet_allocation.ipynb
[I 2021-08-16 17:35:22.797 ServerApp] Saving file at /machine_learning/nlp/latent_dirichlet_allocation.ipynb
[I 2021-08-16 18:05:22.926 ServerApp] Saving file at /machine_learning/nlp/latent_dirichlet_allocation.ipynb
[I 2021-08-16 18:07:22.975 ServerApp] Saving file at /machine_learning/nlp/latent_dirichlet_allocation.ipynb
[I 2021-08-16 18:09:23.021 ServerApp] Saving file at /machine_learning/nlp/latent_dirichlet_allocation.ipynb
[I 2021-08-16 18:10:18.583 ServerApp] Saving file at /machine_learning/nlp/latent_dirichlet_allocation.ipynb
[I 2021-08-16 18:10:20.281 ServerApp] Starting buffering for a89b21ad-576a-40bc-8b0e-478f744172e5:f94cc3d0-6617-47b9-a2cd-76425be2a817
[I 2021-08-16 18:10:20.282 ServerApp] Starting buffering for 8628a09c-f11b-46cd-8c08-f1fd4b84c884:9e04617a-708c-45c3-9a3c-710732319b0b
[I 2021-08-16 18:10:58.352 ServerApp] jupyterlab | extension was successfully linked.
[I 2021-08-16 18:10:58.808 ServerApp] jupyter_nbextensions_configurator | extension was found and enabled by nbclassic. Consider moving the extension to Jupyter Server's extension paths.
[I 2021-08-16 18:10:58.808 ServerApp] jupyter_nbextensions_configurator | extension was successfully linked.
[I 2021-08-16 18:10:58.809 ServerApp] nbclassic | extension was successfully linked.
[I 2021-08-16 18:10:58.878 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-08-16 18:10:58.879 ServerApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 2021-08-16 18:10:58.879 ServerApp] jupyter_nbextensions_configurator | extension was successfully loaded.
[I 2021-08-16 18:10:58.880 LabApp] JupyterLab extension loaded from /Users/othrif/.pyenv/versions/3.9.6/lib/python3.9/site-packages/jupyterlab
[I 2021-08-16 18:10:58.881 LabApp] JupyterLab application directory is /Users/othrif/.pyenv/versions/3.9.6/share/jupyter/lab
[I 2021-08-16 18:10:58.886 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-08-16 18:10:58.887 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-08-16 18:10:58.888 ServerApp] Serving notebooks from local directory: /Users/othrif/github/notes/content
[I 2021-08-16 18:10:58.889 ServerApp] Jupyter Server 1.9.0 is running at:
[I 2021-08-16 18:10:58.889 ServerApp] http://localhost:8889/lab?token=e826c782944a4cda930e8cc8c994e023273ca71d6f42bb35
[I 2021-08-16 18:10:58.889 ServerApp]  or http://127.0.0.1:8889/lab?token=e826c782944a4cda930e8cc8c994e023273ca71d6f42bb35
[I 2021-08-16 18:10:58.889 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2021-08-16 18:10:58.912 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///Users/othrif/Library/Jupyter/runtime/jpserver-47930-open.html
    Or copy and paste one of these URLs:
        http://localhost:8889/lab?token=e826c782944a4cda930e8cc8c994e023273ca71d6f42bb35
     or http://127.0.0.1:8889/lab?token=e826c782944a4cda930e8cc8c994e023273ca71d6f42bb35
[I 2021-08-16 18:11:03.357 LabApp] Build is up to date
[I 2021-08-16 18:11:04.781 ServerApp] Kernel started: 51bd46e2-6789-4fc1-b966-616a84d0b7f9
[W 2021-08-16 18:11:08.497 ServerApp] Got events for closed stream None
[I 2021-08-16 18:12:11.113 ServerApp] Saving file at /machine_learning/nlp/latent_dirichlet_allocation.ipynb
[I 2021-08-16 18:12:27.680 ServerApp] Kernel started: fdab2aae-0765-48ca-9210-2f38002d4101
[I 2021-08-16 18:12:32.756 ServerApp] Copying python/basics/config_file.ipynb to /python/basics
[I 2021-08-16 18:12:39.952 ServerApp] Kernel started: 15a82e71-b7e3-4141-806c-879c2b493bc1
[I 2021-08-16 18:14:27.574 ServerApp] Saving file at /python/basics/config_file.ipynb
[I 2021-08-16 18:14:40.788 ServerApp] Saving file at /python/basics/inspect_module.ipynb
[I 2021-08-16 18:16:14.965 ServerApp] Saving file at /python/basics/inspect_module.ipynb
[I 2021-08-16 18:16:22.021 ServerApp] Saving file at /python/basics/inspect_module.ipynb
[I 2021-08-16 18:16:23.324 ServerApp] Starting buffering for 15a82e71-b7e3-4141-806c-879c2b493bc1:ddcb6b6c-7f86-4f82-99b7-db09b8bfc029
[I 2021-08-16 18:16:23.325 ServerApp] Starting buffering for fdab2aae-0765-48ca-9210-2f38002d4101:831e56f6-1d11-40cf-a5ba-738430f63310
[I 2021-08-16 18:16:23.326 ServerApp] Starting buffering for 51bd46e2-6789-4fc1-b966-616a84d0b7f9:56731f9c-9c1b-452c-b82b-2ec48b67e60c
